{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ed9f53",
   "metadata": {},
   "source": [
    "# The Complexity Stress Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aafe23",
   "metadata": {},
   "source": [
    "First, install the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6403bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textcomplexity\n",
      "  Downloading textcomplexity-0.11.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textcomplexity) (3.4.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textcomplexity) (3.9.1)\n",
      "Collecting nltk-tgrep (from textcomplexity)\n",
      "  Downloading nltk_tgrep-1.0.6-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textcomplexity) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textcomplexity) (1.13.1)\n",
      "Requirement already satisfied: click in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->textcomplexity) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->textcomplexity) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->textcomplexity) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->textcomplexity) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk->textcomplexity) (0.4.6)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk-tgrep->textcomplexity) (3.2.0)\n",
      "Downloading textcomplexity-0.11.0-py3-none-any.whl (90 kB)\n",
      "Downloading nltk_tgrep-1.0.6-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: nltk-tgrep, textcomplexity\n",
      "\n",
      "   -------------------- ------------------- 1/2 [textcomplexity]\n",
      "   ---------------------------------------- 2/2 [textcomplexity]\n",
      "\n",
      "Successfully installed nltk-tgrep-1.0.6 textcomplexity-0.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading stanza-1.11.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (6.30.2)\n",
      "Requirement already satisfied: requests in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (3.4.2)\n",
      "Requirement already satisfied: tomli in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (2.2.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (2.7.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->stanza) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.13.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->stanza) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->stanza) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->stanza) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->stanza) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.13.0->stanza) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Downloading stanza-1.11.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 13.2 MB/s eta 0:00:00\n",
      "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "   ---------------------------------------- 0.0/608.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 608.4/608.4 kB 11.2 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji, stanza\n",
      "\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   ---------------------------------------- 2/2 [stanza]\n",
      "\n",
      "Successfully installed emoji-2.15.0 stanza-1.11.0\n",
      "Collecting wordfreq"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading wordfreq-3.1.1-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ftfy>=6.1 (from wordfreq)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting langcodes>=3.0 (from wordfreq)\n",
      "  Downloading langcodes-3.5.1-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting locate<2.0.0,>=1.1.1 (from wordfreq)\n",
      "  Downloading locate-1.1.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.7 (from wordfreq)\n",
      "  Downloading msgpack-1.1.2-cp310-cp310-win_amd64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: regex>=2023.10.3 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wordfreq) (2024.11.6)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
      "Downloading wordfreq-3.1.1-py3-none-any.whl (56.8 MB)\n",
      "   ---------------------------------------- 0.0/56.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 7.6/56.8 MB 52.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 23.6/56.8 MB 64.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 37.5/56.8 MB 66.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 48.5/56.8 MB 60.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 56.8/56.8 MB 28.1 MB/s eta 0:00:00\n",
      "Downloading locate-1.1.1-py3-none-any.whl (5.4 kB)\n",
      "Downloading msgpack-1.1.2-cp310-cp310-win_amd64.whl (71 kB)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading langcodes-3.5.1-py3-none-any.whl (183 kB)\n",
      "Installing collected packages: msgpack, locate, langcodes, ftfy, wordfreq\n",
      "\n",
      "   ---------------- ----------------------- 2/5 [langcodes]\n",
      "   ------------------------ --------------- 3/5 [ftfy]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   ---------------------------------------- 5/5 [wordfreq]\n",
      "\n",
      "Successfully installed ftfy-6.3.1 langcodes-3.5.1 locate-1.1.1 msgpack-1.1.2 wordfreq-3.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python310\\python.exe: No module named spacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.67.1)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp310-cp310-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp310-cp310-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (80.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp310-cp310-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Downloading spacy-3.8.11-cp310-cp310-win_amd64.whl (15.3 MB)\n",
      "   ---------------------------------------- 0.0/15.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 8.4/15.3 MB 52.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.2/15.3 MB 59.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.3/15.3 MB 35.7 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp310-cp310-win_amd64.whl (40 kB)\n",
      "Downloading murmurhash-1.0.15-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.12-cp310-cp310-win_amd64.whl (117 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp310-cp310-win_amd64.whl (654 kB)\n",
      "   ---------------------------------------- 0.0/654.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 654.0/654.0 kB 12.6 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.10-cp310-cp310-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 24.3 MB/s eta 0:00:00\n",
      "Downloading blis-1.3.3-cp310-cp310-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 38.0 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Installing collected packages: wasabi, spacy-loggers, spacy-legacy, murmurhash, cymem, cloudpathlib, catalogue, blis, typer-slim, srsly, preshed, confection, weasel, thinc, spacy\n",
      "\n",
      "   ----------------------------------------  0/15 [wasabi]\n",
      "   -- -------------------------------------  1/15 [spacy-loggers]\n",
      "   ----- ----------------------------------  2/15 [spacy-legacy]\n",
      "   ---------- -----------------------------  4/15 [cymem]\n",
      "   ------------- --------------------------  5/15 [cloudpathlib]\n",
      "   ---------------- -----------------------  6/15 [catalogue]\n",
      "   ------------------ ---------------------  7/15 [blis]\n",
      "   --------------------- ------------------  8/15 [typer-slim]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ----------------------------- ---------- 11/15 [confection]\n",
      "   ----------------------------- ---------- 11/15 [confection]\n",
      "   -------------------------------- ------- 12/15 [weasel]\n",
      "   -------------------------------- ------- 12/15 [weasel]\n",
      "   -------------------------------- ------- 12/15 [weasel]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ---------------------------------------- 15/15 [spacy]\n",
      "\n",
      "Successfully installed blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 typer-slim-0.20.0 wasabi-1.1.3 weasel-0.4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install textcomplexity\n",
    "!{sys.executable} -m pip install stanza\n",
    "!{sys.executable} -m pip install wordfreq \n",
    "!{sys.executable} -m spacy download en_core_web_md\n",
    "!{sys.executable} -m pip install tqdm spacy numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625ec67",
   "metadata": {},
   "source": [
    "First, import the following Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6789f29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0def8b1931524a44aabe6830496359e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 11:47:53 INFO: Downloaded file to C:\\Users\\loren\\stanza_resources\\resources.json\n",
      "2025-11-17 11:47:53 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-11-17 11:47:54 INFO: File exists: C:\\Users\\loren\\stanza_resources\\en\\default.zip\n",
      "2025-11-17 11:47:57 INFO: Finished downloading models and saved to C:\\Users\\loren\\stanza_resources\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\loren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\loren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1ac7ef29010>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "from pprint import pprint\n",
    "from typing import Dict, Set, Iterable, Optional, Any, Tuple\n",
    "import importlib.resources as pkg_resources\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "import stanza\n",
    "import textcomplexity  # only used to access en.json\n",
    "from tqdm.auto import tqdm  \n",
    "\n",
    "# Download required resources\n",
    "stanza.download('en')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Make sure WordNet is available; if not, download it.\n",
    "try:\n",
    "    _ = wn.synsets(\"dog\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "spacy_nlp = nlp\n",
    "spacy_nlp.add_pipe(\"sentencizer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57283f",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eef7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets ={'ose_adv_ele':'data_sampled/OSE_adv_ele.csv', \n",
    "           'ose_adv_int':'data_sampled/OSE_adv_int.csv',\n",
    "           'swipe': 'data_sampled/swipe.csv',\n",
    "           'vikidia':'data_sampled/vikidia.csv'}\n",
    "\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path, sep='\\t')\n",
    "    \n",
    "\n",
    "def load_dataset(name):\n",
    "    if name not in datasets:\n",
    "        raise ValueError(f\"Dataset {name} not found\")\n",
    "    return load_data(datasets[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c93eb1",
   "metadata": {},
   "source": [
    "Let's load one of the datasets, in this case \"ose_adv_ele\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae91d38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Simple</th>\n",
       "      <th>Complex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿To tourists, Amsterdam still seems very liber...</td>\n",
       "      <td>﻿Amsterdam still looks liberal to tourists, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>﻿Anitta, a music star from Brazil, has million...</td>\n",
       "      <td>﻿Brazil’s latest funk sensation, Anitta, has w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Simple  \\\n",
       "0  ﻿When you see the word Amazon, what’s the firs...   \n",
       "1  ﻿To tourists, Amsterdam still seems very liber...   \n",
       "2  ﻿Anitta, a music star from Brazil, has million...   \n",
       "\n",
       "                                             Complex  \n",
       "0  ﻿When you see the word Amazon, what’s the firs...  \n",
       "1  ﻿Amsterdam still looks liberal to tourists, wh...  \n",
       "2  ﻿Brazil’s latest funk sensation, Anitta, has w...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('ose_adv_ele')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb2dc3",
   "metadata": {},
   "source": [
    "Let's look at a random row of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7bb885b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE TEXT\n",
      "﻿A top-secret document shows that the US National Security Agency (NSA) now has direct access to the systems of Google, Facebook, Apple and other major US internet companies. The NSA access is part of a program called PRISM, which allows the government to collect search history, the content of emails, file transfers, live chats and more, the document says.\n",
      "The document says that the NSA can now get information “directly from the servers” of major US internet companies. It says the companies help them run the program, but all the companies that commented said they have not heard of the program.\n",
      "Google said: “Google cares very much about the security of our users’ data. We disclose user data to government legally and, when the government asks us for data, we think about it carefully first. Sometimes, people allege that we have created a government 'back door' into our systems, but Google does not have a back door for the government to access private user data.”\n",
      "Several senior tech executives said that they had no knowledge of PRISM or of any similar program. They said they would never be involved in a program like that. “If they are doing this, they are doing it without our knowledge,” one executive said. An Apple spokesman said he has “never heard” of PRISM.\n",
      "Changes to US surveillance law, introduced under President Bush and renewed under Obama in December 2012, made it possible for the NSA to access the information. The program allows a large amount of in-depth surveillance on live communications and stored information. The law allows the NSA to watch customers of companies who live outside the US or Americans who communicate with people outside the US.\n",
      "The document says that some of the world’s largest internet companies have been part of the information-sharing program sinceits introduction in 2007. Microsoft – whose advertising slogan is “Your privacy is our priority” – was the first, in December 2007. It was followed by Yahoo in 2008; Google, Facebook and PalTalk in 2009; YouTube in 2010; Skype and AOL in 2011; and finally Apple, which joined the program in 2012.\n",
      "Under US law, if the government asks for users’ communications, companies must give that information, but the PRISM program allows the government direct access to the companies’ servers.\n",
      "The Foreign Intelligence Surveillance Act (FISA) was changed in December 2012. At the time, several US senators were worried that the law might increase the amount of surveillance and they could see problems with some of the safeguards in the law. When the change in the law was first introduced, its supporters said that one safeguard would be that the NSA could not get electronic communications without the permission of the telecom and internet companies that control the data. But the PRISM program makes that permission unnecessary, because it allows the government to take directly from the companies’ servers communications that include email, video and voice chat, videos, photos, file transfers and social networking details.\n",
      "A senior administration official said: “Section 702 of the FISA does not allow the targeting of any US citizen or of any person who is within the United States. It targets only non- US persons outside the US.\n",
      "“Information that is collected under this program is some of the most important and valuable intelligence information we collect and we use it to protect our nation from a wide variety of threats.”\n",
      "----------------------------------------------------------------------------------------------------\n",
      "COMPLEX TEXT\n",
      "﻿The National Security Agency (NSA) has obtained direct access to the systems of Google, Facebook, Apple and other US internet giants, according to a top-secret document. The NSA access is part of a previously undisclosed program called PRISM, which allows officials to collect material including search history, the content of emails, file transfers and live chats, the document says.\n",
      "The Guardian has verified the authenticity of the document, a 41-slide PowerPoint presentation – classified as top secret with no distribution to foreign allies – which was apparently used to train intelligence operatives on the capabilities of the program. The document claims “collection directly from the servers” of major US service providers. Although the presentation claims the program is run with the assistance of the companies, all those who responded to a request for comment denied knowledge of any such program.\n",
      "In a statement, Google said: “Google cares deeply about the security of our users’ data. We disclose user data to government in accordance with the law and we review all such requests carefully. From time to time, people allege that we have created a government 'back door' into our systems, but Google does not have a back door for the government to access private user data.”\n",
      "Several senior tech executives insisted that they had no knowledge of PRISM or of any similar scheme. They said they would never have been involved in such a program. “If they are doing this, they are doing it without our knowledge,” one said. An Apple spokesman said he had “never heard” of PRISM.\n",
      "The NSA access was enabled by changes to US surveillance law, introduced under President Bush and renewed under Obama in December 2012. The program facilitates extensive, in-depth surveillance on live communications and stored information. The law allows for the targeting of any customers of participating firms who live outside the US, or those Americans whose communications include people outside the US. It also opens the possibility of communications made entirely within the US being collected without warrants.\n",
      "Disclosure of the PRISM program follows a leak to the Guardian on Wednesday of a top-secret court order compelling telecoms provider Verizon to turn over the telephone records of millions of US customers. The participation of the internet companies in PRISM will add to the debate about the scale of surveillance by the intelligence services. Unlike the collection of those call records, this surveillance can include the content of communications and not just the metadata. Some of the world’s largest internet brands are claimed to be part of the information-sharing program since its introduction in 2007. Microsoft – which is currently running an advertising campaign with the slogan “Your privacy is our priority” – was the first, with collection beginning in December 2007. It was followed by Yahoo in 2008; Google, Facebook and PalTalk in 2009; YouTube in 2010; Skype and AOL in 2011; and finally Apple, which joined the program in 2012. The program is continuing to expand, with other providers due to come online. Collectively, the companies cover the vast majority of online email, search, video and communications networks. The extent and nature of the data collected from each company varies. Companies are legally obliged to comply with requests for users’ communications under US law, but the PRISM program allows the intelligence services direct access to the companies’ servers. The NSA document notes the operations have the “assistance of communications providers in the US ”. The revelation also supports concerns raised by several US senators during the renewal of the FISA Amendments Act (FAA) in December 2012, who warned about the scale of surveillance the law might enable and shortcomings in the safeguards it introduces. When the FAA was first enacted, defenders of the statute argued that a significant check on abuse would be the NSA’s inability to obtain electronic communications without the consent of the telecom and internet companies that control the data.\n",
      "But the PRISM program renders that consent unnecessary, as it allows the agency to directly and unilaterally seize the communications off the companies’ servers.\n",
      "A chart prepared by the NSA, contained within the top-secret document, highlights the breadth of the data it is able to obtain: email, video and voice chat, videos, photos, voice-over-IP (Skype, for example) chats, file transfers, social networking details and more. The document is recent, dating to April 2013. Such a leak is extremely rare in the history of the NSA, which prides itself on maintaining a high level of secrecy.\n",
      "The PRISM program allows the NSA, the world’s largest surveillance organization, to obtain targeted communications without having to request them from the service providers and without having to obtain individual court orders. With this program, the NSA is able to reach directly into the servers of the participating companies and obtain both stored communications and perform real-time collection on targeted users.\n",
      "A senior administration official said in a statement: “The Guardian and Washington Post articles refer to collection of communications pursuant to Section 702 of the Foreign Intelligence Surveillance Act (FISA). This law does not allow the targeting of any US citizen or of any person located within the United States. The program is subject to oversight by the Foreign Intelligence Surveillance Court, the Executive Branch and Congress. It involves extensive procedures, specifically approved by the court, to ensure that only non-US persons outside the US are targeted and that minimize the acquisition, retention and dissemination of incidentally acquired information about US persons.\n",
      "“This program was recently reauthorized by Congress after extensive hearings and debate. Information collected under this program is among the most important and valuable intelligence information we collect and is used to protect our nation from a wide variety of threats.”\n"
     ]
    }
   ],
   "source": [
    "row = df.sample(1)\n",
    "\n",
    "print('SIMPLE TEXT')\n",
    "print(row['Simple'].iloc[0])\n",
    "print('-'*100)\n",
    "print('COMPLEX TEXT')\n",
    "print(row['Complex'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ebbb2",
   "metadata": {},
   "source": [
    "Let's look at the size of each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72529061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ose_adv_ele: 189 rows\n",
      "ose_adv_int: 189 rows\n",
      "swipe: 1233 rows\n",
      "vikidia: 1233 rows\n",
      "Total: 2844 rows\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for name, path in datasets.items():\n",
    "    df = load_dataset(name)\n",
    "    print(f\"{name}: {df.shape[0]} rows\")\n",
    "    cnt += df.shape[0]\n",
    "print(f\"Total: {cnt} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571b7484",
   "metadata": {},
   "source": [
    "Let's load again the dataset for computing the complexity measure in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b7e233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset('ose_adv_ele')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd81ace0",
   "metadata": {},
   "source": [
    "## Complexity measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d9ff4",
   "metadata": {},
   "source": [
    "We provide the function for computing the complexity measures with respect to the lexical, syntactic, and discourse dimensions. It is worth noticing that complexity functions on the datasets should be computed offline beforehand and the results saved, as they are computationally expensive. Specifically, for both the Simple and Complex texts, the complexity measures should be calculated first and stored (e.g., in Pandas DataFrames with the new complexity feature columns added) before feeding the augmented datasets to the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0d91084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cache stanza pipelines to avoid re-loading models\n",
    "_STANZA_PIPELINES: Dict[str, stanza.Pipeline] = {}\n",
    "\n",
    "# UPOS tags considered content words (C)\n",
    "CONTENT_UPOS = {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def load_cow_top5000_en() -> Set[str]:\n",
    "    \"\"\"\n",
    "    Load the COW-based list of the 5,000 most frequent English content words\n",
    "    from textcomplexity's English language definition file (en.json).\n",
    "\n",
    "    We ignore POS tags and keep only lowercased word forms.\n",
    "    \"\"\"\n",
    "    with pkg_resources.files(textcomplexity).joinpath(\"en.json\").open(\n",
    "        \"r\", encoding=\"utf-8\"\n",
    "    ) as f:\n",
    "        lang_def = json.load(f)\n",
    "\n",
    "    most_common = lang_def[\"most_common\"]  # list of [word, xpos]\n",
    "    cow_top5000 = {w.lower() for w, xpos in most_common}\n",
    "    return cow_top5000\n",
    "\n",
    "\n",
    "def get_stanza_pipeline(lang: str = \"en\", use_gpu: bool = False) -> stanza.Pipeline:\n",
    "    \"\"\"\n",
    "    Get (or create) a cached stanza Pipeline for a given language.\n",
    "\n",
    "    NOTE: You must have downloaded the models beforehand, e.g.:\n",
    "        import stanza\n",
    "        stanza.download('en')\n",
    "    \"\"\"\n",
    "    if lang not in _STANZA_PIPELINES:\n",
    "        _STANZA_PIPELINES[lang] = stanza.Pipeline(\n",
    "            lang=lang,\n",
    "            processors=\"tokenize,pos,lemma,depparse,constituency\",\n",
    "            use_gpu=use_gpu,\n",
    "            tokenize_no_ssplit=False,\n",
    "        )\n",
    "    return _STANZA_PIPELINES[lang]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a53cb",
   "metadata": {},
   "source": [
    "### Lexical complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "929089ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_mtld(tokens: Iterable[str], ttr_threshold: float = 0.72) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute MTLD (Measure of Textual Lexical Diversity) for a list of tokens.\n",
    "\n",
    "    MTLD = total_number_of_tokens / number_of_factors\n",
    "\n",
    "    A factor is a contiguous segment where the running TTR stays >= threshold.\n",
    "    When the TTR drops below the threshold, we close a factor (at the previous\n",
    "    token) and start a new one. At the end, the remaining partial segment is\n",
    "    counted as a fractional factor, with weight proportional to how close the\n",
    "    final TTR is to the threshold.\n",
    "    \"\"\"\n",
    "    tokens = [tok for tok in tokens if tok]\n",
    "    if not tokens:\n",
    "        return None\n",
    "\n",
    "    types = set()\n",
    "    factor_count = 0.0\n",
    "    token_count_in_factor = 0\n",
    "\n",
    "    for tok in tokens:\n",
    "        token_count_in_factor += 1\n",
    "        types.add(tok)\n",
    "        ttr = len(types) / token_count_in_factor\n",
    "\n",
    "        if ttr < ttr_threshold:\n",
    "            factor_count += 1.0\n",
    "            types = set()\n",
    "            token_count_in_factor = 0\n",
    "\n",
    "    # final partial factor\n",
    "    if token_count_in_factor > 0:\n",
    "        final_ttr = len(types) / token_count_in_factor\n",
    "        if final_ttr < 1.0:\n",
    "            fractional = (1.0 - final_ttr) / (1.0 - ttr_threshold)\n",
    "            fractional = max(0.0, min(1.0, fractional))\n",
    "            factor_count += fractional\n",
    "\n",
    "    if factor_count == 0:\n",
    "        return None\n",
    "\n",
    "    return len(tokens) / factor_count\n",
    "\n",
    "\n",
    "\n",
    "def _compute_lexical_density(total_tokens: int, content_tokens: int) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LD = |C| / |T|\n",
    "    where:\n",
    "        |C| = number of content-word tokens\n",
    "        |T| = total number of non-punctuation tokens\n",
    "    \"\"\"\n",
    "    if total_tokens == 0:\n",
    "        return None\n",
    "    return content_tokens / total_tokens\n",
    "\n",
    "\n",
    "def _compute_lexical_sophistication_cow(\n",
    "    content_forms: Iterable[str],\n",
    "    cow_top5000: set,\n",
    ") -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LS = |{ w in C : w not in R }| / |C|\n",
    "    where:\n",
    "        C = content-word tokens (surface forms, lowercased)\n",
    "        R = COW top-5000 content word forms (lowercased)\n",
    "    \"\"\"\n",
    "    forms = [f for f in content_forms if f]\n",
    "    if not forms:\n",
    "        return None\n",
    "\n",
    "    off_list = sum(1 for f in forms if f not in cow_top5000)\n",
    "    return off_list / len(forms)\n",
    "\n",
    "\n",
    "\n",
    "def lexical_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MTLD, LD, LS from a stanza Document.\n",
    "    \"\"\"\n",
    "    cow_top5000 = load_cow_top5000_en()\n",
    "\n",
    "    mtld_tokens = []\n",
    "    total_tokens = 0\n",
    "    content_tokens = 0\n",
    "    content_forms = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.upos == \"PUNCT\":\n",
    "                continue\n",
    "\n",
    "            lemma = (word.lemma or word.text or \"\").lower()\n",
    "            if not lemma:\n",
    "                continue\n",
    "\n",
    "            mtld_tokens.append(lemma)\n",
    "            total_tokens += 1\n",
    "\n",
    "            if word.upos in CONTENT_UPOS:\n",
    "                content_tokens += 1\n",
    "                form = (word.text or \"\").lower()\n",
    "                content_forms.append(form)\n",
    "\n",
    "    mtld = _compute_mtld(mtld_tokens) if mtld_tokens else None\n",
    "    ld = _compute_lexical_density(total_tokens, content_tokens)\n",
    "    ls = _compute_lexical_sophistication_cow(content_forms, cow_top5000)\n",
    "\n",
    "    return {\"MTLD\": mtld, \"LD\": ld, \"LS\": ls}\n",
    "\n",
    "\n",
    "def lexical_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute lexical measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return lexical_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute lexical measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = lexical_measures_from_text(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "\n",
    "    return {\"MTLD\": mtld_res, \"LD\": ld_res, \"LS\": ls_res}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4583f900",
   "metadata": {},
   "source": [
    "### Syntactic complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mdd_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute Mean Dependency Distance (MDD) from a stanza Document.\n",
    "\n",
    "    For each sentence s_i with dependency set D_i:\n",
    "        MDD_i = (1 / |D_i|) * sum_{(h,d) in D_i} |h - d|\n",
    "    Then:\n",
    "        MDD = (1 / k) * sum_i MDD_i, over all sentences with at least one dependency.\n",
    "    \"\"\"\n",
    "    sentence_mdds = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        distances = []\n",
    "        for w in sent.words:\n",
    "            if w.head is None or w.head == 0:\n",
    "                continue\n",
    "            distances.append(abs(w.id - w.head))\n",
    "\n",
    "        if distances:\n",
    "            sentence_mdds.append(sum(distances) / len(distances))\n",
    "\n",
    "    if not sentence_mdds:\n",
    "        return None\n",
    "    return sum(sentence_mdds) / len(sentence_mdds)\n",
    "\n",
    "\n",
    "\n",
    "def _count_clauses_in_tree(tree) -> int:\n",
    "    \"\"\"\n",
    "    Count clause nodes in a constituency tree.\n",
    "\n",
    "    A simple and standard heuristic (PTB-style) is:\n",
    "        count all nodes whose label starts with 'S'\n",
    "        (S, SBAR, SBARQ, SINV, SQ, etc.).\n",
    "\n",
    "    This aligns with the idea of counting finite and subordinate clauses\n",
    "    as in Hunt (1965) and later complexity work.\n",
    "    \"\"\"\n",
    "    if tree is None:\n",
    "        return 0\n",
    "\n",
    "    # Stanza's constituency tree: tree.label, tree.children\n",
    "    count = 1 if getattr(tree, \"label\", \"\").startswith(\"S\") else 0\n",
    "\n",
    "    for child in getattr(tree, \"children\", []):\n",
    "        # leaves can be strings or terminals without 'label'\n",
    "        if hasattr(child, \"label\"):\n",
    "            count += _count_clauses_in_tree(child)\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def cs_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute CS (clauses per sentence) from a stanza Document.\n",
    "\n",
    "        CS = (1 / k) * sum_i L_i\n",
    "\n",
    "    where L_i is the number of clauses in sentence s_i, estimated by counting\n",
    "    all constituents whose label starts with 'S' in the constituency tree of s_i.\n",
    "    \"\"\"\n",
    "    clause_counts = []\n",
    "    for sent in doc.sentences:\n",
    "        tree = getattr(sent, \"constituency\", None)\n",
    "        if tree is None:\n",
    "            # No constituency tree available for this sentence\n",
    "            continue\n",
    "        num_clauses = _count_clauses_in_tree(tree)\n",
    "        clause_counts.append(num_clauses)\n",
    "\n",
    "    if not clause_counts:\n",
    "        return None\n",
    "\n",
    "    return sum(clause_counts) / len(clause_counts)\n",
    "\n",
    "\n",
    "\n",
    "def syntactic_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MDD and CS from a stanza Document.\n",
    "    \"\"\"\n",
    "    mdd = mdd_from_doc(doc)\n",
    "    cs = cs_from_doc(doc)\n",
    "    return {\"MDD\": mdd, \"CS\": cs}\n",
    "\n",
    "\n",
    "def syntactic_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute syntactic measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return syntactic_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "def compute_syntactic_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute syntactic measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MDD\": {index: value},\n",
    "            \"CS\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = syntactic_measures_from_text(text, lang=lang)\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "    return {\"MDD\": mdd_res, \"CS\": cs_res}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20110b8",
   "metadata": {},
   "source": [
    "### Discourse complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "739a62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Approximate set of content POS tags (spaCy universal POS)\n",
    "CONTENT_POS =  {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "def is_content_token(tok):\n",
    "    \"\"\"\n",
    "    Return True if token is considered a content word.\n",
    "    We ignore stopwords, punctuation, and non-alphabetic tokens.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        tok.is_alpha\n",
    "        and not tok.is_stop\n",
    "        and tok.pos_ in CONTENT_POS\n",
    "    )\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=100000)\n",
    "def get_related_lemmas(lemma):\n",
    "    \"\"\"\n",
    "    Return a set of semantically related lemmas for the given lemma\n",
    "    using WordNet, including:\n",
    "      - synonyms\n",
    "      - antonyms\n",
    "      - hypernyms / hyponyms\n",
    "      - meronyms (part/member/substance)\n",
    "      - coordinate terms (siblings under the same hypernym)\n",
    "\n",
    "    NOTE: Some older examples mention 'troponyms', but in NLTK's\n",
    "    WordNet interface there is no 'troponyms()' method on Synset,\n",
    "    so we do NOT use it here.\n",
    "    \"\"\"\n",
    "    lemma = lemma.lower()\n",
    "    related = set()\n",
    "    synsets = wn.synsets(lemma)\n",
    "\n",
    "    for syn in synsets:\n",
    "        # Synonyms and antonyms\n",
    "        for l in syn.lemmas():\n",
    "            related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "            for ant in l.antonyms():\n",
    "                related.add(ant.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Hypernyms (more general) and hyponyms (more specific)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for l in hyper.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "        for hypo in syn.hyponyms():\n",
    "            for l in hypo.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Meronyms: part/member/substance\n",
    "        for mer in syn.part_meronyms() + syn.member_meronyms() + syn.substance_meronyms():\n",
    "            for l in mer.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Coordinate terms (siblings under same hypernym)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for sibling in hyper.hyponyms():\n",
    "                if sibling == syn:\n",
    "                    continue\n",
    "                for l in sibling.lemmas():\n",
    "                    related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "    # Remove the lemma itself if present\n",
    "    related.discard(lemma)\n",
    "    return related\n",
    "\n",
    "\n",
    "def lexical_cohesion_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Lexical Cohesion (LC) for a single document:\n",
    "\n",
    "        LC = |C| / m\n",
    "\n",
    "    where:\n",
    "      - |C| is the number of cohesive devices between sentences\n",
    "        (lexical repetition + semantic relations),\n",
    "      - m  is the total number of word tokens (alphabetic) in the document.\n",
    "\n",
    "    If the document has fewer than 2 sentences or no valid words,\n",
    "    LC is returned as 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Total number of alphabetic tokens (denominator m)\n",
    "    m = sum(1 for tok in doc if tok.is_alpha)\n",
    "    if m == 0:\n",
    "        return 0.0\n",
    "\n",
    "    sentences = list(doc.sents)\n",
    "    if len(sentences) < 2:\n",
    "        # With only one sentence, cross-sentence cohesion is not defined\n",
    "        return 0.0\n",
    "\n",
    "    # Collect sets of content lemmas per sentence\n",
    "    sent_lemmas = []\n",
    "    for sent in sentences:\n",
    "        lemmas = set(\n",
    "            tok.lemma_.lower()\n",
    "            for tok in sent\n",
    "            if is_content_token(tok)\n",
    "        )\n",
    "        if lemmas:\n",
    "            sent_lemmas.append(lemmas)\n",
    "\n",
    "    if len(sent_lemmas) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    cohesive_count = 0\n",
    "\n",
    "    for i in range(len(sent_lemmas) - 1):\n",
    "        for j in range(i + 1, len(sent_lemmas)):\n",
    "            li = sent_lemmas[i]\n",
    "            lj = sent_lemmas[j]\n",
    "\n",
    "            # 1) Lexical repetition: shared lemmas\n",
    "            shared = li & lj\n",
    "            cohesive_count += len(shared)\n",
    "\n",
    "            # 2) Semantic relations via WordNet\n",
    "            for lemma in li:\n",
    "                related = get_related_lemmas(lemma)\n",
    "                cohesive_count += len(related & lj)\n",
    "\n",
    "    return float(cohesive_count) / float(m)\n",
    "\n",
    "\n",
    "def sentence_vector(sent, vector_size):\n",
    "    \"\"\"\n",
    "    Represent a sentence as the average of token vectors.\n",
    "    If no token has a vector, return a zero vector.\n",
    "    \"\"\"\n",
    "    vecs = [\n",
    "        tok.vector\n",
    "        for tok in sent\n",
    "        if tok.has_vector and not tok.is_punct and not tok.is_space\n",
    "    ]\n",
    "    if not vecs:\n",
    "        return np.zeros(vector_size, dtype=\"float32\")\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "def coherence_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Coherence (CoH) for a single document as the average\n",
    "    cosine similarity between adjacent sentence vectors:\n",
    "\n",
    "        CoH = (1 / (k-1)) * sum_{i=1}^{k-1} cos(h_i, h_{i+1})\n",
    "\n",
    "    where h_i is the sentence/topic vector for sentence i.\n",
    "\n",
    "    If the document has fewer than 2 sentences, CoH = 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    if nlp.vocab.vectors_length == 0:\n",
    "        raise ValueError(\n",
    "            \"The loaded spaCy model does not contain word vectors \"\n",
    "            \"(nlp.vocab.vectors_length == 0). \"\n",
    "            \"Use a model like 'en_core_web_md' or similar.\"\n",
    "        )\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    k = len(sentences)\n",
    "\n",
    "    if k < 2:\n",
    "        # Only one sentence: no adjacent pair, coherence = 0.0\n",
    "        return 0.0\n",
    "\n",
    "    vector_size = nlp.vocab.vectors_length\n",
    "    sent_vectors = [\n",
    "        sentence_vector(sent, vector_size)\n",
    "        for sent in sentences\n",
    "    ]\n",
    "\n",
    "    sims = []\n",
    "    for i in range(k - 1):\n",
    "        v1 = sent_vectors[i]\n",
    "        v2 = sent_vectors[i + 1]\n",
    "        norm1 = np.linalg.norm(v1)\n",
    "        norm2 = np.linalg.norm(v2)\n",
    "        denom = norm1 * norm2\n",
    "        if denom == 0.0:\n",
    "            # Skip pairs where at least one sentence vector is zero\n",
    "            continue\n",
    "        cos_sim = float(np.dot(v1, v2) / denom)\n",
    "        sims.append(cos_sim)\n",
    "\n",
    "    if not sims:\n",
    "        return 0.0\n",
    "\n",
    "    return float(np.mean(sims))\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_cohesion_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute LC for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with lemmatizer, POS tagger, etc.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of LC scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [lexical_cohesion_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_coherence_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute CoH for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with word vectors.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of CoH scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [coherence_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_discourse_measures(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute both LC and CoH for each row of a DataFrame and return\n",
    "    them in a dictionary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"LC\":  np.ndarray of lexical cohesion scores,\n",
    "            \"CoH\": np.ndarray of coherence scores\n",
    "        }\n",
    "    \"\"\"\n",
    "    lc_vec = compute_lexical_cohesion_vector(df, nlp, column=column)\n",
    "    coh_vec = compute_coherence_vector(df, nlp, column=column)\n",
    "    return {\"LC\": lc_vec, \"CoH\": coh_vec}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70f0bb",
   "metadata": {},
   "source": [
    "### Text complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c285251",
   "metadata": {},
   "source": [
    "Here we compute the complexity of each function. Note that we use a method that calculates all measures at once. However, it is advisable to compute each measure separately so that you can better handle any potential errors. For example, calculate MLTD first and save it, then LD, and so on. The following code is an example of how to compute the measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88a6a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze_text_all(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Parse a text with stanza and compute all measures (lexical + syntactic)\n",
    "    in a single pass.\n",
    "\n",
    "    Returns a dict with keys:\n",
    "        \"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\"\n",
    "    (Discourse measures LC/CoH are added later at DataFrame level, via spaCy.)\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None, \"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    lex = lexical_measures_from_doc(doc)\n",
    "    syn = syntactic_measures_from_doc(doc)\n",
    "\n",
    "    out: Dict[str, Optional[float]] = {}\n",
    "    out.update(lex)\n",
    "    out.update(syn)\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_all_complexity_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    "    spacy_nlp=None,\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute all complexity measures for each row in df[column].\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with a text column.\n",
    "    column : str, default \"text\"\n",
    "        Name of the text column.\n",
    "    lang : str, default \"en\"\n",
    "        Language code for stanza.\n",
    "    n_jobs : int, default 1\n",
    "        Number of worker processes to use.\n",
    "            - 1  : sequential execution (no multiprocessing).\n",
    "            - >1 : multiprocessing with that many workers.\n",
    "            - 0 or None : use cpu_count() workers.\n",
    "    spacy_nlp : spaCy Language, required for LC / CoH\n",
    "        Pre-loaded spaCy pipeline with:\n",
    "            - POS / lemmatizer for LC\n",
    "            - word vectors for CoH (e.g. 'en_core_web_md').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "            \"MDD\":  {index: value},\n",
    "            \"CS\":   {index: value},\n",
    "            \"LC\":   {index: value},\n",
    "            \"CoH\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    items = list(df[column].items())  # list[(index, text)]\n",
    "    total_items = len(items)\n",
    "\n",
    "    # ---- Lexical + syntactic (stanza) ----\n",
    "    for idx, text in tqdm(\n",
    "        items,\n",
    "        total=total_items,\n",
    "        desc=\"Computing lexical & syntactic complexity (sequential)\",\n",
    "    ):\n",
    "        metrics = _analyze_text_all(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "\n",
    "    # ---- Discourse measures (spaCy: LC & CoH) ----\n",
    "    if spacy_nlp is None:\n",
    "        raise ValueError(\n",
    "            \"spacy_nlp must be provided to compute LC and CoH. \"\n",
    "            \"Load a spaCy model with vectors, e.g. 'en_core_web_md', and \"\n",
    "            \"pass it as spacy_nlp=...\"\n",
    "        )\n",
    "\n",
    "    discourse = compute_discourse_measures(df, spacy_nlp, column=column)\n",
    "    lc_vec = discourse[\"LC\"]\n",
    "    coh_vec = discourse[\"CoH\"]\n",
    "\n",
    "    lc_res: Dict[Any, float] = {}\n",
    "    coh_res: Dict[Any, float] = {}\n",
    "\n",
    "    # Map arrays back to DataFrame indices\n",
    "    for i, idx in enumerate(df.index):\n",
    "        lc_res[idx] = float(lc_vec[i])\n",
    "        coh_res[idx] = float(coh_vec[i])\n",
    "\n",
    "    return {\n",
    "        \"MTLD\": mtld_res,\n",
    "        \"LD\": ld_res,\n",
    "        \"LS\": ls_res,\n",
    "        \"MDD\": mdd_res,\n",
    "        \"CS\": cs_res,\n",
    "        \"LC\": lc_res,\n",
    "        \"CoH\": coh_res,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f60f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b598e3de228e41d39b03194aea0f9b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing lexical & syntactic complexity (sequential):   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 12:02:51 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c16b196ef24ad98ecdc035d7c46a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 12:02:51 INFO: Downloaded file to C:\\Users\\loren\\stanza_resources\\resources.json\n",
      "2025-11-17 12:02:51 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-11-17 12:02:52 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "======================================\n",
      "\n",
      "2025-11-17 12:02:52 INFO: Using device: cpu\n",
      "2025-11-17 12:02:52 INFO: Loading: tokenize\n",
      "2025-11-17 12:02:53 INFO: Loading: mwt\n",
      "2025-11-17 12:02:53 INFO: Loading: pos\n",
      "2025-11-17 12:02:55 INFO: Loading: lemma\n",
      "2025-11-17 12:02:55 INFO: Loading: constituency\n",
      "2025-11-17 12:02:55 INFO: Loading: depparse\n",
      "2025-11-17 12:02:56 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All complexity measures (per row):\n",
      "{'CS': {48: 4.08,\n",
      "        73: 4.9411764705882355,\n",
      "        94: 4.552631578947368,\n",
      "        96: 5.32258064516129,\n",
      "        144: 4.1923076923076925},\n",
      " 'CoH': {48: 0.859318733215332,\n",
      "         73: 0.8645098805427551,\n",
      "         94: 0.8154566884040833,\n",
      "         96: 0.8710702061653137,\n",
      "         144: 0.8327414393424988},\n",
      " 'LC': {48: 3.3013436794281006,\n",
      "        73: 1.055384635925293,\n",
      "        94: 2.75268816947937,\n",
      "        96: 3.1626408100128174,\n",
      "        144: 1.4617284536361694},\n",
      " 'LD': {48: 0.585635359116022,\n",
      "        73: 0.5667655786350149,\n",
      "        94: 0.4896,\n",
      "        96: 0.487987987987988,\n",
      "        144: 0.49061032863849763},\n",
      " 'LS': {48: 0.22955974842767296,\n",
      "        73: 0.24607329842931938,\n",
      "        94: 0.27450980392156865,\n",
      "        96: 0.12307692307692308,\n",
      "        144: 0.2727272727272727},\n",
      " 'MDD': {48: 3.490334845568164,\n",
      "         73: 3.254310991497692,\n",
      "         94: 3.2783950568864206,\n",
      "         96: 3.4721190136018305,\n",
      "         144: 3.161831521723758},\n",
      " 'MTLD': {48: 65.14836636314944,\n",
      "          73: 84.74656188605108,\n",
      "          94: 56.99823943661972,\n",
      "          96: 66.6,\n",
      "          144: 71.99608482871126}}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Example script: load a DataFrame and compute all complexity measures.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df_example = df.sample(n=5) # We sample 5 random rows\n",
    "    # Compute all measures for Simple texts\n",
    "    metrics = compute_all_complexity_measures_df(\n",
    "        df_example,\n",
    "        column=\"Simple\", # Note that we use the column \"Simple\" for the Simple text. Use 'Complex' for the Complex text.\n",
    "        lang=\"en\",\n",
    "\n",
    "        spacy_nlp=spacy_nlp\n",
    "    )\n",
    "\n",
    "    print(\"All complexity measures (per row):\")\n",
    "    pprint(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f4165",
   "metadata": {},
   "source": [
    "Pay attention when using the function and ensure proper error handling for NaN values. As a rule, if any complexity dimension produces NaN values for a sample, that dimension must be discarded and not included in the subsequent model training or analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afebc35",
   "metadata": {},
   "source": [
    "**It is strongly recommended to implement a function that incorporates a backup strategy in case errors occur during execution (e.g., IO errors). Please note that if it is impossible to calculate a measure for at least one row (e.g., NaN value), that row must be discarded. At the end of this process, the goal is to obtain a dataframe with 16 columns. The columns should include Simple and Complex, followed by 7 columns containing the measures for the Simple text, and the final 7 columns containing the complexity measures for the Complex text (pay attention to use distinct names for the Simple and Complex columns.)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
