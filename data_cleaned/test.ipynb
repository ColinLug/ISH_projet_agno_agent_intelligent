{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ed9f53",
   "metadata": {},
   "source": [
    "# The Complexity Stress Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aafe23",
   "metadata": {},
   "source": [
    "First, install the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6403bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textcomplexity\n",
      "  Downloading textcomplexity-0.11.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textcomplexity) (3.4.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textcomplexity) (3.9.1)\n",
      "Collecting nltk-tgrep (from textcomplexity)\n",
      "  Downloading nltk_tgrep-1.0.6-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textcomplexity) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textcomplexity) (1.13.1)\n",
      "Requirement already satisfied: click in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->textcomplexity) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->textcomplexity) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->textcomplexity) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->textcomplexity) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk->textcomplexity) (0.4.6)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk-tgrep->textcomplexity) (3.2.0)\n",
      "Downloading textcomplexity-0.11.0-py3-none-any.whl (90 kB)\n",
      "Downloading nltk_tgrep-1.0.6-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: nltk-tgrep, textcomplexity\n",
      "\n",
      "   -------------------- ------------------- 1/2 [textcomplexity]\n",
      "   ---------------------------------------- 2/2 [textcomplexity]\n",
      "\n",
      "Successfully installed nltk-tgrep-1.0.6 textcomplexity-0.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading stanza-1.11.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (6.30.2)\n",
      "Requirement already satisfied: requests in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (3.4.2)\n",
      "Requirement already satisfied: tomli in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (2.2.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (2.7.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->stanza) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.13.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->stanza) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->stanza) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->stanza) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->stanza) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.13.0->stanza) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Downloading stanza-1.11.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 13.2 MB/s eta 0:00:00\n",
      "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "   ---------------------------------------- 0.0/608.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 608.4/608.4 kB 11.2 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji, stanza\n",
      "\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   ---------------------------------------- 2/2 [stanza]\n",
      "\n",
      "Successfully installed emoji-2.15.0 stanza-1.11.0\n",
      "Collecting wordfreq"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading wordfreq-3.1.1-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ftfy>=6.1 (from wordfreq)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting langcodes>=3.0 (from wordfreq)\n",
      "  Downloading langcodes-3.5.1-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting locate<2.0.0,>=1.1.1 (from wordfreq)\n",
      "  Downloading locate-1.1.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.7 (from wordfreq)\n",
      "  Downloading msgpack-1.1.2-cp310-cp310-win_amd64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: regex>=2023.10.3 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wordfreq) (2024.11.6)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
      "Downloading wordfreq-3.1.1-py3-none-any.whl (56.8 MB)\n",
      "   ---------------------------------------- 0.0/56.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 7.6/56.8 MB 52.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 23.6/56.8 MB 64.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 37.5/56.8 MB 66.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 48.5/56.8 MB 60.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 56.8/56.8 MB 28.1 MB/s eta 0:00:00\n",
      "Downloading locate-1.1.1-py3-none-any.whl (5.4 kB)\n",
      "Downloading msgpack-1.1.2-cp310-cp310-win_amd64.whl (71 kB)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading langcodes-3.5.1-py3-none-any.whl (183 kB)\n",
      "Installing collected packages: msgpack, locate, langcodes, ftfy, wordfreq\n",
      "\n",
      "   ---------------- ----------------------- 2/5 [langcodes]\n",
      "   ------------------------ --------------- 3/5 [ftfy]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   ---------------------------------------- 5/5 [wordfreq]\n",
      "\n",
      "Successfully installed ftfy-6.3.1 langcodes-3.5.1 locate-1.1.1 msgpack-1.1.2 wordfreq-3.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python310\\python.exe: No module named spacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.67.1)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp310-cp310-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp310-cp310-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (80.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp310-cp310-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Downloading spacy-3.8.11-cp310-cp310-win_amd64.whl (15.3 MB)\n",
      "   ---------------------------------------- 0.0/15.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 8.4/15.3 MB 52.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.2/15.3 MB 59.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.3/15.3 MB 35.7 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp310-cp310-win_amd64.whl (40 kB)\n",
      "Downloading murmurhash-1.0.15-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.12-cp310-cp310-win_amd64.whl (117 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp310-cp310-win_amd64.whl (654 kB)\n",
      "   ---------------------------------------- 0.0/654.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 654.0/654.0 kB 12.6 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.10-cp310-cp310-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 24.3 MB/s eta 0:00:00\n",
      "Downloading blis-1.3.3-cp310-cp310-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 38.0 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Installing collected packages: wasabi, spacy-loggers, spacy-legacy, murmurhash, cymem, cloudpathlib, catalogue, blis, typer-slim, srsly, preshed, confection, weasel, thinc, spacy\n",
      "\n",
      "   ----------------------------------------  0/15 [wasabi]\n",
      "   -- -------------------------------------  1/15 [spacy-loggers]\n",
      "   ----- ----------------------------------  2/15 [spacy-legacy]\n",
      "   ---------- -----------------------------  4/15 [cymem]\n",
      "   ------------- --------------------------  5/15 [cloudpathlib]\n",
      "   ---------------- -----------------------  6/15 [catalogue]\n",
      "   ------------------ ---------------------  7/15 [blis]\n",
      "   --------------------- ------------------  8/15 [typer-slim]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ----------------------------- ---------- 11/15 [confection]\n",
      "   ----------------------------- ---------- 11/15 [confection]\n",
      "   -------------------------------- ------- 12/15 [weasel]\n",
      "   -------------------------------- ------- 12/15 [weasel]\n",
      "   -------------------------------- ------- 12/15 [weasel]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ---------------------------------------- 15/15 [spacy]\n",
      "\n",
      "Successfully installed blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 typer-slim-0.20.0 wasabi-1.1.3 weasel-0.4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install textcomplexity\n",
    "!{sys.executable} -m pip install stanza\n",
    "!{sys.executable} -m pip install wordfreq \n",
    "!{sys.executable} -m spacy download en_core_web_md\n",
    "!{sys.executable} -m pip install tqdm spacy numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625ec67",
   "metadata": {},
   "source": [
    "First, import the following Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6789f29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 41.9MB/s]                    \n",
      "2025-12-14 19:21:44 INFO: Downloaded file to C:\\Users\\rroll\\stanza_resources\\resources.json\n",
      "2025-12-14 19:21:44 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-12-14 19:21:47 INFO: File exists: C:\\Users\\rroll\\stanza_resources\\en\\default.zip\n",
      "2025-12-14 19:21:54 INFO: Finished downloading models and saved to C:\\Users\\rroll\\stanza_resources\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rroll\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rroll\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1d5c9a86f40>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "from pprint import pprint\n",
    "from typing import Dict, Set, Iterable, Optional, Any, Tuple\n",
    "import importlib.resources as pkg_resources\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "import stanza\n",
    "import textcomplexity  # only used to access en.json\n",
    "from tqdm.auto import tqdm  \n",
    "\n",
    "# Download required resources\n",
    "stanza.download('en')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Make sure WordNet is available; if not, download it.\n",
    "try:\n",
    "    _ = wn.synsets(\"dog\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "spacy_nlp = nlp\n",
    "spacy_nlp.add_pipe(\"sentencizer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57283f",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eef7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets ={'ose_adv_ele':'data_sampled/OSE_adv_ele.txt', \n",
    "           'ose_adv_int':'data_sampled/OSE_adv_int.txt',\n",
    "           'swipe': 'data_sampled/swipe.txt',\n",
    "           'vikidia':'data_sampled/vikidia.txt'}\n",
    "\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path, sep='\\t')\n",
    "    \n",
    "\n",
    "def load_dataset(name):\n",
    "    if name not in datasets:\n",
    "        raise ValueError(f\"Dataset {name} not found\")\n",
    "    return load_data(datasets[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c93eb1",
   "metadata": {},
   "source": [
    "Let's load one of the datasets, in this case \"ose_adv_ele\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae91d38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Simple</th>\n",
       "      <th>Complex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿To tourists, Amsterdam still seems very liber...</td>\n",
       "      <td>﻿Amsterdam still looks liberal to tourists, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>﻿Anitta, a music star from Brazil, has million...</td>\n",
       "      <td>﻿Brazil’s latest funk sensation, Anitta, has w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Simple  \\\n",
       "0  ﻿When you see the word Amazon, what’s the firs...   \n",
       "1  ﻿To tourists, Amsterdam still seems very liber...   \n",
       "2  ﻿Anitta, a music star from Brazil, has million...   \n",
       "\n",
       "                                             Complex  \n",
       "0  ﻿When you see the word Amazon, what’s the firs...  \n",
       "1  ﻿Amsterdam still looks liberal to tourists, wh...  \n",
       "2  ﻿Brazil’s latest funk sensation, Anitta, has w...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('ose_adv_ele')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb2dc3",
   "metadata": {},
   "source": [
    "Let's look at a random row of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7bb885b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE TEXT\n",
      "﻿The European Parliament have said that health warnings will cover nearly two-thirds of cigarette packs and there will be a ban on menthol cigarettes in the EU.\n",
      "The EU will ban menthol and other flavours from 2022. MEPs also decided that most electronic cigarettes, which are more and more popular as alternatives to normal cigarettes, do not to be need regulated in the same way as medicines.\n",
      "The Department of Health and e-cigarette companies in Britain want to find out exactly what this means – for example, will e-cigarette companies be banned from advertising at sports events?\n",
      "The Department of Health said: “We are very pleased to see tougher action on smoking, with European controls banning flavoured cigarettes and the introduction of stricter rules on health warnings on cigarette packs.\n",
      "“But we are disappointed with the decision not to regulate nicotine-containing products (NCPs), including e-cigarettes, as medicines. We believe these products need to be regulated as medicines.\n",
      "“Smoking levels in England are at their lowest since records began – 19.5 per cent – but we want to reduce the numbers of people smoking even more and believe this important step will help.”\n",
      "UK e-cigarette companies, who were happy with the parliament’s vote, said they were already in talks with the Advertising Standards Authority. But they said that it would not be a good idea to ban all advertising.\n",
      "MEPs decided e-cigarettes should only be regulated as medical products if the e-cigarette companies said they could stop people from smoking.\n",
      "Other groups want e-cigarettes, used by about 1.3 million people in Britain, to be regulated in the same way as gums, patches and mouth sprays, which are aimed at helping smokers to quit.\n",
      "The MEPs voted to put health warnings on 65% of each cigarette pack. At the moment, the warnings cover at least 30% on the front and 40% on the back. The UK government has not decided if they will do the same as Australia and introduce standardized packaging. First, they want to know that this will stop people from smoking.\n",
      "The MEPs’ decision about the bigger health warnings on the packaging could become law in 2014.\n",
      "“The UK continues to believe that medicinal regulation of NCPs is best for public health,” said the Medicines and Healthcare Products Regulatory Authority.\n",
      "Linda McAvan, Labour MEP for Yorkshire and the Humber, said: “We know that it is children, not adults, who start smoking. There are fewer and fewer adult smokers in most EU countries, but there are more young smokers.”\n",
      "Martin Callanan, the Conservative MEP for North East England, said that banning e-cigarettes would have been totally crazy. “These are products that have helped many people stop smoking more harmful cigarettes.”\n",
      "British American Tobacco said the bigger health warnings were not necessary and that a ban on menthol cigarettes would make more people want to buy from the black market.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "COMPLEX TEXT\n",
      "﻿Health warnings covering nearly two-thirds of cigarette packs and a ban on menthol cigarettes across the EU have come a step nearer following a vote in the European Parliament. Menthol and other flavours will be banned from 2022, but, in a blow to the UK government, MEPs decided that most electronic cigarettes, increasingly popular as alternatives to tobacco products, need not be regulated in the same way as medicines.\n",
      "Health officials and the e-cigarette industry in Britain are seeking to clarify what this mean – for instance, whether companies in the fast-expanding market face the same bans on sponsorship and promotion at sports events as tobacco firms.\n",
      "The Department of Health would not comment on the advertising issue until officials had studied the MEPs’ decisions. But, in a statement, the DH said: “We are very pleased to see the move towards tougher action on tobacco, with Europe-wide controls banning flavoured cigarettes and the introduction of stricter rules on front-of-pack health warnings. “However, we are disappointed with the decision to reject the proposal to regulate nicotine-containing products (NCPs), including e-cigarettes, as medicines. We believe these products need to be regulated as medicines and will continue to make this point during further negotiations.\n",
      "“Figures show smoking levels in England are at their lowest since records began – 19.5 per cent – but we are determined to further reduce rates of smoking and believe this important step will help.”\n",
      "The UK e-cigarette industry, which broadly welcomed the parliament’s vote, said it was already in talks with the Advertising Standards Authority, but added that it would not be “sensible, proportionate, reasonable or useful” to ban all advertising.\n",
      "MEPs decided e-cigarettes should only be regulated as medical products if manufacturers claimed they could prevent tobacco smoking – a decision criticized by the government’s main medicines regulator.\n",
      "They want to put the products, used by an estimated 1.3 million people in Britain by 2014, on the same legal basis as gums, patches and mouth sprays aimed at helping smokers to quit, but the industry says the expensive process of licensing would help force alternatives to tobacco off the shelves.\n",
      "The MEPs voted to put health warnings on 65% of each cigarette pack, as opposed to a proposed 75%. At present, the warnings cover at least 30% on the front and 40% on the back. The UK government has delayed a decision on whether to follow Australia by introducing standardized packaging until there is evidence that such measures cut tobacco use.\n",
      "The MEPs’ votes in the first reading of the draft tobacco directive, which could become law in 2014, will be followed by negotiations with the EU Council of Ministers.\n",
      "The Medicines and Healthcare Products Regulatory Authority had already invited manufacturers to cooperate by opting for voluntary regulation in June 2013 in advance of what it still hopes will be compulsory across Europe. “The legislative process is still not complete and there will be further negotiation. The UK continues to believe that medicinal regulation of NCPs is the best way to deliver a benefit to public health,” said a spokesman.\n",
      "Linda McAvan, Labour MEP for Yorkshire and the Humber and spokesperson on tobacco issues for the parliament’s Socialists & Democrats group, said: “We know that it is children, not adults, who start smoking. And, despite the downward trend in most member states of adult smokers, the World Health Organization figures show worrying upward trends in a number of our member states of young smokers.\n",
      "“We need to stop tobacco companies targeting young people with an array of gimmicky products and we need to make sure that cigarette packs carry effective warnings.”\n",
      "Martin Callanan, the Conservative MEP for North East England, said: “Forcing e-cigs off the shelves would have been totally crazy. These are products that have helped countless people stop smoking more harmful cigarettes and yet some MEPs wanted to make them harder to manufacture than ordinary tobacco.”\n",
      "Katherine Devlin, president of ECITA, the e-cigarette industry association, said “the really important” decision by MEPs not to support medicines regulation meant that was now off the table.\n",
      "British American Tobacco claimed the larger health warnings demanded by MEPs went “well beyond” what was needed to inform consumers of health risks from smoking, while a ban on mentholated cigarettes would increase demand for black-market goods.\n"
     ]
    }
   ],
   "source": [
    "row = df.sample(1)\n",
    "\n",
    "print('SIMPLE TEXT')\n",
    "print(row['Simple'].iloc[0])\n",
    "print('-'*100)\n",
    "print('COMPLEX TEXT')\n",
    "print(row['Complex'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ebbb2",
   "metadata": {},
   "source": [
    "Let's look at the size of each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72529061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ose_adv_ele: 189 rows\n",
      "ose_adv_int: 189 rows\n",
      "swipe: 1233 rows\n",
      "vikidia: 1233 rows\n",
      "Total: 2844 rows\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for name, path in datasets.items():\n",
    "    df = load_dataset(name)\n",
    "    print(f\"{name}: {df.shape[0]} rows\")\n",
    "    cnt += df.shape[0]\n",
    "print(f\"Total: {cnt} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571b7484",
   "metadata": {},
   "source": [
    "Let's load again the dataset for computing the complexity measure in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b7e233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset('ose_adv_ele')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd81ace0",
   "metadata": {},
   "source": [
    "## Complexity measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d9ff4",
   "metadata": {},
   "source": [
    "We provide the function for computing the complexity measures with respect to the lexical, syntactic, and discourse dimensions. It is worth noticing that complexity functions on the datasets should be computed offline beforehand and the results saved, as they are computationally expensive. Specifically, for both the Simple and Complex texts, the complexity measures should be calculated first and stored (e.g., in Pandas DataFrames with the new complexity feature columns added) before feeding the augmented datasets to the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0d91084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cache stanza pipelines to avoid re-loading models\n",
    "_STANZA_PIPELINES: Dict[str, stanza.Pipeline] = {}\n",
    "\n",
    "# UPOS tags considered content words (C)\n",
    "CONTENT_UPOS = {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def load_cow_top5000_en() -> Set[str]:\n",
    "    \"\"\"\n",
    "    Load the COW-based list of the 5,000 most frequent English content words\n",
    "    from textcomplexity's English language definition file (en.json).\n",
    "\n",
    "    We ignore POS tags and keep only lowercased word forms.\n",
    "    \"\"\"\n",
    "    with pkg_resources.files(textcomplexity).joinpath(\"en.json\").open(\n",
    "        \"r\", encoding=\"utf-8\"\n",
    "    ) as f:\n",
    "        lang_def = json.load(f)\n",
    "\n",
    "    most_common = lang_def[\"most_common\"]  # list of [word, xpos]\n",
    "    cow_top5000 = {w.lower() for w, xpos in most_common}\n",
    "    return cow_top5000\n",
    "\n",
    "\n",
    "def get_stanza_pipeline(lang: str = \"en\", use_gpu: bool = False) -> stanza.Pipeline:\n",
    "    \"\"\"\n",
    "    Get (or create) a cached stanza Pipeline for a given language.\n",
    "\n",
    "    NOTE: You must have downloaded the models beforehand, e.g.:\n",
    "        import stanza\n",
    "        stanza.download('en')\n",
    "    \"\"\"\n",
    "    if lang not in _STANZA_PIPELINES:\n",
    "        _STANZA_PIPELINES[lang] = stanza.Pipeline(\n",
    "            lang=lang,\n",
    "            processors=\"tokenize,pos,lemma,depparse,constituency\",\n",
    "            use_gpu=use_gpu,\n",
    "            tokenize_no_ssplit=False,\n",
    "        )\n",
    "    return _STANZA_PIPELINES[lang]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a53cb",
   "metadata": {},
   "source": [
    "### Lexical complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "929089ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_mtld(tokens: Iterable[str], ttr_threshold: float = 0.72) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute MTLD (Measure of Textual Lexical Diversity) for a list of tokens.\n",
    "\n",
    "    MTLD = total_number_of_tokens / number_of_factors\n",
    "\n",
    "    A factor is a contiguous segment where the running TTR stays >= threshold.\n",
    "    When the TTR drops below the threshold, we close a factor (at the previous\n",
    "    token) and start a new one. At the end, the remaining partial segment is\n",
    "    counted as a fractional factor, with weight proportional to how close the\n",
    "    final TTR is to the threshold.\n",
    "    \"\"\"\n",
    "    tokens = [tok for tok in tokens if tok]\n",
    "    if not tokens:\n",
    "        return None\n",
    "\n",
    "    types = set()\n",
    "    factor_count = 0.0\n",
    "    token_count_in_factor = 0\n",
    "\n",
    "    for tok in tokens:\n",
    "        token_count_in_factor += 1\n",
    "        types.add(tok)\n",
    "        ttr = len(types) / token_count_in_factor\n",
    "\n",
    "        if ttr < ttr_threshold:\n",
    "            factor_count += 1.0\n",
    "            types = set()\n",
    "            token_count_in_factor = 0\n",
    "\n",
    "    # final partial factor\n",
    "    if token_count_in_factor > 0:\n",
    "        final_ttr = len(types) / token_count_in_factor\n",
    "        if final_ttr < 1.0:\n",
    "            fractional = (1.0 - final_ttr) / (1.0 - ttr_threshold)\n",
    "            fractional = max(0.0, min(1.0, fractional))\n",
    "            factor_count += fractional\n",
    "\n",
    "    if factor_count == 0:\n",
    "        return None\n",
    "\n",
    "    return len(tokens) / factor_count\n",
    "\n",
    "\n",
    "\n",
    "def _compute_lexical_density(total_tokens: int, content_tokens: int) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LD = |C| / |T|\n",
    "    where:\n",
    "        |C| = number of content-word tokens\n",
    "        |T| = total number of non-punctuation tokens\n",
    "    \"\"\"\n",
    "    if total_tokens == 0:\n",
    "        return None\n",
    "    return content_tokens / total_tokens\n",
    "\n",
    "\n",
    "def _compute_lexical_sophistication_cow(\n",
    "    content_forms: Iterable[str],\n",
    "    cow_top5000: set,\n",
    ") -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LS = |{ w in C : w not in R }| / |C|\n",
    "    where:\n",
    "        C = content-word tokens (surface forms, lowercased)\n",
    "        R = COW top-5000 content word forms (lowercased)\n",
    "    \"\"\"\n",
    "    forms = [f for f in content_forms if f]\n",
    "    if not forms:\n",
    "        return None\n",
    "\n",
    "    off_list = sum(1 for f in forms if f not in cow_top5000)\n",
    "    return off_list / len(forms)\n",
    "\n",
    "\n",
    "\n",
    "def lexical_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MTLD, LD, LS from a stanza Document.\n",
    "    \"\"\"\n",
    "    cow_top5000 = load_cow_top5000_en()\n",
    "\n",
    "    mtld_tokens = []\n",
    "    total_tokens = 0\n",
    "    content_tokens = 0\n",
    "    content_forms = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.upos == \"PUNCT\":\n",
    "                continue\n",
    "\n",
    "            lemma = (word.lemma or word.text or \"\").lower()\n",
    "            if not lemma:\n",
    "                continue\n",
    "\n",
    "            mtld_tokens.append(lemma)\n",
    "            total_tokens += 1\n",
    "\n",
    "            if word.upos in CONTENT_UPOS:\n",
    "                content_tokens += 1\n",
    "                form = (word.text or \"\").lower()\n",
    "                content_forms.append(form)\n",
    "\n",
    "    mtld = _compute_mtld(mtld_tokens) if mtld_tokens else None\n",
    "    ld = _compute_lexical_density(total_tokens, content_tokens)\n",
    "    ls = _compute_lexical_sophistication_cow(content_forms, cow_top5000)\n",
    "\n",
    "    return {\"MTLD\": mtld, \"LD\": ld, \"LS\": ls}\n",
    "\n",
    "\n",
    "def lexical_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute lexical measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return lexical_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute lexical measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = lexical_measures_from_text(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "\n",
    "    return {\"MTLD\": mtld_res, \"LD\": ld_res, \"LS\": ls_res}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4583f900",
   "metadata": {},
   "source": [
    "### Syntactic complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1f6ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mdd_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute Mean Dependency Distance (MDD) from a stanza Document.\n",
    "\n",
    "    For each sentence s_i with dependency set D_i:\n",
    "        MDD_i = (1 / |D_i|) * sum_{(h,d) in D_i} |h - d|\n",
    "    Then:\n",
    "        MDD = (1 / k) * sum_i MDD_i, over all sentences with at least one dependency.\n",
    "    \"\"\"\n",
    "    sentence_mdds = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        distances = []\n",
    "        for w in sent.words:\n",
    "            if w.head is None or w.head == 0:\n",
    "                continue\n",
    "            distances.append(abs(w.id - w.head))\n",
    "\n",
    "        if distances:\n",
    "            sentence_mdds.append(sum(distances) / len(distances))\n",
    "\n",
    "    if not sentence_mdds:\n",
    "        return None\n",
    "    return sum(sentence_mdds) / len(sentence_mdds)\n",
    "\n",
    "\n",
    "\n",
    "def _count_clauses_in_tree(tree) -> int:\n",
    "    \"\"\"\n",
    "    Count clause nodes in a constituency tree.\n",
    "\n",
    "    A simple and standard heuristic (PTB-style) is:\n",
    "        count all nodes whose label starts with 'S'\n",
    "        (S, SBAR, SBARQ, SINV, SQ, etc.).\n",
    "\n",
    "    This aligns with the idea of counting finite and subordinate clauses\n",
    "    as in Hunt (1965) and later complexity work.\n",
    "    \"\"\"\n",
    "    if tree is None:\n",
    "        return 0\n",
    "\n",
    "    # Stanza's constituency tree: tree.label, tree.children\n",
    "    count = 1 if getattr(tree, \"label\", \"\").startswith(\"S\") else 0\n",
    "\n",
    "    for child in getattr(tree, \"children\", []):\n",
    "        # leaves can be strings or terminals without 'label'\n",
    "        if hasattr(child, \"label\"):\n",
    "            count += _count_clauses_in_tree(child)\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def cs_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute CS (clauses per sentence) from a stanza Document.\n",
    "\n",
    "        CS = (1 / k) * sum_i L_i\n",
    "\n",
    "    where L_i is the number of clauses in sentence s_i, estimated by counting\n",
    "    all constituents whose label starts with 'S' in the constituency tree of s_i.\n",
    "    \"\"\"\n",
    "    clause_counts = []\n",
    "    for sent in doc.sentences:\n",
    "        tree = getattr(sent, \"constituency\", None)\n",
    "        if tree is None:\n",
    "            # No constituency tree available for this sentence\n",
    "            continue\n",
    "        num_clauses = _count_clauses_in_tree(tree)\n",
    "        clause_counts.append(num_clauses)\n",
    "\n",
    "    if not clause_counts:\n",
    "        return None\n",
    "\n",
    "    return sum(clause_counts) / len(clause_counts)\n",
    "\n",
    "\n",
    "\n",
    "def syntactic_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MDD and CS from a stanza Document.\n",
    "    \"\"\"\n",
    "    mdd = mdd_from_doc(doc)\n",
    "    cs = cs_from_doc(doc)\n",
    "    return {\"MDD\": mdd, \"CS\": cs}\n",
    "\n",
    "\n",
    "def syntactic_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute syntactic measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return syntactic_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "def compute_syntactic_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute syntactic measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MDD\": {index: value},\n",
    "            \"CS\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = syntactic_measures_from_text(text, lang=lang)\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "    return {\"MDD\": mdd_res, \"CS\": cs_res}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20110b8",
   "metadata": {},
   "source": [
    "### Discourse complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "739a62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Approximate set of content POS tags (spaCy universal POS)\n",
    "CONTENT_POS =  {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "def is_content_token(tok):\n",
    "    \"\"\"\n",
    "    Return True if token is considered a content word.\n",
    "    We ignore stopwords, punctuation, and non-alphabetic tokens.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        tok.is_alpha\n",
    "        and not tok.is_stop\n",
    "        and tok.pos_ in CONTENT_POS\n",
    "    )\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=100000)\n",
    "def get_related_lemmas(lemma):\n",
    "    \"\"\"\n",
    "    Return a set of semantically related lemmas for the given lemma\n",
    "    using WordNet, including:\n",
    "      - synonyms\n",
    "      - antonyms\n",
    "      - hypernyms / hyponyms\n",
    "      - meronyms (part/member/substance)\n",
    "      - coordinate terms (siblings under the same hypernym)\n",
    "\n",
    "    NOTE: Some older examples mention 'troponyms', but in NLTK's\n",
    "    WordNet interface there is no 'troponyms()' method on Synset,\n",
    "    so we do NOT use it here.\n",
    "    \"\"\"\n",
    "    lemma = lemma.lower()\n",
    "    related = set()\n",
    "    synsets = wn.synsets(lemma)\n",
    "\n",
    "    for syn in synsets:\n",
    "        # Synonyms and antonyms\n",
    "        for l in syn.lemmas():\n",
    "            related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "            for ant in l.antonyms():\n",
    "                related.add(ant.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Hypernyms (more general) and hyponyms (more specific)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for l in hyper.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "        for hypo in syn.hyponyms():\n",
    "            for l in hypo.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Meronyms: part/member/substance\n",
    "        for mer in syn.part_meronyms() + syn.member_meronyms() + syn.substance_meronyms():\n",
    "            for l in mer.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Coordinate terms (siblings under same hypernym)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for sibling in hyper.hyponyms():\n",
    "                if sibling == syn:\n",
    "                    continue\n",
    "                for l in sibling.lemmas():\n",
    "                    related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "    # Remove the lemma itself if present\n",
    "    related.discard(lemma)\n",
    "    return related\n",
    "\n",
    "\n",
    "def lexical_cohesion_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Lexical Cohesion (LC) for a single document:\n",
    "\n",
    "        LC = |C| / m\n",
    "\n",
    "    where:\n",
    "      - |C| is the number of cohesive devices between sentences\n",
    "        (lexical repetition + semantic relations),\n",
    "      - m  is the total number of word tokens (alphabetic) in the document.\n",
    "\n",
    "    If the document has fewer than 2 sentences or no valid words,\n",
    "    LC is returned as 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Total number of alphabetic tokens (denominator m)\n",
    "    m = sum(1 for tok in doc if tok.is_alpha)\n",
    "    if m == 0:\n",
    "        return 0.0\n",
    "\n",
    "    sentences = list(doc.sents)\n",
    "    if len(sentences) < 2:\n",
    "        # With only one sentence, cross-sentence cohesion is not defined\n",
    "        return 0.0\n",
    "\n",
    "    # Collect sets of content lemmas per sentence\n",
    "    sent_lemmas = []\n",
    "    for sent in sentences:\n",
    "        lemmas = set(\n",
    "            tok.lemma_.lower()\n",
    "            for tok in sent\n",
    "            if is_content_token(tok)\n",
    "        )\n",
    "        if lemmas:\n",
    "            sent_lemmas.append(lemmas)\n",
    "\n",
    "    if len(sent_lemmas) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    cohesive_count = 0\n",
    "\n",
    "    for i in range(len(sent_lemmas) - 1):\n",
    "        for j in range(i + 1, len(sent_lemmas)):\n",
    "            li = sent_lemmas[i]\n",
    "            lj = sent_lemmas[j]\n",
    "\n",
    "            # 1) Lexical repetition: shared lemmas\n",
    "            shared = li & lj\n",
    "            cohesive_count += len(shared)\n",
    "\n",
    "            # 2) Semantic relations via WordNet\n",
    "            for lemma in li:\n",
    "                related = get_related_lemmas(lemma)\n",
    "                cohesive_count += len(related & lj)\n",
    "\n",
    "    return float(cohesive_count) / float(m)\n",
    "\n",
    "\n",
    "def sentence_vector(sent, vector_size):\n",
    "    \"\"\"\n",
    "    Represent a sentence as the average of token vectors.\n",
    "    If no token has a vector, return a zero vector.\n",
    "    \"\"\"\n",
    "    vecs = [\n",
    "        tok.vector\n",
    "        for tok in sent\n",
    "        if tok.has_vector and not tok.is_punct and not tok.is_space\n",
    "    ]\n",
    "    if not vecs:\n",
    "        return np.zeros(vector_size, dtype=\"float32\")\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "def coherence_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Coherence (CoH) for a single document as the average\n",
    "    cosine similarity between adjacent sentence vectors:\n",
    "\n",
    "        CoH = (1 / (k-1)) * sum_{i=1}^{k-1} cos(h_i, h_{i+1})\n",
    "\n",
    "    where h_i is the sentence/topic vector for sentence i.\n",
    "\n",
    "    If the document has fewer than 2 sentences, CoH = 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    if nlp.vocab.vectors_length == 0:\n",
    "        raise ValueError(\n",
    "            \"The loaded spaCy model does not contain word vectors \"\n",
    "            \"(nlp.vocab.vectors_length == 0). \"\n",
    "            \"Use a model like 'en_core_web_md' or similar.\"\n",
    "        )\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    k = len(sentences)\n",
    "\n",
    "    if k < 2:\n",
    "        # Only one sentence: no adjacent pair, coherence = 0.0\n",
    "        return 0.0\n",
    "\n",
    "    vector_size = nlp.vocab.vectors_length\n",
    "    sent_vectors = [\n",
    "        sentence_vector(sent, vector_size)\n",
    "        for sent in sentences\n",
    "    ]\n",
    "\n",
    "    sims = []\n",
    "    for i in range(k - 1):\n",
    "        v1 = sent_vectors[i]\n",
    "        v2 = sent_vectors[i + 1]\n",
    "        norm1 = np.linalg.norm(v1)\n",
    "        norm2 = np.linalg.norm(v2)\n",
    "        denom = norm1 * norm2\n",
    "        if denom == 0.0:\n",
    "            # Skip pairs where at least one sentence vector is zero\n",
    "            continue\n",
    "        cos_sim = float(np.dot(v1, v2) / denom)\n",
    "        sims.append(cos_sim)\n",
    "\n",
    "    if not sims:\n",
    "        return 0.0\n",
    "\n",
    "    return float(np.mean(sims))\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_cohesion_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute LC for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with lemmatizer, POS tagger, etc.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of LC scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [lexical_cohesion_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_coherence_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute CoH for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with word vectors.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of CoH scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [coherence_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_discourse_measures(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute both LC and CoH for each row of a DataFrame and return\n",
    "    them in a dictionary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"LC\":  np.ndarray of lexical cohesion scores,\n",
    "            \"CoH\": np.ndarray of coherence scores\n",
    "        }\n",
    "    \"\"\"\n",
    "    lc_vec = compute_lexical_cohesion_vector(df, nlp, column=column)\n",
    "    coh_vec = compute_coherence_vector(df, nlp, column=column)\n",
    "    return {\"LC\": lc_vec, \"CoH\": coh_vec}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70f0bb",
   "metadata": {},
   "source": [
    "### Text complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c285251",
   "metadata": {},
   "source": [
    "Here we compute the complexity of each function. Note that we use a method that calculates all measures at once. However, it is advisable to compute each measure separately so that you can better handle any potential errors. For example, calculate MLTD first and save it, then LD, and so on. The following code is an example of how to compute the measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88a6a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze_text_all(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Parse a text with stanza and compute all measures (lexical + syntactic)\n",
    "    in a single pass.\n",
    "\n",
    "    Returns a dict with keys:\n",
    "        \"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\"\n",
    "    (Discourse measures LC/CoH are added later at DataFrame level, via spaCy.)\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None, \"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    lex = lexical_measures_from_doc(doc)\n",
    "    syn = syntactic_measures_from_doc(doc)\n",
    "\n",
    "    out: Dict[str, Optional[float]] = {}\n",
    "    out.update(lex)\n",
    "    out.update(syn)\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_all_complexity_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    "    spacy_nlp=None,\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute all complexity measures for each row in df[column].\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with a text column.\n",
    "    column : str, default \"text\"\n",
    "        Name of the text column.\n",
    "    lang : str, default \"en\"\n",
    "        Language code for stanza.\n",
    "    n_jobs : int, default 1\n",
    "        Number of worker processes to use.\n",
    "            - 1  : sequential execution (no multiprocessing).\n",
    "            - >1 : multiprocessing with that many workers.\n",
    "            - 0 or None : use cpu_count() workers.\n",
    "    spacy_nlp : spaCy Language, required for LC / CoH\n",
    "        Pre-loaded spaCy pipeline with:\n",
    "            - POS / lemmatizer for LC\n",
    "            - word vectors for CoH (e.g. 'en_core_web_md').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "            \"MDD\":  {index: value},\n",
    "            \"CS\":   {index: value},\n",
    "            \"LC\":   {index: value},\n",
    "            \"CoH\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    items = list(df[column].items())  # list[(index, text)]\n",
    "    total_items = len(items)\n",
    "\n",
    "    # ---- Lexical + syntactic (stanza) ----\n",
    "    for idx, text in tqdm(\n",
    "        items,\n",
    "        total=total_items,\n",
    "        desc=\"Computing lexical & syntactic complexity (sequential)\",\n",
    "    ):\n",
    "        metrics = _analyze_text_all(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "\n",
    "    # ---- Discourse measures (spaCy: LC & CoH) ----\n",
    "    if spacy_nlp is None:\n",
    "        raise ValueError(\n",
    "            \"spacy_nlp must be provided to compute LC and CoH. \"\n",
    "            \"Load a spaCy model with vectors, e.g. 'en_core_web_md', and \"\n",
    "            \"pass it as spacy_nlp=...\"\n",
    "        )\n",
    "\n",
    "    discourse = compute_discourse_measures(df, spacy_nlp, column=column)\n",
    "    lc_vec = discourse[\"LC\"]\n",
    "    coh_vec = discourse[\"CoH\"]\n",
    "\n",
    "    lc_res: Dict[Any, float] = {}\n",
    "    coh_res: Dict[Any, float] = {}\n",
    "\n",
    "    # Map arrays back to DataFrame indices\n",
    "    for i, idx in enumerate(df.index):\n",
    "        lc_res[idx] = float(lc_vec[i])\n",
    "        coh_res[idx] = float(coh_vec[i])\n",
    "\n",
    "    return {\n",
    "        \"MTLD\": mtld_res,\n",
    "        \"LD\": ld_res,\n",
    "        \"LS\": ls_res,\n",
    "        \"MDD\": mdd_res,\n",
    "        \"CS\": cs_res,\n",
    "        \"LC\": lc_res,\n",
    "        \"CoH\": coh_res,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "914f60f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing lexical & syntactic complexity (sequential):   0%|          | 0/5 [00:00<?, ?it/s]2025-12-14 19:28:14 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 43.6MB/s]                    \n",
      "2025-12-14 19:28:14 INFO: Downloaded file to C:\\Users\\rroll\\stanza_resources\\resources.json\n",
      "2025-12-14 19:28:14 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-12-14 19:28:16 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "======================================\n",
      "\n",
      "2025-12-14 19:28:16 INFO: Using device: cpu\n",
      "2025-12-14 19:28:16 INFO: Loading: tokenize\n",
      "2025-12-14 19:28:20 INFO: Loading: mwt\n",
      "2025-12-14 19:28:20 INFO: Loading: pos\n",
      "2025-12-14 19:28:25 INFO: Loading: lemma\n",
      "2025-12-14 19:28:26 INFO: Loading: constituency\n",
      "2025-12-14 19:28:27 INFO: Loading: depparse\n",
      "2025-12-14 19:28:28 INFO: Done loading processors!\n",
      "Computing lexical & syntactic complexity (sequential): 100%|██████████| 5/5 [01:19<00:00, 16.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All complexity measures (per row):\n",
      "{'CS': {8: 4.0,\n",
      "        18: 2.6923076923076925,\n",
      "        54: 3.1707317073170733,\n",
      "        166: 2.275,\n",
      "        188: 3.5217391304347827},\n",
      " 'CoH': {8: 0.8728904128074646,\n",
      "         18: 0.8116506934165955,\n",
      "         54: 0.8372233510017395,\n",
      "         166: 0.7928222417831421,\n",
      "         188: 0.8636001944541931},\n",
      " 'LC': {8: 2.5833332538604736,\n",
      "        18: 1.3486238718032837,\n",
      "        54: 3.4348485469818115,\n",
      "        166: 1.627368450164795,\n",
      "        188: 2.472766876220703},\n",
      " 'LD': {8: 0.5510688836104513,\n",
      "        18: 0.5404255319148936,\n",
      "        54: 0.5331369661266568,\n",
      "        166: 0.5266393442622951,\n",
      "        188: 0.5390946502057613},\n",
      " 'LS': {8: 0.16810344827586207,\n",
      "        18: 0.23622047244094488,\n",
      "        54: 0.30939226519337015,\n",
      "        166: 0.3424124513618677,\n",
      "        188: 0.14122137404580154},\n",
      " 'MDD': {8: 3.1878925541588297,\n",
      "         18: 3.072799586796078,\n",
      "         54: 3.212641674453416,\n",
      "         166: 2.7180534336020017,\n",
      "         188: 3.4891918254223446},\n",
      " 'MTLD': {8: 72.97333333333333,\n",
      "          18: 67.82048529095985,\n",
      "          54: 78.78737647433853,\n",
      "          166: 54.5533114138093,\n",
      "          188: 65.14021000617664}}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Example script: load a DataFrame and compute all complexity measures.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df_example = df.sample(n=5) # We sample 5 random rows\n",
    "    # Compute all measures for Simple texts\n",
    "    metrics = compute_all_complexity_measures_df(\n",
    "        df_example,\n",
    "        column=\"Simple\", # Note that we use the column \"Simple\" for the Simple text. Use 'Complex' for the Complex text.\n",
    "        lang=\"en\",\n",
    "\n",
    "        spacy_nlp=spacy_nlp\n",
    "    )\n",
    "\n",
    "    print(\"All complexity measures (per row):\")\n",
    "    pprint(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f4165",
   "metadata": {},
   "source": [
    "Pay attention when using the function and ensure proper error handling for NaN values. As a rule, if any complexity dimension produces NaN values for a sample, that dimension must be discarded and not included in the subsequent model training or analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afebc35",
   "metadata": {},
   "source": [
    "**It is strongly recommended to implement a function that incorporates a backup strategy in case errors occur during execution (e.g., IO errors). Please note that if it is impossible to calculate a measure for at least one row (e.g., NaN value), that row must be discarded. At the end of this process, the goal is to obtain a dataframe with 16 columns. The columns should include Simple and Complex, followed by 7 columns containing the measures for the Simple text, and the final 7 columns containing the complexity measures for the Complex text (pay attention to use distinct names for the Simple and Complex columns.)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
