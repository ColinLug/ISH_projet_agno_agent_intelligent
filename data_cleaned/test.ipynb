{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ed9f53",
   "metadata": {},
   "source": [
    "# The Complexity Stress Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aafe23",
   "metadata": {},
   "source": [
    "First, install the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6403bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.11.3-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2025.11.3-cp313-cp313-win_amd64.whl (277 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   ---------------------------------------- 5/5 [nltk]\n",
      "\n",
      "Successfully installed click-8.3.1 joblib-1.5.3 nltk-3.9.2 regex-2025.11.3 tqdm-4.67.1\n",
      "Collecting textcomplexity\n",
      "  Using cached textcomplexity-0.11.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting networkx (from textcomplexity)\n",
      "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from textcomplexity) (3.9.2)\n",
      "Collecting nltk-tgrep (from textcomplexity)\n",
      "  Using cached nltk_tgrep-1.0.6-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting numpy (from textcomplexity)\n",
      "  Downloading numpy-2.3.5-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting scipy (from textcomplexity)\n",
      "  Downloading scipy-1.16.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: click in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk->textcomplexity) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk->textcomplexity) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk->textcomplexity) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk->textcomplexity) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk->textcomplexity) (0.4.6)\n",
      "Collecting pyparsing (from nltk-tgrep->textcomplexity)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Using cached textcomplexity-0.11.0-py3-none-any.whl (90 kB)\n",
      "Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached nltk_tgrep-1.0.6-py3-none-any.whl (18 kB)\n",
      "Downloading numpy-2.3.5-cp313-cp313-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 12.8/12.8 MB 86.9 MB/s  0:00:00\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Downloading scipy-1.16.3-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "   ---------------------------------------- 0.0/38.5 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 22.0/38.5 MB 109.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  37.7/38.5 MB 93.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.5/38.5 MB 85.6 MB/s  0:00:00\n",
      "Installing collected packages: pyparsing, numpy, networkx, scipy, nltk-tgrep, textcomplexity\n",
      "\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   -------------------- ------------------- 3/6 [scipy]\n",
      "   ---------------------------------------- 6/6 [textcomplexity]\n",
      "\n",
      "Successfully installed networkx-3.6.1 nltk-tgrep-1.0.6 numpy-2.3.5 pyparsing-3.2.5 scipy-1.16.3 textcomplexity-0.11.0\n",
      "Collecting stanza\n",
      "  Using cached stanza-1.11.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Using cached emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from stanza) (2.3.5)\n",
      "Collecting protobuf>=3.15.0 (from stanza)\n",
      "  Using cached protobuf-6.33.2-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting requests (from stanza)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from stanza) (3.6.1)\n",
      "Collecting torch>=1.13.0 (from stanza)\n",
      "  Downloading torch-2.9.1-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from stanza) (4.67.1)\n",
      "Collecting filelock (from torch>=1.13.0->stanza)\n",
      "  Using cached filelock-3.20.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch>=1.13.0->stanza)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.13.0->stanza)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting jinja2 (from torch>=1.13.0->stanza)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch>=1.13.0->stanza)\n",
      "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setuptools (from torch>=1.13.0->stanza)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.13.0->stanza)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.13.0->stanza)\n",
      "  Downloading markupsafe-3.0.3-cp313-cp313-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->stanza)\n",
      "  Downloading charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->stanza)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->stanza)\n",
      "  Using cached urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->stanza)\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Using cached stanza-1.11.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached protobuf-6.33.2-cp310-abi3-win_amd64.whl (436 kB)\n",
      "Downloading torch-2.9.1-cp313-cp313-win_amd64.whl (110.9 MB)\n",
      "   ---------------------------------------- 0.0/110.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.6/110.9 MB 8.7 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 5.2/110.9 MB 14.0 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.4/110.9 MB 24.8 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 34.6/110.9 MB 44.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 57.7/110.9 MB 58.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 80.7/110.9 MB 67.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 104.3/110.9 MB 74.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 110.9/110.9 MB 70.6 MB/s  0:00:01\n",
      "Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "Using cached filelock-3.20.1-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp313-cp313-win_amd64.whl (15 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl (107 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: mpmath, urllib3, typing-extensions, sympy, setuptools, protobuf, MarkupSafe, idna, fsspec, filelock, emoji, charset_normalizer, certifi, requests, jinja2, torch, stanza\n",
      "\n",
      "   ----------------------------------------  0/17 [mpmath]\n",
      "   -- -------------------------------------  1/17 [urllib3]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   ------- --------------------------------  3/17 [sympy]\n",
      "   --------- ------------------------------  4/17 [setuptools]\n",
      "   --------- ------------------------------  4/17 [setuptools]\n",
      "   --------- ------------------------------  4/17 [setuptools]\n",
      "   --------- ------------------------------  4/17 [setuptools]\n",
      "   --------- ------------------------------  4/17 [setuptools]\n",
      "   ----------- ----------------------------  5/17 [protobuf]\n",
      "   ------------------ ---------------------  8/17 [fsspec]\n",
      "   --------------------- ------------------  9/17 [filelock]\n",
      "   -------------------------------- ------- 14/17 [jinja2]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ----------------------------------- ---- 15/17 [torch]\n",
      "   ------------------------------------- -- 16/17 [stanza]\n",
      "   ------------------------------------- -- 16/17 [stanza]\n",
      "   ------------------------------------- -- 16/17 [stanza]\n",
      "   ------------------------------------- -- 16/17 [stanza]\n",
      "   ------------------------------------- -- 16/17 [stanza]\n",
      "   ------------------------------------- -- 16/17 [stanza]\n",
      "   ---------------------------------------- 17/17 [stanza]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.3 certifi-2025.11.12 charset_normalizer-3.4.4 emoji-2.15.0 filelock-3.20.1 fsspec-2025.12.0 idna-3.11 jinja2-3.1.6 mpmath-1.3.0 protobuf-6.33.2 requests-2.32.5 setuptools-80.9.0 stanza-1.11.0 sympy-1.14.0 torch-2.9.1 typing-extensions-4.15.0 urllib3-2.6.2\n",
      "Collecting wordfreq\n",
      "  Using cached wordfreq-3.1.1-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ftfy>=6.1 (from wordfreq)\n",
      "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting langcodes>=3.0 (from wordfreq)\n",
      "  Using cached langcodes-3.5.1-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting locate<2.0.0,>=1.1.1 (from wordfreq)\n",
      "  Using cached locate-1.1.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.7 (from wordfreq)\n",
      "  Downloading msgpack-1.1.2-cp313-cp313-win_amd64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: regex>=2023.10.3 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wordfreq) (2025.11.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\rroll\\appdata\\roaming\\python\\python313\\site-packages (from ftfy>=6.1->wordfreq) (0.2.14)\n",
      "Using cached wordfreq-3.1.1-py3-none-any.whl (56.8 MB)\n",
      "Using cached locate-1.1.1-py3-none-any.whl (5.4 kB)\n",
      "Downloading msgpack-1.1.2-cp313-cp313-win_amd64.whl (72 kB)\n",
      "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Using cached langcodes-3.5.1-py3-none-any.whl (183 kB)\n",
      "Installing collected packages: msgpack, locate, langcodes, ftfy, wordfreq\n",
      "\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   ---------------------------------------- 5/5 [wordfreq]\n",
      "\n",
      "Successfully installed ftfy-6.3.1 langcodes-3.5.1 locate-1.1.1 msgpack-1.1.2 wordfreq-3.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named spacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python313\\site-packages (from tqdm) (0.4.6)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp313-cp313-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp313-cp313-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp313-cp313-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp313-cp313-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Using cached weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rroll\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (25.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.41.5-cp313-cp313-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp313-cp313-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Using cached cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Using cached smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading wrapt-2.0.1-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->spacy) (3.0.3)\n",
      "Downloading spacy-3.8.11-cp313-cp313-win_amd64.whl (14.2 MB)\n",
      "   ---------------------------------------- 0.0/14.2 MB ? eta -:--:--\n",
      "   ---------------------------------------  14.2/14.2 MB 83.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.2/14.2 MB 63.3 MB/s  0:00:00\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp313-cp313-win_amd64.whl (40 kB)\n",
      "Downloading murmurhash-1.0.15-cp313-cp313-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.12-cp313-cp313-win_amd64.whl (117 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 88.0 MB/s  0:00:00\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp313-cp313-win_amd64.whl (653 kB)\n",
      "   ---------------------------------------- 0.0/653.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 653.1/653.1 kB 54.1 MB/s  0:00:00\n",
      "Downloading thinc-8.3.10-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 36.4 MB/s  0:00:00\n",
      "Downloading blis-1.3.3-cp313-cp313-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 80.9 MB/s  0:00:00\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Using cached cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Using cached smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading wrapt-2.0.1-cp313-cp313-win_amd64.whl (60 kB)\n",
      "Installing collected packages: wrapt, wasabi, typing-inspection, spacy-loggers, spacy-legacy, pydantic-core, murmurhash, cymem, cloudpathlib, catalogue, blis, annotated-types, typer-slim, srsly, smart-open, pydantic, preshed, confection, weasel, thinc, spacy\n",
      "\n",
      "   ------- --------------------------------  4/21 [spacy-legacy]\n",
      "   ------------------- -------------------- 10/21 [blis]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------------- ----- 18/21 [weasel]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   ---------------------------------------- 21/21 [spacy]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 pydantic-2.12.5 pydantic-core-2.41.5 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 typer-slim-0.20.0 typing-inspection-0.4.2 wasabi-1.1.3 weasel-0.4.3 wrapt-2.0.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install textcomplexity\n",
    "!{sys.executable} -m pip install stanza\n",
    "!{sys.executable} -m pip install wordfreq \n",
    "!{sys.executable} -m spacy download en_core_web_md\n",
    "!{sys.executable} -m pip install tqdm spacy numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625ec67",
   "metadata": {},
   "source": [
    "First, import the following Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6789f29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 8.05MB/s]                    \n",
      "2025-12-18 20:21:16 INFO: Downloaded file to C:\\Users\\rroll\\stanza_resources\\resources.json\n",
      "2025-12-18 20:21:16 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-12-18 20:21:17 INFO: File exists: C:\\Users\\rroll\\stanza_resources\\en\\default.zip\n",
      "2025-12-18 20:21:20 INFO: Finished downloading models and saved to C:\\Users\\rroll\\stanza_resources\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rroll\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rroll\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x2bebcb28f10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "from pprint import pprint\n",
    "from typing import Dict, Set, Iterable, Optional, Any, Tuple\n",
    "import importlib.resources as pkg_resources\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "import stanza\n",
    "import textcomplexity  # only used to access en.json\n",
    "from tqdm.auto import tqdm  \n",
    "\n",
    "# Download required resources\n",
    "stanza.download('en')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Make sure WordNet is available; if not, download it.\n",
    "try:\n",
    "    _ = wn.synsets(\"dog\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "spacy_nlp = nlp\n",
    "spacy_nlp.add_pipe(\"sentencizer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57283f",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eef7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets ={'ose_adv_ele':'data_sampled/OSE_adv_ele.csv', \n",
    "           'ose_adv_int':'data_sampled/OSE_adv_int.csv',\n",
    "           'swipe': 'data_sampled/swipe.csv',\n",
    "           'vikidia':'data_sampled/vikidia.csv'}\n",
    "\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path, sep='\\t')\n",
    "    \n",
    "\n",
    "def load_dataset(name):\n",
    "    if name not in datasets:\n",
    "        raise ValueError(f\"Dataset {name} not found\")\n",
    "    return load_data(datasets[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c93eb1",
   "metadata": {},
   "source": [
    "Let's load one of the datasets, in this case \"ose_adv_ele\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae91d38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Simple</th>\n",
       "      <th>Complex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿To tourists, Amsterdam still seems very liber...</td>\n",
       "      <td>﻿Amsterdam still looks liberal to tourists, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>﻿Anitta, a music star from Brazil, has million...</td>\n",
       "      <td>﻿Brazil’s latest funk sensation, Anitta, has w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Simple  \\\n",
       "0  ﻿When you see the word Amazon, what’s the firs...   \n",
       "1  ﻿To tourists, Amsterdam still seems very liber...   \n",
       "2  ﻿Anitta, a music star from Brazil, has million...   \n",
       "\n",
       "                                             Complex  \n",
       "0  ﻿When you see the word Amazon, what’s the firs...  \n",
       "1  ﻿Amsterdam still looks liberal to tourists, wh...  \n",
       "2  ﻿Brazil’s latest funk sensation, Anitta, has w...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('ose_adv_ele')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb2dc3",
   "metadata": {},
   "source": [
    "Let's look at a random row of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7bb885b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE TEXT\n",
      "﻿The Manchester United manager, Sir Alex Ferguson, will retire at the end of the season after 27 years. He will become a director of the club. He is the most successful manager in British football. He has won 13 Premier League titles, two Champions Leagues, the Cup Winners’ Cup, five FA Cups and four League Cups.\n",
      "“The decision to retire is one that I have thought a lot about,” Ferguson said. “It is the right time. It was important to me to leave an organization in the strongest possible condition and I believe I have done so.” He said that he thinks the quality of the team will bring continued success at the highest level. They also have lots of good young players, so Ferguson thinks the club has a very good future.\n",
      "“Our training facilities are some of the best in world sport,” he added. “Our stadium, Old Trafford, is one of the most important venues in the world. I am delighted to become both director and ambassador for the club. I am looking forward to the future.” He also thanked his family for their love and support.\n",
      "“I would like to thank all my players and staff, past and present, for an incredible level of professionalism and hard work that has helped to bring so many memorable triumphs. Without them, the history of this great club would not be as rich. In my early years, the support of the board of directors gave me the confidence and time to build a football club, not just a football team.\n",
      "“Over the past ten years, the owners of the club have made it possible for me to manage Manchester United to the best of my ability. I have been very lucky to work with David Gill, a talented and trustworthy chief executive. I am grateful to all of them.” He also thanked the fans for their support and said he had really enjoyed his time as manager of Manchester United.\n",
      "Joel Glazer, one of the owners of Manchester United, said: “Alex has shown us so often what a fantastic manager he is, but he’s also a wonderful person. His determination to succeed and his hard work for the club have been remarkable. I will never forget the wonderful memories he has given us, like that magical night in Moscow.”\n",
      "Avie Glazer, his brother, said: “I am very happy to tell you that Alex has agreed to stay with the club as a director. His contributions to Manchester United over the last 27 years have been extraordinary and, like all United fans, I want him to be a part of its future.”\n",
      "David Gill added: “I’ve had the great pleasure of working very closely with Alex for 16 unforgettable years. We knew that his retirement would come one day and we both have been planning for it. Alex’s vision, energy and ability have built teams that are some of the best and most loyal in world sport. The way he cares for this club, his staff and for the football family in general is something that I admire. We will never forget what he has done for this club and for the game in general. Working with Alex has been the greatest experience of my working life and it is a great honour to be able to call him a friend.”\n",
      "First-team coach René Meulensteen told everyone how Ferguson told his staff the news. “I found out this morning when I came to the club,” he said. “He asked us to go into his office and told us his decision. I’m sure he thought hard about it. I wish him well for the future. He’s been fantastic for this club and I hope all the fans give the new manager the same support.”\n",
      "----------------------------------------------------------------------------------------------------\n",
      "COMPLEX TEXT\n",
      "﻿Sir Alex Ferguson will retire as Manchester United manager at the end of the season in the 27th year of a tenure that has made him the most successful manager in British football. While he will become a United director and ambassador, the club will now have to find someone to replace a man who has won 13 English Premier League titles, two Champions Leagues, the Cup Winners’ Cup, five FA Cups and four League Cups.\n",
      "Regarding his decision, Ferguson said: “The decision to retire is one that I have thought a great deal about and one that I have not taken lightly. It is the right time. It was important to me to leave an organization in the strongest possible shape and I believe I have done so. The quality of this league-winning squad, and the balance of ages within it, bodes well for continued success at the highest level whilst the structure of the youth set-up will ensure that the long-term future of the club remains a bright one.\n",
      "“Our training facilities are amongst the finest in global sport and our home, Old Trafford, is rightfully regarded as one of the leading venues in the world. Going forward, I am delighted to take on the roles of both director and ambassador for the club. With these activities, along with my many other interests, I am looking forward to the future. I must pay tribute to my family; their love and support has been essential. My wife, Cathy, has been the key figure throughout my career, providing a bedrock of both stability and encouragement. Words are not enough to express what this has meant to me.\n",
      "“As for my players and staff, past and present, I would like to thank them all for a staggering level of professional conduct and dedication that has helped to deliver so many memorable triumphs. Without their contribution, the history of this great club would not be as rich. In my early years, the backing of the board, and Sir Bobby Charlton in particular, gave me the confidence and time to build a football club, rather than just a football team.\n",
      "“Over the past decade, the Glazer family have provided me with the platform to manage Manchester United to the best of my ability and\n",
      "I have been extremely fortunate to have worked with a talented and trustworthy chief executive in David Gill. I am truly grateful to all of them. To the fans, thank you. The support you have provided over the years has been truly humbling. It has been an honour and an enormous privilege to have had the opportunity to lead your club and I have treasured my time as manager of Manchester United.”\n",
      "The sudden nature of Ferguson’s departure is in keeping with how the Scot stated he would leave the post after he mentioned a first retirement during the 2001/2 season before performing a U-turn. It is understood he gathered the players in the first-team changing room shortly after they arrived for training on Wednesday morning. In an emotional speech, he announced he was to step down. He then took his backroom staff aside before finally addressing the rest of the staff in the canteen.\n",
      "Joel Glazer, joint owner of Manchester United, said: “Alex has proven time and time again what a fantastic manager he is but he’s also a wonderful person. His determination to succeed and dedication to the club have been truly remarkable. I will always cherish the wonderful memories he has given us, like that magical night in Moscow.”\n",
      "Avie Glazer, his brother, said: “I am delighted to announce that Alex has agreed to stay with the club as a director. His contributions to Manchester United over the last 27 years have been extraordinary and, like all United fans, I want him to be a part of its future.”\n",
      "David Gill, the outgoing chief executive, said: “I’ve had the tremendous pleasure of working very closely with Alex for 16 unforgettable years – through the treble, the double, countless trophy wins and numerous signings. We knew that his retirement would come one day and we both have been planning for it by ensuring the quality of the squad and club structures are in first-class condition. Alex’s vision, energy and ability have built teams – both on and off the pitch – that his successor can count on as among the best and most loyal in world sport.\n",
      "“The way he cares for this club, his staff and for the football family in general is something that I admire. It is a side to him that is often hidden from public view but it is something that I have been privileged to witness in the last 16 years.\n",
      "What he has done for this club and for the game in general will never be forgotten. It has been the greatest experience of my working life being alongside Alex and a great honour to be able to call him a friend.”\n",
      "First-team coach René Meulensteen revealed how Ferguson broke the news to his backroom staff on Wednesday morning. “I found out this morning when I came to the club,” he said. “He called us into his office and told us what decision he had taken. It’s always been on the cards – there’s speculation every season. I think the manager kept his cards close to his chest. I think he felt the time was right now and he made a decision.\n",
      "“He’s obviously a man who thinks very, very hard so I’m sure he’s put a lot of thought into making this decision. I wish him well. He’s been fantastic for this club and I hope all the fans give whoever’s going to come in the same support that he gets.”\n"
     ]
    }
   ],
   "source": [
    "row = df.sample(1)\n",
    "\n",
    "print('SIMPLE TEXT')\n",
    "print(row['Simple'].iloc[0])\n",
    "print('-'*100)\n",
    "print('COMPLEX TEXT')\n",
    "print(row['Complex'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ebbb2",
   "metadata": {},
   "source": [
    "Let's look at the size of each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72529061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ose_adv_ele: 189 rows\n",
      "ose_adv_int: 189 rows\n",
      "swipe: 1233 rows\n",
      "vikidia: 1233 rows\n",
      "Total: 2844 rows\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for name, path in datasets.items():\n",
    "    df = load_dataset(name)\n",
    "    print(f\"{name}: {df.shape[0]} rows\")\n",
    "    cnt += df.shape[0]\n",
    "print(f\"Total: {cnt} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571b7484",
   "metadata": {},
   "source": [
    "Let's load again the dataset for computing the complexity measure in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b7e233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset('ose_adv_ele')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd81ace0",
   "metadata": {},
   "source": [
    "## Complexity measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0d91084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cache stanza pipelines to avoid re-loading models\n",
    "_STANZA_PIPELINES: Dict[str, stanza.Pipeline] = {}\n",
    "\n",
    "# UPOS tags considered content words (C)\n",
    "CONTENT_UPOS = {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def load_cow_top5000_en() -> Set[str]:\n",
    "    \"\"\"\n",
    "    Load the COW-based list of the 5,000 most frequent English content words\n",
    "    from textcomplexity's English language definition file (en.json).\n",
    "\n",
    "    We ignore POS tags and keep only lowercased word forms.\n",
    "    \"\"\"\n",
    "    with pkg_resources.files(textcomplexity).joinpath(\"en.json\").open(\n",
    "        \"r\", encoding=\"utf-8\"\n",
    "    ) as f:\n",
    "        lang_def = json.load(f)\n",
    "\n",
    "    most_common = lang_def[\"most_common\"]  # list of [word, xpos]\n",
    "    cow_top5000 = {w.lower() for w, xpos in most_common}\n",
    "    return cow_top5000\n",
    "\n",
    "\n",
    "def get_stanza_pipeline(lang: str = \"en\", use_gpu: bool = False) -> stanza.Pipeline:\n",
    "    \"\"\"\n",
    "    Get (or create) a cached stanza Pipeline for a given language.\n",
    "\n",
    "    NOTE: You must have downloaded the models beforehand, e.g.:\n",
    "        import stanza\n",
    "        stanza.download('en')\n",
    "    \"\"\"\n",
    "    if lang not in _STANZA_PIPELINES:\n",
    "        _STANZA_PIPELINES[lang] = stanza.Pipeline(\n",
    "            lang=lang,\n",
    "            processors=\"tokenize,pos,lemma,depparse,constituency\",\n",
    "            use_gpu=use_gpu,\n",
    "            tokenize_no_ssplit=False,\n",
    "        )\n",
    "    return _STANZA_PIPELINES[lang]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a53cb",
   "metadata": {},
   "source": [
    "### Lexical complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929089ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_mtld(tokens: Iterable[str], ttr_threshold: float = 0.72) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute MTLD (Measure of Textual Lexical Diversity) for a list of tokens.\n",
    "\n",
    "    MTLD = total_number_of_tokens / number_of_factors\n",
    "\n",
    "    A factor is a contiguous segment where the running TTR stays >= threshold.\n",
    "    When the TTR drops below the threshold, we close a factor (at the previous\n",
    "    token) and start a new one. At the end, the remaining partial segment is\n",
    "    counted as a fractional factor, with weight proportional to how close the\n",
    "    final TTR is to the threshold.\n",
    "    \"\"\"\n",
    "    tokens = [tok for tok in tokens if tok]\n",
    "    if not tokens:\n",
    "        return None\n",
    "\n",
    "    types = set()\n",
    "    factor_count = 0.0\n",
    "    token_count_in_factor = 0\n",
    "\n",
    "    for tok in tokens:\n",
    "        token_count_in_factor += 1\n",
    "        types.add(tok)\n",
    "        ttr = len(types) / token_count_in_factor\n",
    "\n",
    "        if ttr < ttr_threshold:\n",
    "            factor_count += 1.0\n",
    "            types = set()\n",
    "            token_count_in_factor = 0\n",
    "\n",
    "    # final partial factor\n",
    "    if token_count_in_factor > 0:\n",
    "        final_ttr = len(types) / token_count_in_factor\n",
    "        if final_ttr < 1.0:\n",
    "            fractional = (1.0 - final_ttr) / (1.0 - ttr_threshold)\n",
    "            fractional = max(0.0, min(1.0, fractional))\n",
    "            factor_count += fractional\n",
    "\n",
    "    if factor_count == 0:\n",
    "        return None\n",
    "\n",
    "    return len(tokens) / factor_count\n",
    "\n",
    "\n",
    "\n",
    "def _compute_lexical_density(total_tokens: int, content_tokens: int) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LD = |C| / |T|\n",
    "    where:\n",
    "        |C| = number of content-word tokens\n",
    "        |T| = total number of non-punctuation tokens\n",
    "    \"\"\"\n",
    "    if total_tokens == 0:\n",
    "        return None\n",
    "    return content_tokens / total_tokens\n",
    "\n",
    "\n",
    "def _compute_lexical_sophistication_cow(\n",
    "    content_forms: Iterable[str],\n",
    "    cow_top5000: set,\n",
    ") -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LS = |{ w in C : w not in R }| / |C|\n",
    "    where:\n",
    "        C = content-word tokens (surface forms, lowercased)\n",
    "        R = COW top-5000 content word forms (lowercased)\n",
    "    \"\"\"\n",
    "    forms = [f for f in content_forms if f]\n",
    "    if not forms:\n",
    "        return None\n",
    "\n",
    "    off_list = sum(1 for f in forms if f not in cow_top5000)\n",
    "    return off_list / len(forms)\n",
    "\n",
    "\n",
    "\n",
    "def lexical_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MTLD, LD, LS from a stanza Document.\n",
    "    \"\"\"\n",
    "    cow_top5000 = load_cow_top5000_en()\n",
    "\n",
    "    mtld_tokens = []\n",
    "    total_tokens = 0\n",
    "    content_tokens = 0\n",
    "    content_forms = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.upos == \"PUNCT\":\n",
    "                continue\n",
    "\n",
    "            lemma = (word.lemma or word.text or \"\").lower()\n",
    "            if not lemma:\n",
    "                continue\n",
    "\n",
    "            mtld_tokens.append(lemma)\n",
    "            total_tokens += 1\n",
    "\n",
    "            if word.upos in CONTENT_UPOS:\n",
    "                content_tokens += 1\n",
    "                form = (word.text or \"\").lower()\n",
    "                content_forms.append(form)\n",
    "\n",
    "    mtld = _compute_mtld(mtld_tokens) if mtld_tokens else None\n",
    "    ld = _compute_lexical_density(total_tokens, content_tokens)\n",
    "    ls = _compute_lexical_sophistication_cow(content_forms, cow_top5000)\n",
    "\n",
    "    return {\"MTLD\": mtld, \"LD\": ld, \"LS\": ls}\n",
    "\n",
    "\n",
    "def lexical_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute lexical measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return lexical_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute lexical measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = lexical_measures_from_text(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "\n",
    "    return {\"MTLD\": mtld_res, \"LD\": ld_res, \"LS\": ls_res}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4583f900",
   "metadata": {},
   "source": [
    "### Syntactic complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1f6ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mdd_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute Mean Dependency Distance (MDD) from a stanza Document.\n",
    "\n",
    "    For each sentence s_i with dependency set D_i:\n",
    "        MDD_i = (1 / |D_i|) * sum_{(h,d) in D_i} |h - d|\n",
    "    Then:\n",
    "        MDD = (1 / k) * sum_i MDD_i, over all sentences with at least one dependency.\n",
    "    \"\"\"\n",
    "    sentence_mdds = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        distances = []\n",
    "        for w in sent.words:\n",
    "            if w.head is None or w.head == 0:\n",
    "                continue\n",
    "            distances.append(abs(w.id - w.head))\n",
    "\n",
    "        if distances:\n",
    "            sentence_mdds.append(sum(distances) / len(distances))\n",
    "\n",
    "    if not sentence_mdds:\n",
    "        return None\n",
    "    return sum(sentence_mdds) / len(sentence_mdds)\n",
    "\n",
    "\n",
    "\n",
    "def _count_clauses_in_tree(tree) -> int:\n",
    "    \"\"\"\n",
    "    Count clause nodes in a constituency tree.\n",
    "\n",
    "    A simple and standard heuristic (PTB-style) is:\n",
    "        count all nodes whose label starts with 'S'\n",
    "        (S, SBAR, SBARQ, SINV, SQ, etc.).\n",
    "\n",
    "    This aligns with the idea of counting finite and subordinate clauses\n",
    "    as in Hunt (1965) and later complexity work.\n",
    "    \"\"\"\n",
    "    if tree is None:\n",
    "        return 0\n",
    "\n",
    "    # Stanza's constituency tree: tree.label, tree.children\n",
    "    count = 1 if getattr(tree, \"label\", \"\").startswith(\"S\") else 0\n",
    "\n",
    "    for child in getattr(tree, \"children\", []):\n",
    "        # leaves can be strings or terminals without 'label'\n",
    "        if hasattr(child, \"label\"):\n",
    "            count += _count_clauses_in_tree(child)\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def cs_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute CS (clauses per sentence) from a stanza Document.\n",
    "\n",
    "        CS = (1 / k) * sum_i L_i\n",
    "\n",
    "    where L_i is the number of clauses in sentence s_i, estimated by counting\n",
    "    all constituents whose label starts with 'S' in the constituency tree of s_i.\n",
    "    \"\"\"\n",
    "    clause_counts = []\n",
    "    for sent in doc.sentences:\n",
    "        tree = getattr(sent, \"constituency\", None)\n",
    "        if tree is None:\n",
    "            # No constituency tree available for this sentence\n",
    "            continue\n",
    "        num_clauses = _count_clauses_in_tree(tree)\n",
    "        clause_counts.append(num_clauses)\n",
    "\n",
    "    if not clause_counts:\n",
    "        return None\n",
    "\n",
    "    return sum(clause_counts) / len(clause_counts)\n",
    "\n",
    "\n",
    "\n",
    "def syntactic_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MDD and CS from a stanza Document.\n",
    "    \"\"\"\n",
    "    mdd = mdd_from_doc(doc)\n",
    "    cs = cs_from_doc(doc)\n",
    "    return {\"MDD\": mdd, \"CS\": cs}\n",
    "\n",
    "\n",
    "def syntactic_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute syntactic measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return syntactic_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "def compute_syntactic_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute syntactic measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MDD\": {index: value},\n",
    "            \"CS\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = syntactic_measures_from_text(text, lang=lang)\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "    return {\"MDD\": mdd_res, \"CS\": cs_res}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20110b8",
   "metadata": {},
   "source": [
    "### Discourse complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "739a62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Approximate set of content POS tags (spaCy universal POS)\n",
    "CONTENT_POS =  {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "def is_content_token(tok):\n",
    "    \"\"\"\n",
    "    Return True if token is considered a content word.\n",
    "    We ignore stopwords, punctuation, and non-alphabetic tokens.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        tok.is_alpha\n",
    "        and not tok.is_stop\n",
    "        and tok.pos_ in CONTENT_POS\n",
    "    )\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=100000)\n",
    "def get_related_lemmas(lemma):\n",
    "    \"\"\"\n",
    "    Return a set of semantically related lemmas for the given lemma\n",
    "    using WordNet, including:\n",
    "      - synonyms\n",
    "      - antonyms\n",
    "      - hypernyms / hyponyms\n",
    "      - meronyms (part/member/substance)\n",
    "      - coordinate terms (siblings under the same hypernym)\n",
    "\n",
    "    NOTE: Some older examples mention 'troponyms', but in NLTK's\n",
    "    WordNet interface there is no 'troponyms()' method on Synset,\n",
    "    so we do NOT use it here.\n",
    "    \"\"\"\n",
    "    lemma = lemma.lower()\n",
    "    related = set()\n",
    "    synsets = wn.synsets(lemma)\n",
    "\n",
    "    for syn in synsets:\n",
    "        # Synonyms and antonyms\n",
    "        for l in syn.lemmas():\n",
    "            related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "            for ant in l.antonyms():\n",
    "                related.add(ant.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Hypernyms (more general) and hyponyms (more specific)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for l in hyper.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "        for hypo in syn.hyponyms():\n",
    "            for l in hypo.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Meronyms: part/member/substance\n",
    "        for mer in syn.part_meronyms() + syn.member_meronyms() + syn.substance_meronyms():\n",
    "            for l in mer.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Coordinate terms (siblings under same hypernym)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for sibling in hyper.hyponyms():\n",
    "                if sibling == syn:\n",
    "                    continue\n",
    "                for l in sibling.lemmas():\n",
    "                    related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "    # Remove the lemma itself if present\n",
    "    related.discard(lemma)\n",
    "    return related\n",
    "\n",
    "\n",
    "def lexical_cohesion_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Lexical Cohesion (LC) for a single document:\n",
    "\n",
    "        LC = |C| / m\n",
    "\n",
    "    where:\n",
    "      - |C| is the number of cohesive devices between sentences\n",
    "        (lexical repetition + semantic relations),\n",
    "      - m  is the total number of word tokens (alphabetic) in the document.\n",
    "\n",
    "    If the document has fewer than 2 sentences or no valid words,\n",
    "    LC is returned as 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Total number of alphabetic tokens (denominator m)\n",
    "    m = sum(1 for tok in doc if tok.is_alpha)\n",
    "    if m == 0:\n",
    "        return 0.0\n",
    "\n",
    "    sentences = list(doc.sents)\n",
    "    if len(sentences) < 2:\n",
    "        # With only one sentence, cross-sentence cohesion is not defined\n",
    "        return 0.0\n",
    "\n",
    "    # Collect sets of content lemmas per sentence\n",
    "    sent_lemmas = []\n",
    "    for sent in sentences:\n",
    "        lemmas = set(\n",
    "            tok.lemma_.lower()\n",
    "            for tok in sent\n",
    "            if is_content_token(tok)\n",
    "        )\n",
    "        if lemmas:\n",
    "            sent_lemmas.append(lemmas)\n",
    "\n",
    "    if len(sent_lemmas) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    cohesive_count = 0\n",
    "\n",
    "    for i in range(len(sent_lemmas) - 1):\n",
    "        for j in range(i + 1, len(sent_lemmas)):\n",
    "            li = sent_lemmas[i]\n",
    "            lj = sent_lemmas[j]\n",
    "\n",
    "            # 1) Lexical repetition: shared lemmas\n",
    "            shared = li & lj\n",
    "            cohesive_count += len(shared)\n",
    "\n",
    "            # 2) Semantic relations via WordNet\n",
    "            for lemma in li:\n",
    "                related = get_related_lemmas(lemma)\n",
    "                cohesive_count += len(related & lj)\n",
    "\n",
    "    return float(cohesive_count) / float(m)\n",
    "\n",
    "\n",
    "def sentence_vector(sent, vector_size):\n",
    "    \"\"\"\n",
    "    Represent a sentence as the average of token vectors.\n",
    "    If no token has a vector, return a zero vector.\n",
    "    \"\"\"\n",
    "    vecs = [\n",
    "        tok.vector\n",
    "        for tok in sent\n",
    "        if tok.has_vector and not tok.is_punct and not tok.is_space\n",
    "    ]\n",
    "    if not vecs:\n",
    "        return np.zeros(vector_size, dtype=\"float32\")\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "def coherence_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Coherence (CoH) for a single document as the average\n",
    "    cosine similarity between adjacent sentence vectors:\n",
    "\n",
    "        CoH = (1 / (k-1)) * sum_{i=1}^{k-1} cos(h_i, h_{i+1})\n",
    "\n",
    "    where h_i is the sentence/topic vector for sentence i.\n",
    "\n",
    "    If the document has fewer than 2 sentences, CoH = 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    if nlp.vocab.vectors_length == 0:\n",
    "        raise ValueError(\n",
    "            \"The loaded spaCy model does not contain word vectors \"\n",
    "            \"(nlp.vocab.vectors_length == 0). \"\n",
    "            \"Use a model like 'en_core_web_md' or similar.\"\n",
    "        )\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    k = len(sentences)\n",
    "\n",
    "    if k < 2:\n",
    "        # Only one sentence: no adjacent pair, coherence = 0.0\n",
    "        return 0.0\n",
    "\n",
    "    vector_size = nlp.vocab.vectors_length\n",
    "    sent_vectors = [\n",
    "        sentence_vector(sent, vector_size)\n",
    "        for sent in sentences\n",
    "    ]\n",
    "\n",
    "    sims = []\n",
    "    for i in range(k - 1):\n",
    "        v1 = sent_vectors[i]\n",
    "        v2 = sent_vectors[i + 1]\n",
    "        norm1 = np.linalg.norm(v1)\n",
    "        norm2 = np.linalg.norm(v2)\n",
    "        denom = norm1 * norm2\n",
    "        if denom == 0.0:\n",
    "            # Skip pairs where at least one sentence vector is zero\n",
    "            continue\n",
    "        cos_sim = float(np.dot(v1, v2) / denom)\n",
    "        sims.append(cos_sim)\n",
    "\n",
    "    if not sims:\n",
    "        return 0.0\n",
    "\n",
    "    return float(np.mean(sims))\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_cohesion_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute LC for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with lemmatizer, POS tagger, etc.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of LC scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [lexical_cohesion_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_coherence_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute CoH for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with word vectors.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of CoH scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [coherence_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_discourse_measures(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute both LC and CoH for each row of a DataFrame and return\n",
    "    them in a dictionary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"LC\":  np.ndarray of lexical cohesion scores,\n",
    "            \"CoH\": np.ndarray of coherence scores\n",
    "        }\n",
    "    \"\"\"\n",
    "    lc_vec = compute_lexical_cohesion_vector(df, nlp, column=column)\n",
    "    coh_vec = compute_coherence_vector(df, nlp, column=column)\n",
    "    return {\"LC\": lc_vec, \"CoH\": coh_vec}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70f0bb",
   "metadata": {},
   "source": [
    "### Text complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c285251",
   "metadata": {},
   "source": [
    "Here we compute the complexity of each function. Note that we use a method that calculates all measures at once. However, it is advisable to compute each measure separately so that you can better handle any potential errors. For example, calculate MLTD first and save it, then LD, and so on. The following code is an example of how to compute the measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88a6a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze_text_all(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Parse a text with stanza and compute all measures (lexical + syntactic)\n",
    "    in a single pass.\n",
    "\n",
    "    Returns a dict with keys:\n",
    "        \"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\"\n",
    "    (Discourse measures LC/CoH are added later at DataFrame level, via spaCy.)\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None, \"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    lex = lexical_measures_from_doc(doc)\n",
    "    syn = syntactic_measures_from_doc(doc)\n",
    "\n",
    "    out: Dict[str, Optional[float]] = {}\n",
    "    out.update(lex)\n",
    "    out.update(syn)\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_all_complexity_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    "    spacy_nlp=None,\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute all complexity measures for each row in df[column].\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with a text column.\n",
    "    column : str, default \"text\"\n",
    "        Name of the text column.\n",
    "    lang : str, default \"en\"\n",
    "        Language code for stanza.\n",
    "    n_jobs : int, default 1\n",
    "        Number of worker processes to use.\n",
    "            - 1  : sequential execution (no multiprocessing).\n",
    "            - >1 : multiprocessing with that many workers.\n",
    "            - 0 or None : use cpu_count() workers.\n",
    "    spacy_nlp : spaCy Language, required for LC / CoH\n",
    "        Pre-loaded spaCy pipeline with:\n",
    "            - POS / lemmatizer for LC\n",
    "            - word vectors for CoH (e.g. 'en_core_web_md').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "            \"MDD\":  {index: value},\n",
    "            \"CS\":   {index: value},\n",
    "            \"LC\":   {index: value},\n",
    "            \"CoH\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    items = list(df[column].items())  # list[(index, text)]\n",
    "    total_items = len(items)\n",
    "\n",
    "    # ---- Lexical + syntactic (stanza) ----\n",
    "    for idx, text in tqdm(\n",
    "        items,\n",
    "        total=total_items,\n",
    "        desc=\"Computing lexical & syntactic complexity (sequential)\",\n",
    "    ):\n",
    "        metrics = _analyze_text_all(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "\n",
    "    # ---- Discourse measures (spaCy: LC & CoH) ----\n",
    "    if spacy_nlp is None:\n",
    "        raise ValueError(\n",
    "            \"spacy_nlp must be provided to compute LC and CoH. \"\n",
    "            \"Load a spaCy model with vectors, e.g. 'en_core_web_md', and \"\n",
    "            \"pass it as spacy_nlp=...\"\n",
    "        )\n",
    "\n",
    "    discourse = compute_discourse_measures(df, spacy_nlp, column=column)\n",
    "    lc_vec = discourse[\"LC\"]\n",
    "    coh_vec = discourse[\"CoH\"]\n",
    "\n",
    "    lc_res: Dict[Any, float] = {}\n",
    "    coh_res: Dict[Any, float] = {}\n",
    "\n",
    "    # Map arrays back to DataFrame indices\n",
    "    for i, idx in enumerate(df.index):\n",
    "        lc_res[idx] = float(lc_vec[i])\n",
    "        coh_res[idx] = float(coh_vec[i])\n",
    "\n",
    "    return {\n",
    "        \"MTLD\": mtld_res,\n",
    "        \"LD\": ld_res,\n",
    "        \"LS\": ls_res,\n",
    "        \"MDD\": mdd_res,\n",
    "        \"CS\": cs_res,\n",
    "        \"LC\": lc_res,\n",
    "        \"CoH\": coh_res,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "914f60f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing lexical & syntactic complexity (sequential):   0%|          | 0/5 [00:00<?, ?it/s]2025-12-18 17:34:19 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 7.44MB/s]                    \n",
      "2025-12-18 17:34:19 INFO: Downloaded file to C:\\Users\\rroll\\stanza_resources\\resources.json\n",
      "2025-12-18 17:34:19 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-12-18 17:34:20 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "======================================\n",
      "\n",
      "2025-12-18 17:34:20 INFO: Using device: cpu\n",
      "2025-12-18 17:34:20 INFO: Loading: tokenize\n",
      "2025-12-18 17:34:20 INFO: Loading: mwt\n",
      "2025-12-18 17:34:20 INFO: Loading: pos\n",
      "2025-12-18 17:34:21 INFO: Loading: lemma\n",
      "2025-12-18 17:34:22 INFO: Loading: constituency\n",
      "2025-12-18 17:34:22 INFO: Loading: depparse\n",
      "2025-12-18 17:34:22 INFO: Done loading processors!\n",
      "Computing lexical & syntactic complexity (sequential): 100%|██████████| 5/5 [00:24<00:00,  4.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All complexity measures (per row):\n",
      "{'CS': {17: 2.9069767441860463,\n",
      "        38: 3.6333333333333333,\n",
      "        46: 4.8,\n",
      "        51: 3.3548387096774195,\n",
      "        55: 3.1707317073170733},\n",
      " 'CoH': {17: 0.8373317718505859,\n",
      "         38: 0.8397873044013977,\n",
      "         46: 0.8502054214477539,\n",
      "         51: 0.8557181358337402,\n",
      "         55: 0.8372233510017395},\n",
      " 'LC': {17: 2.9229559898376465,\n",
      "        38: 1.283203125,\n",
      "        46: 1.7163375616073608,\n",
      "        51: 1.116279125213623,\n",
      "        55: 3.4348485469818115},\n",
      " 'LD': {17: 0.5186846038863976,\n",
      "        38: 0.48295454545454547,\n",
      "        46: 0.5365853658536586,\n",
      "        51: 0.45020746887966806,\n",
      "        55: 0.5331369661266568},\n",
      " 'LS': {17: 0.1988472622478386,\n",
      "        38: 0.3058823529411765,\n",
      "        46: 0.22402597402597402,\n",
      "        51: 0.25806451612903225,\n",
      "        55: 0.30939226519337015},\n",
      " 'MDD': {17: 3.0743672533969515,\n",
      "         38: 3.090355057151884,\n",
      "         46: 3.459314447642028,\n",
      "         51: 2.9663430377463005,\n",
      "         55: 3.212641674453416},\n",
      " 'MTLD': {17: 67.30484114977307,\n",
      "          38: 54.352941176470594,\n",
      "          46: 57.4,\n",
      "          51: 44.323324396782844,\n",
      "          55: 78.78737647433853}}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Example script: load a DataFrame and compute all complexity measures.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df_example = df.sample(n=5) # We sample 5 random rows\n",
    "    # Compute all measures for Simple texts\n",
    "    metrics = compute_all_complexity_measures_df(\n",
    "        df_example,\n",
    "        column=\"Simple\", # Note that we use the column \"Simple\" for the Simple text. Use 'Complex' for the Complex text.\n",
    "        lang=\"en\",\n",
    "\n",
    "        spacy_nlp=spacy_nlp\n",
    "    )\n",
    "\n",
    "    print(\"All complexity measures (per row):\")\n",
    "    pprint(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f4165",
   "metadata": {},
   "source": [
    "Pay attention when using the function and ensure proper error handling for NaN values. As a rule, if any complexity dimension produces NaN values for a sample, that dimension must be discarded and not included in the subsequent model training or analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afebc35",
   "metadata": {},
   "source": [
    "**It is strongly recommended to implement a function that incorporates a backup strategy in case errors occur during execution (e.g., IO errors). Please note that if it is impossible to calculate a measure for at least one row (e.g., NaN value), that row must be discarded. At the end of this process, the goal is to obtain a dataframe with 16 columns. The columns should include Simple and Complex, followed by 7 columns containing the measures for the Simple text, and the final 7 columns containing the complexity measures for the Complex text (pay attention to use distinct names for the Simple and Complex columns.)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
