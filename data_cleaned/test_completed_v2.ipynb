{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ed9f53",
   "metadata": {},
   "source": [
    "# The Complexity Stress Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aafe23",
   "metadata": {},
   "source": [
    "First, install the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6403bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textcomplexity\n",
      "  Downloading textcomplexity-0.11.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textcomplexity) (3.4.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textcomplexity) (3.9.1)\n",
      "Collecting nltk-tgrep (from textcomplexity)\n",
      "  Downloading nltk_tgrep-1.0.6-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textcomplexity) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textcomplexity) (1.13.1)\n",
      "Requirement already satisfied: click in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->textcomplexity) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->textcomplexity) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->textcomplexity) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->textcomplexity) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk->textcomplexity) (0.4.6)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk-tgrep->textcomplexity) (3.2.0)\n",
      "Downloading textcomplexity-0.11.0-py3-none-any.whl (90 kB)\n",
      "Downloading nltk_tgrep-1.0.6-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: nltk-tgrep, textcomplexity\n",
      "\n",
      "   -------------------- ------------------- 1/2 [textcomplexity]\n",
      "   ---------------------------------------- 2/2 [textcomplexity]\n",
      "\n",
      "Successfully installed nltk-tgrep-1.0.6 textcomplexity-0.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading stanza-1.11.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (6.30.2)\n",
      "Requirement already satisfied: requests in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (3.4.2)\n",
      "Requirement already satisfied: tomli in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (2.2.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (2.7.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->stanza) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.13.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->stanza) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->stanza) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->stanza) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->stanza) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.13.0->stanza) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Downloading stanza-1.11.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 13.2 MB/s eta 0:00:00\n",
      "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "   ---------------------------------------- 0.0/608.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 608.4/608.4 kB 11.2 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji, stanza\n",
      "\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   -------------------- ------------------- 1/2 [stanza]\n",
      "   ---------------------------------------- 2/2 [stanza]\n",
      "\n",
      "Successfully installed emoji-2.15.0 stanza-1.11.0\n",
      "Collecting wordfreq"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading wordfreq-3.1.1-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ftfy>=6.1 (from wordfreq)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting langcodes>=3.0 (from wordfreq)\n",
      "  Downloading langcodes-3.5.1-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting locate<2.0.0,>=1.1.1 (from wordfreq)\n",
      "  Downloading locate-1.1.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.7 (from wordfreq)\n",
      "  Downloading msgpack-1.1.2-cp310-cp310-win_amd64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: regex>=2023.10.3 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wordfreq) (2024.11.6)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
      "Downloading wordfreq-3.1.1-py3-none-any.whl (56.8 MB)\n",
      "   ---------------------------------------- 0.0/56.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 7.6/56.8 MB 52.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 23.6/56.8 MB 64.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 37.5/56.8 MB 66.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 48.5/56.8 MB 60.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 56.8/56.8 MB 28.1 MB/s eta 0:00:00\n",
      "Downloading locate-1.1.1-py3-none-any.whl (5.4 kB)\n",
      "Downloading msgpack-1.1.2-cp310-cp310-win_amd64.whl (71 kB)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading langcodes-3.5.1-py3-none-any.whl (183 kB)\n",
      "Installing collected packages: msgpack, locate, langcodes, ftfy, wordfreq\n",
      "\n",
      "   ---------------- ----------------------- 2/5 [langcodes]\n",
      "   ------------------------ --------------- 3/5 [ftfy]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   -------------------------------- ------- 4/5 [wordfreq]\n",
      "   ---------------------------------------- 5/5 [wordfreq]\n",
      "\n",
      "Successfully installed ftfy-6.3.1 langcodes-3.5.1 locate-1.1.1 msgpack-1.1.2 wordfreq-3.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python310\\python.exe: No module named spacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.67.1)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp310-cp310-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp310-cp310-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (80.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\rroll\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp310-cp310-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rroll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Downloading spacy-3.8.11-cp310-cp310-win_amd64.whl (15.3 MB)\n",
      "   ---------------------------------------- 0.0/15.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 8.4/15.3 MB 52.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.2/15.3 MB 59.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.3/15.3 MB 35.7 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp310-cp310-win_amd64.whl (40 kB)\n",
      "Downloading murmurhash-1.0.15-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.12-cp310-cp310-win_amd64.whl (117 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp310-cp310-win_amd64.whl (654 kB)\n",
      "   ---------------------------------------- 0.0/654.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 654.0/654.0 kB 12.6 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.10-cp310-cp310-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 24.3 MB/s eta 0:00:00\n",
      "Downloading blis-1.3.3-cp310-cp310-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 38.0 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Installing collected packages: wasabi, spacy-loggers, spacy-legacy, murmurhash, cymem, cloudpathlib, catalogue, blis, typer-slim, srsly, preshed, confection, weasel, thinc, spacy\n",
      "\n",
      "   ----------------------------------------  0/15 [wasabi]\n",
      "   -- -------------------------------------  1/15 [spacy-loggers]\n",
      "   ----- ----------------------------------  2/15 [spacy-legacy]\n",
      "   ---------- -----------------------------  4/15 [cymem]\n",
      "   ------------- --------------------------  5/15 [cloudpathlib]\n",
      "   ---------------- -----------------------  6/15 [catalogue]\n",
      "   ------------------ ---------------------  7/15 [blis]\n",
      "   --------------------- ------------------  8/15 [typer-slim]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ------------------------ ---------------  9/15 [srsly]\n",
      "   ----------------------------- ---------- 11/15 [confection]\n",
      "   ----------------------------- ---------- 11/15 [confection]\n",
      "   -------------------------------- ------- 12/15 [weasel]\n",
      "   -------------------------------- ------- 12/15 [weasel]\n",
      "   -------------------------------- ------- 12/15 [weasel]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ---------------------------------- ----- 13/15 [thinc]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ------------------------------------- -- 14/15 [spacy]\n",
      "   ---------------------------------------- 15/15 [spacy]\n",
      "\n",
      "Successfully installed blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 typer-slim-0.20.0 wasabi-1.1.3 weasel-0.4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install textcomplexity\n",
    "!{sys.executable} -m pip install stanza\n",
    "!{sys.executable} -m pip install wordfreq \n",
    "!{sys.executable} -m spacy download en_core_web_md\n",
    "!{sys.executable} -m pip install tqdm spacy numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625ec67",
   "metadata": {},
   "source": [
    "First, import the following Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6789f29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 45.3MB/s]                    \n",
      "2025-12-15 15:07:15 INFO: Downloaded file to C:\\Users\\rroll\\stanza_resources\\resources.json\n",
      "2025-12-15 15:07:15 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-12-15 15:07:19 INFO: File exists: C:\\Users\\rroll\\stanza_resources\\en\\default.zip\n",
      "2025-12-15 15:07:26 INFO: Finished downloading models and saved to C:\\Users\\rroll\\stanza_resources\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rroll\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rroll\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x260970c8dc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "from pprint import pprint\n",
    "from typing import Dict, Set, Iterable, Optional, Any, Tuple\n",
    "import importlib.resources as pkg_resources\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "import stanza\n",
    "import textcomplexity  # only used to access en.json\n",
    "from tqdm.auto import tqdm  \n",
    "\n",
    "# Download required resources\n",
    "stanza.download('en')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Make sure WordNet is available; if not, download it.\n",
    "try:\n",
    "    _ = wn.synsets(\"dog\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "spacy_nlp = nlp\n",
    "spacy_nlp.add_pipe(\"sentencizer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57283f",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eef7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets ={'ose_adv_ele':'data_sampled/OSE_adv_ele.txt', \n",
    "           'ose_adv_int':'data_sampled/OSE_adv_int.txt',\n",
    "           'swipe': 'data_sampled/swipe.txt',\n",
    "           'vikidia':'data_sampled/vikidia.txt'}\n",
    "\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path, sep='\\t')\n",
    "    \n",
    "\n",
    "def load_dataset(name):\n",
    "    if name not in datasets:\n",
    "        raise ValueError(f\"Dataset {name} not found\")\n",
    "    return load_data(datasets[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c93eb1",
   "metadata": {},
   "source": [
    "Let's load one of the datasets, in this case \"ose_adv_ele\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae91d38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Simple</th>\n",
       "      <th>Complex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿To tourists, Amsterdam still seems very liber...</td>\n",
       "      <td>﻿Amsterdam still looks liberal to tourists, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>﻿Anitta, a music star from Brazil, has million...</td>\n",
       "      <td>﻿Brazil’s latest funk sensation, Anitta, has w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Simple  \\\n",
       "0  ﻿When you see the word Amazon, what’s the firs...   \n",
       "1  ﻿To tourists, Amsterdam still seems very liber...   \n",
       "2  ﻿Anitta, a music star from Brazil, has million...   \n",
       "\n",
       "                                             Complex  \n",
       "0  ﻿When you see the word Amazon, what’s the firs...  \n",
       "1  ﻿Amsterdam still looks liberal to tourists, wh...  \n",
       "2  ﻿Brazil’s latest funk sensation, Anitta, has w...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('ose_adv_ele')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb2dc3",
   "metadata": {},
   "source": [
    "Let's look at a random row of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7bb885b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE TEXT\n",
      "﻿Maria is waiting on a black plastic chair. When she is called, she picks up a brown paper bag full of food: pasta, eggs and cornflakes. She can also choose between butternut squash or carrots as this week’s vegetables.\n",
      "Maria is the 34th “client” today at East Hampton Food Pantry, very close to some of the most expensive houses in the world.\n",
      "Every day in the winter, more than 400 families collect their weekly food parcel from the food pantry. The food helps them get through the cold, dark Long Island winter.\n",
      "The Hamptons are historic, oceanfront towns and villages 100 miles from Manhattan, New York. In the summer, it is full of billionaires. But, in early September, the rich and famous shut up their mansions and go back to Manhattan or Beverly Hills. The people who live here all year are mostly immigrants.\n",
      "“The people who come here are rich and famous but we who live here are not,” says Maria. She works 14-hour days in the summer cleaning mansions. She often has no work at all in the winter.\n",
      "Maria laughs when asked if she has enough money. “There is no work in the winter, only in the summer,” says Maria. She, like many of the workers in the Hamptons, is from Latin America. “Here, lots of people live in a single room because they can’t pay the rent.”\n",
      "Lots of her friends can’t pay for heating or medicine and many would be hungry if they did not get food from the East Hampton Food Pantry, she says.\n",
      "Vicki Littman, chairperson of the East Hampton Food Pantry, which gave more than 31,000 food parcels in 2015, says there are more and more people coming to the food pantry.\n",
      "Littman says that, when she talks to the people who come for the summer about the food pantries, they are always shocked. They know only the glamorous side of the Hamptons: the big parties and the beaches and mansions.\n",
      "“But, what the rich people don’t know is that the gardeners, the nannies, the waitresses, they all need their summer earnings to get them through the winter.”\n",
      "Housing is the biggest cost in the Hamptons. Larry Cantwell, who has lived in East Hampton all his life, says homes often cost more than $25 million. “It is very difficult to find your first home here,” Cantwell says. “If you can find a home to buy anywhere in East Hampton for less than $500,000, you’re very lucky.”\n",
      "Cantwell says more than half the town’s homes are empty for most of the year. The population goes from 80,000 in August to 10,000 in the winter months.\n",
      "“There’s a lot of wealth here but almost all of that wealth is in second homes only used in the summer,” says Cantwell, the son of a fisherman father and a house-cleaner mother. “But, the rest of us live here all year.”\n",
      "“There are famous and very wealthy people but also hard-working and poor people. You’ve got to remember that this used to be a farming and fishing community – a real working-class community.”\n",
      "Eddie Vallone, 22, says, “People only see the Hamptons as a rich town but there are a lot of problems here, especially drugs. It’s hard to understand. You think, ‘OK, the summer is over. What am I going to do for the winter?’” Vallone says, “I want to work but there’s no work.”\n",
      "Vallone works cleaning swimming pools. He says that, if he is careful, he can make his summer earnings last until November. “But, work doesn’t start again until May or the beginning of June.”\n",
      "----------------------------------------------------------------------------------------------------\n",
      "COMPLEX TEXT\n",
      "﻿Maria is sitting on a black plastic chair in a community centre on a cold Tuesday afternoon waiting for her number to be called. She is number 34.\n",
      "When it’s her turn, Maria is called forward to pick up a brown paper bag filled with essentials including pasta, eggs and cornflakes, and is invited to choose between butternut squash or carrots as this week’s vegetables.\n",
      "Maria is the 34th “client” so far today at East Hampton Food Pantry, a community initiative set up just streets away from some of the most expensive and exclusive properties in the world. By the end of the day, the food pantry’s organizers expect more than 400 families to have followed Maria through the doors to collect their weekly food parcel to help them get through the cold, dark Long Island winter.\n",
      "In the summertime, the Hamptons, a collection of historic oceanfront towns and villages 100 miles from Manhattan, is a billionaires’ playground. But, come Labor Day in early September, when the rich and famous shut up their mansions and head back to Manhattan or Beverly Hills, the glitz gives way to the gritty reality of life for the mostly immigrant community who live here all year. “The people who come here are rich and famous but we who live here are not,” says Maria, who works 14-hour days in the summer cleaning mansions but goes months without any work at all in the winter.\n",
      "Maria laughs when asked if she has enough money. “There is no work in the winter, only in the summertime,” says Maria, who, like many of the workers in the Hamptons, is from Latin America. “Here, lots of people live in a single room because they can’t pay the rent.”\n",
      "She says some families with up to five children are crammed into basements and still pay more than $1,000 a month in rent. “People come here looking for work but, in the winter, there is nothing.”\n",
      "Lots of her friends can’t pay for heating or medication and many would go hungry if it were not for the East Hampton Food Pantry, she says, which is just one of several food pantries in the town.\n",
      "Vicki Littman, chairperson of the East Hampton Food Pantry, which provided more than 31,000 food parcels in 2015, says the number of people seeking out the food pantry is ever increasing. “Once Labor Day comes and the season is over and people’s hours start to be cut back, our numbers go up to about 400 families a week,” she says. “When they come to us on Tuesday, they get two to three days’ worth of food. Without us, they would struggle that much more.” Littman says it can be hard for outsiders to realize that there are people struggling to get by in a place known the world over for its excess.\n",
      "“When I discuss with the summer community that comes out here about the food pantries, they’re always shocked because there is that glamorous side of the Hamptons where there are galas and the beaches and the mansions that are here. “But, what people don’t realize is that there is that service industry. It’s the landscapers, the nannies, the waitresses – they are all relying on that summer income to get them through the winter but people don’t see that when they’re coming out on holiday.\n",
      "“There are seniors who have to sometimes pick between whether they are going to pay for their medications or pay their bills or buy food, and that shouldn’t be the case.”\n",
      "Littman says the town has lost too many people working key jobs – such as teachers, police officers and even doctors and dentists – because they can’t afford to live in the community and the food pantry board is determined to do more to ensure people have a better shot at staying put. Housing is, by far, the biggest cost in the Hamptons. At $147m, the nation’s most expensive property is hedge fund manager Barry Rosenstein’s 18-acre beachfront estate at 67 Further Lane, a stone’s throw from Maidstone Golf Club, which is considered “the most elite, prestigious and difficult to get into” of all the Hamptons clubs.\n",
      "Larry Cantwell, East Hampton’s town supervisor and lifelong resident, says homes regularly change hands at more than $25m and the rapid price inflation at the top end has trickled down to even the town’s most modest flats.\n",
      "“Finding your first home is a challenge in an area like this,” Cantwell says. “Not just people who you would characterize as poor – working- and middle-class families are also having a hard time. If you can find a home to buy anywhere in East Hampton for $500,000, you’re very lucky.” Cantwell says more than half the town’s homes are empty for most of the year, which causes the population to dwindle to as little as 10,000 in the winter months compared with 80,000 in August. “It’s kind of the tale of two cities. There’s certainly a lot of wealth here but almost all of that wealth is in second homes only used in the summer,” says Cantwell, the son of a fisherman father and a house-cleaner mother. “But, the rest of us live here year round.\n",
      "“There are famous and very wealthy people but then you have hard-working and poor people struggling to get by. You’ve got to remember that this community was founded as a farming and fishing community of people who lived off the land and the water – a real working-class community.”\n",
      "Cantwell says saving up enough money to buy your first house while working as a farmer or fisherman is near-impossible in East Hampton today “and it’s not just the poor – police officers, teachers, young professionals and others all struggle to find a place to live here and many of them cannot afford to own their own home.”\n",
      "Being homeless in the Hamptons means spending a lot of time on a bus. Various houses of worship have joined together to ensure there is somewhere for the homeless to spend the night over the winter. Churches up and down the north and south fork of Long Island take on the burden one night at a time and roughly 50 homeless people are bussed between them every day. Eddie Vallone, 22, is one of those on the bus every night. “People look at the Hamptons as some sort of rich town and there’s no problems going on. But there are a lot of problems here, especially drugs.\n",
      "“It’s hard to really grasp –'OK, the summer is coming to an end. What am I going to do for the winter?'” Vallone says at Maureen’s Haven, a charity that coordinates the homeless shelter programme. “I want to work but there’s no work to be done.”\n",
      "Vallone, who works cleaning pools and doing odd jobs on luxury estates, says that, if he saves well and doesn’t impulse-buy, he can make his summer earnings stretch out until November. “But, work doesn’t start again until May or the beginning of June.”\n"
     ]
    }
   ],
   "source": [
    "row = df.sample(1)\n",
    "\n",
    "print('SIMPLE TEXT')\n",
    "print(row['Simple'].iloc[0])\n",
    "print('-'*100)\n",
    "print('COMPLEX TEXT')\n",
    "print(row['Complex'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ebbb2",
   "metadata": {},
   "source": [
    "Let's look at the size of each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72529061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ose_adv_ele: 189 rows\n",
      "ose_adv_int: 189 rows\n",
      "swipe: 1233 rows\n",
      "vikidia: 1233 rows\n",
      "Total: 2844 rows\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for name, path in datasets.items():\n",
    "    df = load_dataset(name)\n",
    "    print(f\"{name}: {df.shape[0]} rows\")\n",
    "    cnt += df.shape[0]\n",
    "print(f\"Total: {cnt} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571b7484",
   "metadata": {},
   "source": [
    "Let's load again the dataset for computing the complexity measure in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b7e233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset('ose_adv_ele')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd81ace0",
   "metadata": {},
   "source": [
    "## Complexity measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d9ff4",
   "metadata": {},
   "source": [
    "We provide the function for computing the complexity measures with respect to the lexical, syntactic, and discourse dimensions. It is worth noticing that complexity functions on the datasets should be computed offline beforehand and the results saved, as they are computationally expensive. Specifically, for both the Simple and Complex texts, the complexity measures should be calculated first and stored (e.g., in Pandas DataFrames with the new complexity feature columns added) before feeding the augmented datasets to the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d91084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cache stanza pipelines to avoid re-loading models\n",
    "_STANZA_PIPELINES: Dict[str, stanza.Pipeline] = {}\n",
    "\n",
    "# UPOS tags considered content words (C)\n",
    "CONTENT_UPOS = {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def load_cow_top5000_en() -> Set[str]:\n",
    "    \"\"\"\n",
    "    Load the COW-based list of the 5,000 most frequent English content words\n",
    "    from textcomplexity's English language definition file (en.json).\n",
    "\n",
    "    We ignore POS tags and keep only lowercased word forms.\n",
    "    \"\"\"\n",
    "    with pkg_resources.files(textcomplexity).joinpath(\"en.json\").open(\n",
    "        \"r\", encoding=\"utf-8\"\n",
    "    ) as f:\n",
    "        lang_def = json.load(f)\n",
    "\n",
    "    most_common = lang_def[\"most_common\"]  # list of [word, xpos]\n",
    "    cow_top5000 = {w.lower() for w, xpos in most_common}\n",
    "    return cow_top5000\n",
    "\n",
    "\n",
    "def get_stanza_pipeline(lang: str = \"en\", use_gpu: bool = False) -> stanza.Pipeline:\n",
    "    \"\"\"\n",
    "    Get (or create) a cached stanza Pipeline for a given language.\n",
    "\n",
    "    NOTE: You must have downloaded the models beforehand, e.g.:\n",
    "        import stanza\n",
    "        stanza.download('en')\n",
    "    \"\"\"\n",
    "    if lang not in _STANZA_PIPELINES:\n",
    "        _STANZA_PIPELINES[lang] = stanza.Pipeline(\n",
    "            lang=lang,\n",
    "            processors=\"tokenize,pos,lemma,depparse,constituency\",\n",
    "            use_gpu=use_gpu,\n",
    "            tokenize_no_ssplit=False,\n",
    "        )\n",
    "    return _STANZA_PIPELINES[lang]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a53cb",
   "metadata": {},
   "source": [
    "### Lexical complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "929089ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_mtld(tokens: Iterable[str], ttr_threshold: float = 0.72) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute MTLD (Measure of Textual Lexical Diversity) for a list of tokens.\n",
    "\n",
    "    MTLD = total_number_of_tokens / number_of_factors\n",
    "\n",
    "    A factor is a contiguous segment where the running TTR stays >= threshold.\n",
    "    When the TTR drops below the threshold, we close a factor (at the previous\n",
    "    token) and start a new one. At the end, the remaining partial segment is\n",
    "    counted as a fractional factor, with weight proportional to how close the\n",
    "    final TTR is to the threshold.\n",
    "    \"\"\"\n",
    "    tokens = [tok for tok in tokens if tok]\n",
    "    if not tokens:\n",
    "        return None\n",
    "\n",
    "    types = set()\n",
    "    factor_count = 0.0\n",
    "    token_count_in_factor = 0\n",
    "\n",
    "    for tok in tokens:\n",
    "        token_count_in_factor += 1\n",
    "        types.add(tok)\n",
    "        ttr = len(types) / token_count_in_factor\n",
    "\n",
    "        if ttr < ttr_threshold:\n",
    "            factor_count += 1.0\n",
    "            types = set()\n",
    "            token_count_in_factor = 0\n",
    "\n",
    "    # final partial factor\n",
    "    if token_count_in_factor > 0:\n",
    "        final_ttr = len(types) / token_count_in_factor\n",
    "        if final_ttr < 1.0:\n",
    "            fractional = (1.0 - final_ttr) / (1.0 - ttr_threshold)\n",
    "            fractional = max(0.0, min(1.0, fractional))\n",
    "            factor_count += fractional\n",
    "\n",
    "    if factor_count == 0:\n",
    "        return None\n",
    "\n",
    "    return len(tokens) / factor_count\n",
    "\n",
    "\n",
    "\n",
    "def _compute_lexical_density(total_tokens: int, content_tokens: int) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LD = |C| / |T|\n",
    "    where:\n",
    "        |C| = number of content-word tokens\n",
    "        |T| = total number of non-punctuation tokens\n",
    "    \"\"\"\n",
    "    if total_tokens == 0:\n",
    "        return None\n",
    "    return content_tokens / total_tokens\n",
    "\n",
    "\n",
    "def _compute_lexical_sophistication_cow(\n",
    "    content_forms: Iterable[str],\n",
    "    cow_top5000: set,\n",
    ") -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LS = |{ w in C : w not in R }| / |C|\n",
    "    where:\n",
    "        C = content-word tokens (surface forms, lowercased)\n",
    "        R = COW top-5000 content word forms (lowercased)\n",
    "    \"\"\"\n",
    "    forms = [f for f in content_forms if f]\n",
    "    if not forms:\n",
    "        return None\n",
    "\n",
    "    off_list = sum(1 for f in forms if f not in cow_top5000)\n",
    "    return off_list / len(forms)\n",
    "\n",
    "\n",
    "\n",
    "def lexical_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MTLD, LD, LS from a stanza Document.\n",
    "    \"\"\"\n",
    "    cow_top5000 = load_cow_top5000_en()\n",
    "\n",
    "    mtld_tokens = []\n",
    "    total_tokens = 0\n",
    "    content_tokens = 0\n",
    "    content_forms = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.upos == \"PUNCT\":\n",
    "                continue\n",
    "\n",
    "            lemma = (word.lemma or word.text or \"\").lower()\n",
    "            if not lemma:\n",
    "                continue\n",
    "\n",
    "            mtld_tokens.append(lemma)\n",
    "            total_tokens += 1\n",
    "\n",
    "            if word.upos in CONTENT_UPOS:\n",
    "                content_tokens += 1\n",
    "                form = (word.text or \"\").lower()\n",
    "                content_forms.append(form)\n",
    "\n",
    "    mtld = _compute_mtld(mtld_tokens) if mtld_tokens else None\n",
    "    ld = _compute_lexical_density(total_tokens, content_tokens)\n",
    "    ls = _compute_lexical_sophistication_cow(content_forms, cow_top5000)\n",
    "\n",
    "    return {\"MTLD\": mtld, \"LD\": ld, \"LS\": ls}\n",
    "\n",
    "\n",
    "def lexical_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute lexical measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return lexical_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute lexical measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = lexical_measures_from_text(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "\n",
    "    return {\"MTLD\": mtld_res, \"LD\": ld_res, \"LS\": ls_res}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4583f900",
   "metadata": {},
   "source": [
    "### Syntactic complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f6ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mdd_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute Mean Dependency Distance (MDD) from a stanza Document.\n",
    "\n",
    "    For each sentence s_i with dependency set D_i:\n",
    "        MDD_i = (1 / |D_i|) * sum_{(h,d) in D_i} |h - d|\n",
    "    Then:\n",
    "        MDD = (1 / k) * sum_i MDD_i, over all sentences with at least one dependency.\n",
    "    \"\"\"\n",
    "    sentence_mdds = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        distances = []\n",
    "        for w in sent.words:\n",
    "            if w.head is None or w.head == 0:\n",
    "                continue\n",
    "            distances.append(abs(w.id - w.head))\n",
    "\n",
    "        if distances:\n",
    "            sentence_mdds.append(sum(distances) / len(distances))\n",
    "\n",
    "    if not sentence_mdds:\n",
    "        return None\n",
    "    return sum(sentence_mdds) / len(sentence_mdds)\n",
    "\n",
    "\n",
    "\n",
    "def _count_clauses_in_tree(tree) -> int:\n",
    "    \"\"\"\n",
    "    Count clause nodes in a constituency tree.\n",
    "\n",
    "    A simple and standard heuristic (PTB-style) is:\n",
    "        count all nodes whose label starts with 'S'\n",
    "        (S, SBAR, SBARQ, SINV, SQ, etc.).\n",
    "\n",
    "    This aligns with the idea of counting finite and subordinate clauses\n",
    "    as in Hunt (1965) and later complexity work.\n",
    "    \"\"\"\n",
    "    if tree is None:\n",
    "        return 0\n",
    "\n",
    "    # Stanza's constituency tree: tree.label, tree.children\n",
    "    count = 1 if getattr(tree, \"label\", \"\").startswith(\"S\") else 0\n",
    "\n",
    "    for child in getattr(tree, \"children\", []):\n",
    "        # leaves can be strings or terminals without 'label'\n",
    "        if hasattr(child, \"label\"):\n",
    "            count += _count_clauses_in_tree(child)\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def cs_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute CS (clauses per sentence) from a stanza Document.\n",
    "\n",
    "        CS = (1 / k) * sum_i L_i\n",
    "\n",
    "    where L_i is the number of clauses in sentence s_i, estimated by counting\n",
    "    all constituents whose label starts with 'S' in the constituency tree of s_i.\n",
    "    \"\"\"\n",
    "    clause_counts = []\n",
    "    for sent in doc.sentences:\n",
    "        tree = getattr(sent, \"constituency\", None)\n",
    "        if tree is None:\n",
    "            # No constituency tree available for this sentence\n",
    "            continue\n",
    "        num_clauses = _count_clauses_in_tree(tree)\n",
    "        clause_counts.append(num_clauses)\n",
    "\n",
    "    if not clause_counts:\n",
    "        return None\n",
    "\n",
    "    return sum(clause_counts) / len(clause_counts)\n",
    "\n",
    "\n",
    "\n",
    "def syntactic_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MDD and CS from a stanza Document.\n",
    "    \"\"\"\n",
    "    mdd = mdd_from_doc(doc)\n",
    "    cs = cs_from_doc(doc)\n",
    "    return {\"MDD\": mdd, \"CS\": cs}\n",
    "\n",
    "\n",
    "def syntactic_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute syntactic measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return syntactic_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "def compute_syntactic_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute syntactic measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MDD\": {index: value},\n",
    "            \"CS\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = syntactic_measures_from_text(text, lang=lang)\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "    return {\"MDD\": mdd_res, \"CS\": cs_res}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20110b8",
   "metadata": {},
   "source": [
    "### Discourse complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "739a62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Approximate set of content POS tags (spaCy universal POS)\n",
    "CONTENT_POS =  {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "def is_content_token(tok):\n",
    "    \"\"\"\n",
    "    Return True if token is considered a content word.\n",
    "    We ignore stopwords, punctuation, and non-alphabetic tokens.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        tok.is_alpha\n",
    "        and not tok.is_stop\n",
    "        and tok.pos_ in CONTENT_POS\n",
    "    )\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=100000)\n",
    "def get_related_lemmas(lemma):\n",
    "    \"\"\"\n",
    "    Return a set of semantically related lemmas for the given lemma\n",
    "    using WordNet, including:\n",
    "      - synonyms\n",
    "      - antonyms\n",
    "      - hypernyms / hyponyms\n",
    "      - meronyms (part/member/substance)\n",
    "      - coordinate terms (siblings under the same hypernym)\n",
    "\n",
    "    NOTE: Some older examples mention 'troponyms', but in NLTK's\n",
    "    WordNet interface there is no 'troponyms()' method on Synset,\n",
    "    so we do NOT use it here.\n",
    "    \"\"\"\n",
    "    lemma = lemma.lower()\n",
    "    related = set()\n",
    "    synsets = wn.synsets(lemma)\n",
    "\n",
    "    for syn in synsets:\n",
    "        # Synonyms and antonyms\n",
    "        for l in syn.lemmas():\n",
    "            related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "            for ant in l.antonyms():\n",
    "                related.add(ant.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Hypernyms (more general) and hyponyms (more specific)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for l in hyper.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "        for hypo in syn.hyponyms():\n",
    "            for l in hypo.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Meronyms: part/member/substance\n",
    "        for mer in syn.part_meronyms() + syn.member_meronyms() + syn.substance_meronyms():\n",
    "            for l in mer.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Coordinate terms (siblings under same hypernym)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for sibling in hyper.hyponyms():\n",
    "                if sibling == syn:\n",
    "                    continue\n",
    "                for l in sibling.lemmas():\n",
    "                    related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "    # Remove the lemma itself if present\n",
    "    related.discard(lemma)\n",
    "    return related\n",
    "\n",
    "\n",
    "def lexical_cohesion_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Lexical Cohesion (LC) for a single document:\n",
    "\n",
    "        LC = |C| / m\n",
    "\n",
    "    where:\n",
    "      - |C| is the number of cohesive devices between sentences\n",
    "        (lexical repetition + semantic relations),\n",
    "      - m  is the total number of word tokens (alphabetic) in the document.\n",
    "\n",
    "    If the document has fewer than 2 sentences or no valid words,\n",
    "    LC is returned as 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Total number of alphabetic tokens (denominator m)\n",
    "    m = sum(1 for tok in doc if tok.is_alpha)\n",
    "    if m == 0:\n",
    "        return 0.0\n",
    "\n",
    "    sentences = list(doc.sents)\n",
    "    if len(sentences) < 2:\n",
    "        # With only one sentence, cross-sentence cohesion is not defined\n",
    "        return 0.0\n",
    "\n",
    "    # Collect sets of content lemmas per sentence\n",
    "    sent_lemmas = []\n",
    "    for sent in sentences:\n",
    "        lemmas = set(\n",
    "            tok.lemma_.lower()\n",
    "            for tok in sent\n",
    "            if is_content_token(tok)\n",
    "        )\n",
    "        if lemmas:\n",
    "            sent_lemmas.append(lemmas)\n",
    "\n",
    "    if len(sent_lemmas) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    cohesive_count = 0\n",
    "\n",
    "    for i in range(len(sent_lemmas) - 1):\n",
    "        for j in range(i + 1, len(sent_lemmas)):\n",
    "            li = sent_lemmas[i]\n",
    "            lj = sent_lemmas[j]\n",
    "\n",
    "            # 1) Lexical repetition: shared lemmas\n",
    "            shared = li & lj\n",
    "            cohesive_count += len(shared)\n",
    "\n",
    "            # 2) Semantic relations via WordNet\n",
    "            for lemma in li:\n",
    "                related = get_related_lemmas(lemma)\n",
    "                cohesive_count += len(related & lj)\n",
    "\n",
    "    return float(cohesive_count) / float(m)\n",
    "\n",
    "\n",
    "def sentence_vector(sent, vector_size):\n",
    "    \"\"\"\n",
    "    Represent a sentence as the average of token vectors.\n",
    "    If no token has a vector, return a zero vector.\n",
    "    \"\"\"\n",
    "    vecs = [\n",
    "        tok.vector\n",
    "        for tok in sent\n",
    "        if tok.has_vector and not tok.is_punct and not tok.is_space\n",
    "    ]\n",
    "    if not vecs:\n",
    "        return np.zeros(vector_size, dtype=\"float32\")\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "def coherence_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Coherence (CoH) for a single document as the average\n",
    "    cosine similarity between adjacent sentence vectors:\n",
    "\n",
    "        CoH = (1 / (k-1)) * sum_{i=1}^{k-1} cos(h_i, h_{i+1})\n",
    "\n",
    "    where h_i is the sentence/topic vector for sentence i.\n",
    "\n",
    "    If the document has fewer than 2 sentences, CoH = 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    if nlp.vocab.vectors_length == 0:\n",
    "        raise ValueError(\n",
    "            \"The loaded spaCy model does not contain word vectors \"\n",
    "            \"(nlp.vocab.vectors_length == 0). \"\n",
    "            \"Use a model like 'en_core_web_md' or similar.\"\n",
    "        )\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    k = len(sentences)\n",
    "\n",
    "    if k < 2:\n",
    "        # Only one sentence: no adjacent pair, coherence = 0.0\n",
    "        return 0.0\n",
    "\n",
    "    vector_size = nlp.vocab.vectors_length\n",
    "    sent_vectors = [\n",
    "        sentence_vector(sent, vector_size)\n",
    "        for sent in sentences\n",
    "    ]\n",
    "\n",
    "    sims = []\n",
    "    for i in range(k - 1):\n",
    "        v1 = sent_vectors[i]\n",
    "        v2 = sent_vectors[i + 1]\n",
    "        norm1 = np.linalg.norm(v1)\n",
    "        norm2 = np.linalg.norm(v2)\n",
    "        denom = norm1 * norm2\n",
    "        if denom == 0.0:\n",
    "            # Skip pairs where at least one sentence vector is zero\n",
    "            continue\n",
    "        cos_sim = float(np.dot(v1, v2) / denom)\n",
    "        sims.append(cos_sim)\n",
    "\n",
    "    if not sims:\n",
    "        return 0.0\n",
    "\n",
    "    return float(np.mean(sims))\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_cohesion_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute LC for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with lemmatizer, POS tagger, etc.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of LC scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [lexical_cohesion_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_coherence_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute CoH for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with word vectors.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of CoH scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [coherence_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_discourse_measures(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute both LC and CoH for each row of a DataFrame and return\n",
    "    them in a dictionary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"LC\":  np.ndarray of lexical cohesion scores,\n",
    "            \"CoH\": np.ndarray of coherence scores\n",
    "        }\n",
    "    \"\"\"\n",
    "    lc_vec = compute_lexical_cohesion_vector(df, nlp, column=column)\n",
    "    coh_vec = compute_coherence_vector(df, nlp, column=column)\n",
    "    return {\"LC\": lc_vec, \"CoH\": coh_vec}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70f0bb",
   "metadata": {},
   "source": [
    "### Text complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c285251",
   "metadata": {},
   "source": [
    "Here we compute the complexity of each function. Note that we use a method that calculates all measures at once. However, it is advisable to compute each measure separately so that you can better handle any potential errors. For example, calculate MLTD first and save it, then LD, and so on. The following code is an example of how to compute the measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88a6a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze_text_all(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Parse a text with stanza and compute all measures (lexical + syntactic)\n",
    "    in a single pass.\n",
    "\n",
    "    Returns a dict with keys:\n",
    "        \"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\"\n",
    "    (Discourse measures LC/CoH are added later at DataFrame level, via spaCy.)\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None, \"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    lex = lexical_measures_from_doc(doc)\n",
    "    syn = syntactic_measures_from_doc(doc)\n",
    "\n",
    "    out: Dict[str, Optional[float]] = {}\n",
    "    out.update(lex)\n",
    "    out.update(syn)\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_all_complexity_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    "    spacy_nlp=None,\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute all complexity measures for each row in df[column].\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with a text column.\n",
    "    column : str, default \"text\"\n",
    "        Name of the text column.\n",
    "    lang : str, default \"en\"\n",
    "        Language code for stanza.\n",
    "    n_jobs : int, default 1\n",
    "        Number of worker processes to use.\n",
    "            - 1  : sequential execution (no multiprocessing).\n",
    "            - >1 : multiprocessing with that many workers.\n",
    "            - 0 or None : use cpu_count() workers.\n",
    "    spacy_nlp : spaCy Language, required for LC / CoH\n",
    "        Pre-loaded spaCy pipeline with:\n",
    "            - POS / lemmatizer for LC\n",
    "            - word vectors for CoH (e.g. 'en_core_web_md').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "            \"MDD\":  {index: value},\n",
    "            \"CS\":   {index: value},\n",
    "            \"LC\":   {index: value},\n",
    "            \"CoH\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    items = list(df[column].items())  # list[(index, text)]\n",
    "    total_items = len(items)\n",
    "\n",
    "    # ---- Lexical + syntactic (stanza) ----\n",
    "    for idx, text in tqdm(\n",
    "        items,\n",
    "        total=total_items,\n",
    "        desc=\"Computing lexical & syntactic complexity (sequential)\",\n",
    "    ):\n",
    "        metrics = _analyze_text_all(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "\n",
    "    # ---- Discourse measures (spaCy: LC & CoH) ----\n",
    "    if spacy_nlp is None:\n",
    "        raise ValueError(\n",
    "            \"spacy_nlp must be provided to compute LC and CoH. \"\n",
    "            \"Load a spaCy model with vectors, e.g. 'en_core_web_md', and \"\n",
    "            \"pass it as spacy_nlp=...\"\n",
    "        )\n",
    "\n",
    "    discourse = compute_discourse_measures(df, spacy_nlp, column=column)\n",
    "    lc_vec = discourse[\"LC\"]\n",
    "    coh_vec = discourse[\"CoH\"]\n",
    "\n",
    "    lc_res: Dict[Any, float] = {}\n",
    "    coh_res: Dict[Any, float] = {}\n",
    "\n",
    "    # Map arrays back to DataFrame indices\n",
    "    for i, idx in enumerate(df.index):\n",
    "        lc_res[idx] = float(lc_vec[i])\n",
    "        coh_res[idx] = float(coh_vec[i])\n",
    "\n",
    "    return {\n",
    "        \"MTLD\": mtld_res,\n",
    "        \"LD\": ld_res,\n",
    "        \"LS\": ls_res,\n",
    "        \"MDD\": mdd_res,\n",
    "        \"CS\": cs_res,\n",
    "        \"LC\": lc_res,\n",
    "        \"CoH\": coh_res,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "914f60f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing lexical & syntactic complexity (sequential):   0%|          | 0/5 [00:00<?, ?it/s]2025-12-15 15:10:06 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 109MB/s]                     \n",
      "2025-12-15 15:10:06 INFO: Downloaded file to C:\\Users\\rroll\\stanza_resources\\resources.json\n",
      "2025-12-15 15:10:06 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-12-15 15:10:08 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "======================================\n",
      "\n",
      "2025-12-15 15:10:08 INFO: Using device: cpu\n",
      "2025-12-15 15:10:08 INFO: Loading: tokenize\n",
      "2025-12-15 15:10:15 INFO: Loading: mwt\n",
      "2025-12-15 15:10:15 INFO: Loading: pos\n",
      "2025-12-15 15:10:20 INFO: Loading: lemma\n",
      "2025-12-15 15:10:21 INFO: Loading: constituency\n",
      "2025-12-15 15:10:22 INFO: Loading: depparse\n",
      "2025-12-15 15:10:23 INFO: Done loading processors!\n",
      "Computing lexical & syntactic complexity (sequential): 100%|██████████| 5/5 [01:25<00:00, 17.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All complexity measures (per row):\n",
      "{'CS': {20: 3.1578947368421053,\n",
      "        35: 2.891891891891892,\n",
      "        46: 4.8,\n",
      "        118: 3.142857142857143,\n",
      "        149: 4.454545454545454},\n",
      " 'CoH': {20: 0.8818265795707703,\n",
      "         35: 0.8385899662971497,\n",
      "         46: 0.8502054214477539,\n",
      "         118: 0.8159902691841125,\n",
      "         149: 0.8151078224182129},\n",
      " 'LC': {20: 1.7914012670516968,\n",
      "        35: 2.0972447395324707,\n",
      "        46: 1.7163375616073608,\n",
      "        118: 0.8380566835403442,\n",
      "        149: 1.4731183052062988},\n",
      " 'LD': {20: 0.48043818466353677,\n",
      "        35: 0.5132605304212169,\n",
      "        46: 0.5365853658536586,\n",
      "        118: 0.5276752767527675,\n",
      "        149: 0.5061425061425061},\n",
      " 'LS': {20: 0.19543973941368079,\n",
      "        35: 0.1580547112462006,\n",
      "        46: 0.22402597402597402,\n",
      "        118: 0.2867132867132867,\n",
      "        149: 0.2766990291262136},\n",
      " 'MDD': {20: 3.1614132869478517,\n",
      "         35: 3.0303509140449383,\n",
      "         46: 3.4593144476420274,\n",
      "         118: 3.053461449526941,\n",
      "         149: 3.249630024533215},\n",
      " 'MTLD': {20: 64.7691970162352,\n",
      "          35: 74.97557840616967,\n",
      "          46: 57.4,\n",
      "          118: 71.8774145616642,\n",
      "          149: 76.70384615384616}}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Example script: load a DataFrame and compute all complexity measures.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df_example = df.sample(n=5) # We sample 5 random rows\n",
    "    # Compute all measures for Simple texts\n",
    "    metrics = compute_all_complexity_measures_df(\n",
    "        df_example,\n",
    "        column=\"Simple\", # Note that we use the column \"Simple\" for the Simple text. Use 'Complex' for the Complex text.\n",
    "        lang=\"en\",\n",
    "\n",
    "        spacy_nlp=spacy_nlp\n",
    "    )\n",
    "\n",
    "    print(\"All complexity measures (per row):\")\n",
    "    pprint(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f4165",
   "metadata": {},
   "source": [
    "Pay attention when using the function and ensure proper error handling for NaN values. As a rule, if any complexity dimension produces NaN values for a sample, that dimension must be discarded and not included in the subsequent model training or analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afebc35",
   "metadata": {},
   "source": [
    "**It is strongly recommended to implement a function that incorporates a backup strategy in case errors occur during execution (e.g., IO errors). Please note that if it is impossible to calculate a measure for at least one row (e.g., NaN value), that row must be discarded. At the end of this process, the goal is to obtain a dataframe with 16 columns. The columns should include Simple and Complex, followed by 7 columns containing the measures for the Simple text, and the final 7 columns containing the complexity measures for the Complex text (pay attention to use distinct names for the Simple and Complex columns.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a124e145",
   "metadata": {},
   "source": [
    "\n",
    "# Préparation des données d'évaluation (offline)\n",
    "\n",
    "Ce bloc ajoute un pipeline **hors ligne** pour :\n",
    "- calculer les **mesures de complexité** pour les deux colonnes *Simple* et *Complex* ;\n",
    "- **détecter la dominance** (lorsque le texte *Simple* est plus complexe que *Complex* sur **toutes** les mesures) ;\n",
    "- **retirer** ces lignes dominantes du jeu d'évaluation tout en **conservant leurs identifiants** originaux ;\n",
    "- **sauvegarder** les trois tables : *complet*, *conservé*, *retiré* (+ la liste d'IDs retirés).\n",
    "\n",
    "> Remarque : nous considérons ici une dominance **forte** : `Simple >= Complex` sur **toutes** les métriques (`MTLD, LD, LS, MDD, CS, LC, CoH`).  \n",
    "> Vous pouvez affiner cette règle (pondérations, seuils, au moins k/7 mesures, etc.) selon vos besoins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd6c9368",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Dépendances attendues depuis le notebook ===\n",
    "# - compute_all_complexity_measures_df(df, column, lang='en') doit être défini plus haut.\n",
    "# - Des helpers de chargement peuvent exister (ex: load_dataset), mais on n'y dépend pas ici.\n",
    "\n",
    "def ensure_id(df: pd.DataFrame, id_col: str = \"orig_id\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Garantit qu'une colonne d'identifiant stable 'orig_id' existe.\n",
    "    Si absente, on la crée à partir de l'index courant (0..n-1).\n",
    "    Ne modifie pas l'ordre des lignes.\n",
    "    \"\"\"\n",
    "    if id_col not in df.columns:\n",
    "        df = df.copy()\n",
    "        df[id_col] = df.index.astype(\"int64\")\n",
    "    return df\n",
    "\n",
    "def compute_pair_measures_df(\n",
    "    df_raw: pd.DataFrame,\n",
    "    col_simple: str = \"Simple\",\n",
    "    col_complex: str = \"Complex\",\n",
    "    lang: str = \"en\",\n",
    "    id_col: str = \"orig_id\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule les mesures de complexité pour *deux colonnes* d'un même DataFrame :\n",
    "    - col_simple -> suffixe '_S'\n",
    "    - col_complex -> suffixe '_C'\n",
    "    Retourne un DataFrame fusionné (aligné par index), qui conserve id_col.\n",
    "    \"\"\"\n",
    "    df_raw = ensure_id(df_raw, id_col=id_col)\n",
    "\n",
    "    # Sanity check colonnes présentes\n",
    "    for c in (col_simple, col_complex):\n",
    "        if c not in df_raw.columns:\n",
    "            raise KeyError(f\"Colonne manquante: {c!r}. Colonnes disponibles: {list(df_raw.columns)}\")\n",
    "\n",
    "    # Calcul des mesures pour chaque colonne\n",
    "    df_simple = compute_all_complexity_measures_df(df_raw[[col_simple]].rename(columns={col_simple: \"text\"}), \"text\", lang=lang)\n",
    "    df_simple = df_simple.add_suffix(\"_S\")\n",
    "\n",
    "    df_complex = compute_all_complexity_measures_df(df_raw[[col_complex]].rename(columns={col_complex: \"text\"}), \"text\", lang=lang)\n",
    "    df_complex = df_complex.add_suffix(\"_C\")\n",
    "\n",
    "    # On garde l'ID et on fusionne proprement (alignement par index)\n",
    "    df_meas = pd.concat([df_raw[[id_col]], df_simple, df_complex], axis=1)\n",
    "\n",
    "    # Optionnel : remonter l'id en première colonne si besoin\n",
    "    cols = [id_col] + [c for c in df_meas.columns if c != id_col]\n",
    "    df_meas = df_meas[cols]\n",
    "    return df_meas\n",
    "\n",
    "def simple_dominates_complex_flags(\n",
    "    df_meas: pd.DataFrame,\n",
    "    metrics: List[str] = [\"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\", \"LC\", \"CoH\"]\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Renvoie une série booléenne indiquant, pour chaque ligne, si le *Simple* domine le *Complex*\n",
    "    selon la règle forte: Simple_metric >= Complex_metric pour TOUTES les 'metrics'.\n",
    "    Attend que df_meas contienne des colonnes <metric>_S et <metric>_C.\n",
    "    \"\"\"\n",
    "    # Construit la condition conjonctive\n",
    "    cond = pd.Series(True, index=df_meas.index)\n",
    "    for m in metrics:\n",
    "        s_col = f\"{m}_S\"\n",
    "        c_col = f\"{m}_C\"\n",
    "        if s_col not in df_meas.columns or c_col not in df_meas.columns:\n",
    "            raise KeyError(f\"Colonnes de métrique manquantes: {s_col!r} / {c_col!r}\")\n",
    "        cond = cond & (df_meas[s_col] >= df_meas[c_col])\n",
    "    return cond\n",
    "\n",
    "def prepare_dataset(\n",
    "    df_raw: pd.DataFrame,\n",
    "    col_simple: str = \"Simple\",\n",
    "    col_complex: str = \"Complex\",\n",
    "    lang: str = \"en\",\n",
    "    id_col: str = \"orig_id\",\n",
    "    out_prefix: str = \"eval\",\n",
    "    out_dir: str = \"/mnt/data\"\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Étapes:\n",
    "      1) Assure un identifiant stable 'orig_id'.\n",
    "      2) Calcule les mesures de complexité pour les deux colonnes (suffixes _S et _C).\n",
    "      3) Identifie les lignes où Simple >= Complex sur toutes les mesures => dominance True.\n",
    "      4) Sépare en 'kept' (à conserver) et 'removed' (à retirer).\n",
    "      5) Sauvegarde:\n",
    "         - {out_prefix}_measures_full.csv     : toutes les mesures + flag 'simple_dominates'\n",
    "         - {out_prefix}_kept.csv              : lignes à conserver (avec id)\n",
    "         - {out_prefix}_removed.csv           : lignes retirées (avec id)\n",
    "         - {out_prefix}_removed_ids.txt       : liste des ids retirés (une valeur par ligne)\n",
    "    Retourne (df_aug, df_kept, df_removed).\n",
    "    \"\"\"\n",
    "    out_path = Path(out_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df_raw = ensure_id(df_raw, id_col=id_col)\n",
    "\n",
    "    # 2) Mesures\n",
    "    df_meas = compute_pair_measures_df(df_raw, col_simple=col_simple, col_complex=col_complex, lang=lang, id_col=id_col)\n",
    "\n",
    "    # 3) Dominance\n",
    "    df_meas[\"simple_dominates\"] = simple_dominates_complex_flags(df_meas)\n",
    "\n",
    "    # 4) Séparation\n",
    "    df_removed = df_meas[df_meas[\"simple_dominates\"]].copy()\n",
    "    df_kept    = df_meas[~df_meas[\"simple_dominates\"]].copy()\n",
    "\n",
    "    # 5) Sauvegardes robustes\n",
    "    full_csv     = out_path / f\"{out_prefix}_measures_full.csv\"\n",
    "    kept_csv     = out_path / f\"{out_prefix}_kept.csv\"\n",
    "    removed_csv  = out_path / f\"{out_prefix}_removed.csv\"\n",
    "    removed_ids  = out_path / f\"{out_prefix}_removed_ids.txt\"\n",
    "\n",
    "    df_meas.to_csv(full_csv, index=False)\n",
    "    df_kept.to_csv(kept_csv, index=False)\n",
    "    df_removed.to_csv(removed_csv, index=False)\n",
    "    removed_ids.write_text(\"\\n\".join(map(str, df_removed[id_col].tolist())), encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"[OK] Sauvegardes:\\n- {full_csv}\\n- {kept_csv}\\n- {removed_csv}\\n- {removed_ids}\")\n",
    "    return df_meas, df_kept, df_removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "832ada71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing lexical & syntactic complexity (sequential): 100%|██████████| 189/189 [46:18<00:00, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[erreur] spacy_nlp must be provided to compute LC and CoH. Load a spaCy model with vectors, e.g. 'en_core_web_md', and pass it as spacy_nlp=...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rroll\\AppData\\Local\\Temp\\ipykernel_12396\\2114768531.py\", line 22, in <module>\n",
      "    df_aug, df_kept, df_removed = prepare_dataset(\n",
      "  File \"C:\\Users\\rroll\\AppData\\Local\\Temp\\ipykernel_12396\\484542913.py\", line 102, in prepare_dataset\n",
      "    df_meas = compute_pair_measures_df(df_raw, col_simple=col_simple, col_complex=col_complex, lang=lang, id_col=id_col)\n",
      "  File \"C:\\Users\\rroll\\AppData\\Local\\Temp\\ipykernel_12396\\484542913.py\", line 41, in compute_pair_measures_df\n",
      "    df_simple = compute_all_complexity_measures_df(df_raw[[col_simple]].rename(columns={col_simple: \"text\"}), \"text\", lang=lang)\n",
      "  File \"C:\\Users\\rroll\\AppData\\Local\\Temp\\ipykernel_12396\\2664708556.py\", line 94, in compute_all_complexity_measures_df\n",
      "    raise ValueError(\n",
      "ValueError: spacy_nlp must be provided to compute LC and CoH. Load a spaCy model with vectors, e.g. 'en_core_web_md', and pass it as spacy_nlp=...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Exemple d'utilisation (facultatif) =======================================\n",
    "# Vous pouvez ignorer cette cellule si les ressources NLP (Stanza/spaCy) ne sont\n",
    "# pas disponibles sur votre machine au moment de l'exécution.\n",
    "#\n",
    "# Hypothèses:\n",
    "# - Un DataFrame \"df\" est déjà en mémoire avec deux colonnes de texte \"Simple\" et \"Complex\".\n",
    "# - Sinon, si 'datasets' et 'load_dataset' existent (définis plus haut dans le notebook),\n",
    "#   on tentera de charger un petit dataset d'exemple.\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    has_df = 'df' in globals() and isinstance(df, pd.DataFrame)\n",
    "    if not has_df and 'datasets' in globals() and 'load_dataset' in globals():\n",
    "        # Choisit le premier dataset disponible\n",
    "        first_name = next(iter(datasets.keys()))\n",
    "        print(f\"[info] Chargement dataset: {first_name}\")\n",
    "        df = load_dataset(first_name)\n",
    "        has_df = True\n",
    "\n",
    "    if has_df:\n",
    "        # Langue par défaut 'en' (adaptez selon vos jeux de données)\n",
    "        df_aug, df_kept, df_removed = prepare_dataset(\n",
    "            df_raw=df,\n",
    "            col_simple=\"Simple\",\n",
    "            col_complex=\"Complex\",\n",
    "            lang=\"en\",\n",
    "            id_col=\"orig_id\",\n",
    "            out_prefix=\"eval\"\n",
    "        )\n",
    "        print(f\"Tailles -> Complet: {len(df_aug)} | Conservé: {len(df_kept)} | Retiré: {len(df_removed)}\")\n",
    "    else:\n",
    "        print(\"[avertissement] Aucun DataFrame 'df' trouvé et pas de loader disponible. \"\n",
    "              \"Définissez df (avec colonnes 'Simple' et 'Complex') puis relancez cette cellule.\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback, sys\n",
    "    traceback.print_exc()\n",
    "    print(f\"[erreur] {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676edf0a",
   "metadata": {},
   "source": [
    "\n",
    "## Initialisation automatique de spaCy (vecteurs requis pour LC/CoH)\n",
    "\n",
    "Les mesures **LC** et **CoH** nécessitent un modèle spaCy **avec vecteurs**.  \n",
    "Le bloc ci-dessous tente de charger automatiquement un modèle adapté à la langue (`en`, `fr`, `de`, `it`, `es`), en privilégiant la variante **`_md`** puis **`_lg`** si disponible. Si le modèle n'est pas installé, il sera téléchargé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def _try_spacy_load(model_name: str):\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load(model_name)\n",
    "        return nlp\n",
    "    except OSError:\n",
    "        return None\n",
    "\n",
    "def _ensure_spacy_model(model_name: str):\n",
    "    \"\"\"\n",
    "    Télécharge un modèle spaCy s'il est absent, puis le charge.\n",
    "    \"\"\"\n",
    "    import spacy\n",
    "    nlp = _try_spacy_load(model_name)\n",
    "    if nlp is not None:\n",
    "        return nlp\n",
    "    # Téléchargement\n",
    "    print(f\"[info] Téléchargement du modèle spaCy '{model_name}'…\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", model_name], check=False)\n",
    "    # Re-tente le chargement\n",
    "    nlp = _try_spacy_load(model_name)\n",
    "    if nlp is None:\n",
    "        raise RuntimeError(f\"Échec de chargement du modèle spaCy '{model_name}'.\")\n",
    "    return nlp\n",
    "\n",
    "def get_spacy_nlp(lang: str = \"en\"):\n",
    "    \"\"\"\n",
    "    Retourne/initialise une variable globale `spacy_nlp` avec un modèle spaCy qui contient des vecteurs.\n",
    "    Langues prises en charge: en, fr, de, it, es.\n",
    "    Préférence: *_md, sinon *_lg. En dernier recours, *_sm (sans vecteurs), mais cela dégradera LC/CoH.\n",
    "    \"\"\"\n",
    "    global spacy_nlp\n",
    "    try:\n",
    "        import spacy  # vérifie que spaCy est installé\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\"spaCy n'est pas installé. Installez-le avec `pip install spacy`.\") from e\n",
    "\n",
    "    # Si déjà défini et cohérent, on réutilise\n",
    "    if \"spacy_nlp\" in globals() and spacy_nlp is not None:\n",
    "        return spacy_nlp\n",
    "\n",
    "    lang = (lang or \"en\").lower().strip()\n",
    "    candidates = []\n",
    "    if lang == \"en\":\n",
    "        candidates = [\"en_core_web_md\", \"en_core_web_lg\", \"en_core_web_sm\"]\n",
    "    elif lang == \"fr\":\n",
    "        candidates = [\"fr_core_news_md\", \"fr_core_news_lg\", \"fr_core_news_sm\"]\n",
    "    elif lang == \"de\":\n",
    "        candidates = [\"de_core_news_md\", \"de_core_news_lg\", \"de_core_news_sm\"]\n",
    "    elif lang == \"it\":\n",
    "        candidates = [\"it_core_news_md\", \"it_core_news_lg\", \"it_core_news_sm\"]\n",
    "    elif lang == \"es\":\n",
    "        candidates = [\"es_core_news_md\", \"es_core_news_lg\", \"es_core_news_sm\"]\n",
    "    else:\n",
    "        # par défaut anglais\n",
    "        candidates = [\"en_core_web_md\", \"en_core_web_lg\", \"en_core_web_sm\"]\n",
    "\n",
    "    # Tente md puis lg, sinon sm\n",
    "    for name in candidates:\n",
    "        try:\n",
    "            spacy_nlp = _ensure_spacy_model(name)\n",
    "            print(f\"[OK] spaCy chargé: '{name}'\")\n",
    "            return spacy_nlp\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Impossible de charger '{name}': {e}\")\n",
    "\n",
    "    raise RuntimeError(\"Aucun modèle spaCy n'a pu être chargé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45964d09",
   "metadata": {},
   "source": [
    "\n",
    "## Patch : redéfinition de `prepare_dataset` pour garantir `spacy_nlp`\n",
    "\n",
    "Cette version appelle automatiquement `get_spacy_nlp(lang)` avant de calculer les mesures, afin d'éviter l'erreur *« spacy_nlp must be provided to compute LC and CoH »*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb47935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def prepare_dataset(\n",
    "    df_raw: pd.DataFrame,\n",
    "    col_simple: str = \"Simple\",\n",
    "    col_complex: str = \"Complex\",\n",
    "    lang: str = \"en\",\n",
    "    id_col: str = \"orig_id\",\n",
    "    out_prefix: str = \"eval\",\n",
    "    out_dir: str = \"/mnt/data\"\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Étapes:\n",
    "      0) Charge/valide un modèle spaCy avec vecteurs (via get_spacy_nlp(lang)).\n",
    "      1) Assure un identifiant stable 'orig_id'.\n",
    "      2) Calcule les mesures de complexité pour les deux colonnes (suffixes _S et _C).\n",
    "      3) Identifie les lignes où Simple >= Complex sur toutes les mesures => dominance True.\n",
    "      4) Sépare en 'kept' (à conserver) et 'removed' (à retirer).\n",
    "      5) Sauvegarde:\n",
    "         - {out_prefix}_measures_full.csv     : toutes les mesures + flag 'simple_dominates'\n",
    "         - {out_prefix}_kept.csv              : lignes à conserver (avec id)\n",
    "         - {out_prefix}_removed.csv           : lignes retirées (avec id)\n",
    "         - {out_prefix}_removed_ids.txt       : liste des ids retirés (une valeur par ligne)\n",
    "    Retourne (df_aug, df_kept, df_removed).\n",
    "    \"\"\"\n",
    "    # 0) s'assurer que spacy_nlp est dispo (vecteurs requis pour LC/CoH)\n",
    "    try:\n",
    "        _ = get_spacy_nlp(lang=lang)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"Impossible d'initialiser spaCy pour la langue '{lang}'. \"\n",
    "            f\"Installez un modèle avec vecteurs (ex: 'en_core_web_md', 'fr_core_news_md').\\nDétail: {e}\"\n",
    "        ) from e\n",
    "\n",
    "    out_path = Path(out_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df_raw = ensure_id(df_raw, id_col=id_col)\n",
    "\n",
    "    # 2) Mesures\n",
    "    df_meas = compute_pair_measures_df(\n",
    "        df_raw,\n",
    "        col_simple=col_simple,\n",
    "        col_complex=col_complex,\n",
    "        lang=lang,\n",
    "        id_col=id_col\n",
    "    )\n",
    "\n",
    "    # 3) Dominance\n",
    "    df_meas[\"simple_dominates\"] = simple_dominates_complex_flags(df_meas)\n",
    "\n",
    "    # 4) Séparation\n",
    "    df_removed = df_meas[df_meas[\"simple_dominates\"]].copy()\n",
    "    df_kept    = df_meas[~df_meas[\"simple_dominates\"]].copy()\n",
    "\n",
    "    # 5) Sauvegardes robustes\n",
    "    full_csv     = out_path / f\"{out_prefix}_measures_full.csv\"\n",
    "    kept_csv     = out_path / f\"{out_prefix}_kept.csv\"\n",
    "    removed_csv  = out_path / f\"{out_prefix}_removed.csv\"\n",
    "    removed_ids  = out_path / f\"{out_prefix}_removed_ids.txt\"\n",
    "\n",
    "    df_meas.to_csv(full_csv, index=False)\n",
    "    df_kept.to_csv(kept_csv, index=False)\n",
    "    df_removed.to_csv(removed_csv, index=False)\n",
    "    removed_ids.write_text(\"\\n\".join(map(str, df_removed[id_col].tolist())), encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"[OK] Sauvegardes:\\n- {full_csv}\\n- {kept_csv}\\n- {removed_csv}\\n- {removed_ids}\")\n",
    "    return df_meas, df_kept, df_removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35830197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Test rapide (optionnel) ==================================================\n",
    "# if 'df' in globals():\n",
    "#     df_aug, df_kept, df_removed = prepare_dataset(\n",
    "#         df_raw=df,\n",
    "#         col_simple=\"Simple\",\n",
    "#         col_complex=\"Complex\",\n",
    "#         lang=\"en\",           # ou \"fr\", etc.\n",
    "#         id_col=\"orig_id\",\n",
    "#         out_prefix=\"eval2\"\n",
    "#     )\n",
    "#     print(f\"Complet: {len(df_aug)} | Conservé: {len(df_kept)} | Retiré: {len(df_removed)}\")\n",
    "# else:\n",
    "#     print(\"Aucun DataFrame 'df' en mémoire pour ce test rapide.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
