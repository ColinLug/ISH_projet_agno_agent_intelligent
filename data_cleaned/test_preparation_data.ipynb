{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fe89001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 7.66MB/s]                    \n",
      "2025-12-18 21:51:23 INFO: Downloaded file to C:\\Users\\rroll\\stanza_resources\\resources.json\n",
      "2025-12-18 21:51:23 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-12-18 21:51:24 INFO: File exists: C:\\Users\\rroll\\stanza_resources\\en\\default.zip\n",
      "2025-12-18 21:51:27 INFO: Finished downloading models and saved to C:\\Users\\rroll\\stanza_resources\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rroll\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rroll\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x23fb7248550>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "from pprint import pprint\n",
    "from typing import Dict, Set, Iterable, Optional, Any, Tuple\n",
    "import importlib.resources as pkg_resources\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "import stanza\n",
    "import textcomplexity  # only used to access en.json\n",
    "from tqdm.auto import tqdm  \n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Download required resources\n",
    "stanza.download('en')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Make sure WordNet is available; if not, download it.\n",
    "try:\n",
    "    _ = wn.synsets(\"dog\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "spacy_nlp = nlp\n",
    "spacy_nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed542395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire des datasets (identique à test.ipynb)\n",
    "datasets = {\n",
    "    \"ose_adv_ele\": \"data_sampled/OSE_adv_ele.csv\",\n",
    "    \"ose_adv_int\": \"data_sampled/OSE_adv_int.csv\",\n",
    "    \"swipe\": \"data_sampled/swipe.csv\",\n",
    "    \"vikidia\": \"data_sampled/vikidia.csv\",\n",
    "}\n",
    "\n",
    "# Colonnes attendues\n",
    "COL_SIMPLE = \"Simple\"\n",
    "COL_COMPLEX = \"Complex\"\n",
    "ID_COL = \"orig_id\"\n",
    "\n",
    "# Sorties\n",
    "OUT_DIR = Path(\"data_prepared\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Langue / modèles\n",
    "LANG = \"en\"\n",
    "SPACY_MODEL = \"en_core_web_md\"\n",
    "\n",
    "# Dominance : Simple > Complex sur au moins X métriques\n",
    "MIN_METRICS = 5\n",
    "\n",
    "# Liste officielle des métriques (test.ipynb)\n",
    "MEASURES = [\"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\", \"LC\", \"CoH\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e044f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement spaCy (doit contenir des vecteurs pour CoH)\n",
    "nlp = spacy.load(SPACY_MODEL)\n",
    "spacy_nlp = nlp\n",
    "\n",
    "# Important : segmentation en phrases\n",
    "if \"sentencizer\" not in spacy_nlp.pipe_names:\n",
    "    spacy_nlp.add_pipe(\"sentencizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff4f785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets ={'ose_adv_ele':'data_sampled/OSE_adv_ele.csv', \n",
    "           'ose_adv_int':'data_sampled/OSE_adv_int.csv',\n",
    "           'swipe': 'data_sampled/swipe.csv',\n",
    "           'vikidia':'data_sampled/vikidia.csv'}\n",
    "\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path, sep='\\t')\n",
    "    \n",
    "\n",
    "def load_dataset(name):\n",
    "    if name not in datasets:\n",
    "        raise ValueError(f\"Dataset {name} not found\")\n",
    "    return load_data(datasets[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e76a962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Simple</th>\n",
       "      <th>Complex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿To tourists, Amsterdam still seems very liber...</td>\n",
       "      <td>﻿Amsterdam still looks liberal to tourists, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>﻿Anitta, a music star from Brazil, has million...</td>\n",
       "      <td>﻿Brazil’s latest funk sensation, Anitta, has w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Simple  \\\n",
       "0  ﻿When you see the word Amazon, what’s the firs...   \n",
       "1  ﻿To tourists, Amsterdam still seems very liber...   \n",
       "2  ﻿Anitta, a music star from Brazil, has million...   \n",
       "\n",
       "                                             Complex  \n",
       "0  ﻿When you see the word Amazon, what’s the firs...  \n",
       "1  ﻿Amsterdam still looks liberal to tourists, wh...  \n",
       "2  ﻿Brazil’s latest funk sensation, Anitta, has w...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('ose_adv_ele')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "127c2dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE TEXT\n",
      "﻿Women have traditionally had a minor role in professional football but this may be changing. France has just employed its first female professional team manager. It did not matter that it was a second-division club. It did not matter if it was, as some people suggested, just a publicity stunt for a minor team, Clermont Foot 63. Clermont is 14th out of the 20 teams in its league at the moment.\n",
      "What mattered was that they gave Helena Costa the top job. This has made football history because she is the first female manager in the top two divisions of any professional European league. “As a woman, it’s made me happy,” said Véronique Soulier, president of the club’s supporters’ association. “When I first heard the news, I was surprised, but, then, we all agreed that it’s good news. We all agree that a woman at the head of a group of men is no bad thing.”\n",
      "The new manager of Clermont Foot 63 was born in Alhandra, Portugal and has a master’s degree in sports science. She is also a UEFA-licensed coach. She previously coached Benfica’s male youth teams, the Qatar women’s team and, more recently, the Iranian women’s national team.\n",
      "The president of Clermont Foot 63, Claude Michy, gave Costa, 36, a two-year contract. Michy is very good at keeping his club in the news. In 2013, he told everyone the team had signed Messi. They had. Not the Argentinian and Barcelona striker Lionel Messi, but Junior Messi Enguene, a 20-year-old midfielder from Cameroon.\n",
      "Carolina Morace, an Italian who was the only previous woman coach of a men’s professional team, said: “I don’t know Helena, but, if she has been hired by a team, then it means that she knows how to do her job. I hope that, one day, this can become normal.” Morace played for Italy in 153 internationals. In 1999, she became coach of the men’s team Viterbese. But, after only two games, she resigned from the job because of a disagreement with the club’s owner. She added: “I see too many men, even in the women’s game, who do not have the same expertise as women but are working. And the women are not working.”\n",
      "Raymond Domenech, former manager of the French national team, said: “Women know how to play football and how to manage and are good at doing it. Why shouldn’t they manage men’s teams? The opposite happens and doesn’t cause any problems. It’s a natural choice and reflects our society in which women are equal to men. I say well done to President Michy. I told myself that, if I took control of a club again, I’d hire a woman as my number two. Michy did it first.”\n",
      "Clermont Foot 63 says that Costa’s becoming the team’s manager will allow the club to enter “a new era”. On the club supporters’ website, reaction to Costa’s becoming the manager was mixed. “In my opinion, it’s just a publicity stunt to get people talking about the club. I find it hard to believe she’ll be able to get the players’ respect, above all when she’s the same age as the oldest,” wrote one fan. “Her CV isn’t bad, but now the question is: will she be good enough?” added another. A third wrote: “I wish her welcome and success but I think it’ll be hard for her to do well as a woman in such a macho business.”\n",
      "But Soulier was hopeful: “Hopefully, with the new manager, the club can find the motivation they don’t have at the moment,” she said. “The boys in the team can be difficult to manage. With a woman in control, maybe they will be less demanding.”\n",
      "If we believe Costa’s reputation, she will be the person making the demands. After doing work experience at Chelsea during José Mourinho’s first time as manager of the club between 2004 and 2007, people described her as “Mourinho in a skirt”. Costa’s comment on that description was: “Like Mourinho, I always want to win. In that way, yes, I’m happy to be compared with him.”\n",
      "----------------------------------------------------------------------------------------------------\n",
      "COMPLEX TEXT\n",
      "﻿The continual relegation of women to the sidelines of football was given a good kicking when France appointed its first female professional team manager. It did not matter that it was a second-division club. It did not matter if it was, as some bad sports – male, of course – suggested, just a cynical stunt to drum up publicity for a minor team, Clermont Foot 63, currently ranking a lowly 14th out of 20 in its league.\n",
      "What mattered was that Helena Costa had been given the top job, a move that saw her make football history by becoming the first female manager to be appointed in the highest two divisions of any professional European league. “As a woman, it’s made me happy,” Véronique Soulier, president of the club’s supporters’ association, told journalists. “When I first heard the news, I was rather surprised, but, once that passed, we were pretty unanimous that it’s good news. We all agree that a woman at the head of a group of men is no bad thing.”\n",
      "The new manager of Clermont Foot 63, whose average home crowd at the stadium at Clermont- Ferrand in the Auvergne region of south-central France is around 3,800, is a former talent spotter for the Scottish Premiership side Celtic. Costa, 36, was born in Alhandra on the River Tagus in southeast Portugal and graduated with a master’s degree in sports science. She is also a UEFA-licensed coach. She previously coached Benfica’s male youth teams, the Qatar women’s team, which she led to its first international victory in 2012, and, more recently, the Iranian women’s national side, which she left in September 2013.\n",
      "Costa was appointed on a two-year contract by the president of Clermont Foot 63, Claude Michy, who is a champion at grabbing the headlines for his club. In 2013, he announced the team had signed Messi. They had. Not the Argentinian and FC Barcelona record-breaking striker Lionel Messi, but Junior Messi Enguene, a 20-year-old midfielder from Cameroon.\n",
      "France’s women’s minister, Najat Vallaud- Belkacem, tweeted: “Bravo to Clermont Foot for understanding that giving women a place is the future of professional football.”\n",
      "Carolina Morace, an Italian who was the only previous woman coach of a men’s professional team, said: “I don’t know Helena, but, if she has been hired by a team, then it means that she knows how to do her job. I hope that, one day, this can become normal.” Morace was an outstanding player in the women’s game. She was the top scorer for 12 seasons in Italy’s Serie A and played for Italy in 153 internationals. In 1999, she was named as the coach of Viterbese in the men’s Serie C1. But, after only two games, she resigned from the job following a clash with the club’s mercurial proprietor, Luciano Gaucci, who at the time was also the owner of a Serie A side, Perugia.\n",
      "Morace was quoted as saying that she had refused Gaucci’s demand that she fire her deputy and the side’s trainer. “He let me know that I could carry on working with whomever I wanted. But, by then, mutual trust was lacking and I didn’t fancy carrying on in that climate of uncertainty,” she said. Morace works today as a lawyer in Rome and as expert soccer commentator on television and in the pages of the daily Gazzetta dello Sport.\n",
      "She said: “For the time being, I see too many men, even in the women’s game, who are working, despite not having the same expertise as women, who, by contrast, are not working.” Raymond Domenech, former manager of the French national team, said: “Women know how to play football and how to manage and are good at doing it. Why shouldn’t they manage men’s teams? The opposite happens and doesn’t cause any problems. It’s a natural choice and reflects our society in which women are equal to men. I say well done to President Michy. I told myself that, if I took charge of a club again, I’d hire a woman as my number two. He beat me to it.”\n",
      "A statement on Clermont Foot 63’s website said Costa’s appointment would allow the club to enter “a new era”. On the club supporters’ website, reaction to Costa’s appointment was a mix of surprise and a certain cynicism. “In my opinion, it’s just a publicity stunt to get people talking about the club and she won’t last the season. I find it hard to believe she’ll be able to get the players’ respect, above all when she’s the same age as the oldest,” wrote one fan. “Her CV isn’t bad, but now the question is: will she be good enough?” added another. A third wrote: “I wish her welcome and success but I think it’ll be hard for her to make her mark as a woman in such a macho business. Has our president pulled off a media coup?”\n",
      "But Soulier was hopeful: “Hopefully, with the new manager, the club can find the motivation they’re lacking at the moment,” she said. “The boys in the team can be difficult to manage. With a woman in charge, maybe they’ll be less demanding.”\n",
      "If Costa’s reputation is anything to go by, she will be the one making the demands. After doing work experience at Chelsea during her compatriot José Mourinho’s first stint as manager of the club between 2004 and 2007, she was reportedly described as “Mourinho in a skirt”. Costa quickly kicked the sexist remark into touch. “Like Mourinho, I always want to win. As far as that’s concerned, yes, I’m happy to be compared with him,” she said.\n"
     ]
    }
   ],
   "source": [
    "row = df.sample(1)\n",
    "\n",
    "print('SIMPLE TEXT')\n",
    "print(row['Simple'].iloc[0])\n",
    "print('-'*100)\n",
    "print('COMPLEX TEXT')\n",
    "print(row['Complex'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e0dfffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ose_adv_ele: 189 rows\n",
      "ose_adv_int: 189 rows\n",
      "swipe: 1233 rows\n",
      "vikidia: 1233 rows\n",
      "Total: 2844 rows\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for name, path in datasets.items():\n",
    "    df = load_dataset(name)\n",
    "    print(f\"{name}: {df.shape[0]} rows\")\n",
    "    cnt += df.shape[0]\n",
    "print(f\"Total: {cnt} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afca2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset('ose_adv_ele')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa4b934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache stanza pipelines to avoid re-loading models\n",
    "_STANZA_PIPELINES: Dict[str, stanza.Pipeline] = {}\n",
    "\n",
    "# UPOS tags considered content words (C)\n",
    "CONTENT_UPOS = {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def load_cow_top5000_en() -> Set[str]:\n",
    "    \"\"\"\n",
    "    Load the COW-based list of the 5,000 most frequent English content words\n",
    "    from textcomplexity's English language definition file (en.json).\n",
    "\n",
    "    We ignore POS tags and keep only lowercased word forms.\n",
    "    \"\"\"\n",
    "    with pkg_resources.files(textcomplexity).joinpath(\"en.json\").open(\n",
    "        \"r\", encoding=\"utf-8\"\n",
    "    ) as f:\n",
    "        lang_def = json.load(f)\n",
    "\n",
    "    most_common = lang_def[\"most_common\"]  # list of [word, xpos]\n",
    "    cow_top5000 = {w.lower() for w, xpos in most_common}\n",
    "    return cow_top5000\n",
    "\n",
    "\n",
    "def get_stanza_pipeline(lang: str = \"en\", use_gpu: bool = False) -> stanza.Pipeline:\n",
    "    \"\"\"\n",
    "    Get (or create) a cached stanza Pipeline for a given language.\n",
    "\n",
    "    NOTE: You must have downloaded the models beforehand, e.g.:\n",
    "        import stanza\n",
    "        stanza.download('en')\n",
    "    \"\"\"\n",
    "    if lang not in _STANZA_PIPELINES:\n",
    "        _STANZA_PIPELINES[lang] = stanza.Pipeline(\n",
    "            lang=lang,\n",
    "            processors=\"tokenize,pos,lemma,depparse,constituency\",\n",
    "            use_gpu=use_gpu,\n",
    "            tokenize_no_ssplit=False,\n",
    "        )\n",
    "    return _STANZA_PIPELINES[lang]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8ee6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_mtld(tokens: Iterable[str], ttr_threshold: float = 0.72) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute MTLD (Measure of Textual Lexical Diversity) for a list of tokens.\n",
    "\n",
    "    MTLD = total_number_of_tokens / number_of_factors\n",
    "\n",
    "    A factor is a contiguous segment where the running TTR stays >= threshold.\n",
    "    When the TTR drops below the threshold, we close a factor (at the previous\n",
    "    token) and start a new one. At the end, the remaining partial segment is\n",
    "    counted as a fractional factor, with weight proportional to how close the\n",
    "    final TTR is to the threshold.\n",
    "    \"\"\"\n",
    "    tokens = [tok for tok in tokens if tok]\n",
    "    if not tokens:\n",
    "        return None\n",
    "\n",
    "    types = set()\n",
    "    factor_count = 0.0\n",
    "    token_count_in_factor = 0\n",
    "\n",
    "    for tok in tokens:\n",
    "        token_count_in_factor += 1\n",
    "        types.add(tok)\n",
    "        ttr = len(types) / token_count_in_factor\n",
    "\n",
    "        if ttr < ttr_threshold:\n",
    "            factor_count += 1.0\n",
    "            types = set()\n",
    "            token_count_in_factor = 0\n",
    "\n",
    "    # final partial factor\n",
    "    if token_count_in_factor > 0:\n",
    "        final_ttr = len(types) / token_count_in_factor\n",
    "        if final_ttr < 1.0:\n",
    "            fractional = (1.0 - final_ttr) / (1.0 - ttr_threshold)\n",
    "            fractional = max(0.0, min(1.0, fractional))\n",
    "            factor_count += fractional\n",
    "\n",
    "    if factor_count == 0:\n",
    "        return None\n",
    "\n",
    "    return len(tokens) / factor_count\n",
    "\n",
    "\n",
    "\n",
    "def _compute_lexical_density(total_tokens: int, content_tokens: int) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LD = |C| / |T|\n",
    "    where:\n",
    "        |C| = number of content-word tokens\n",
    "        |T| = total number of non-punctuation tokens\n",
    "    \"\"\"\n",
    "    if total_tokens == 0:\n",
    "        return None\n",
    "    return content_tokens / total_tokens\n",
    "\n",
    "\n",
    "def _compute_lexical_sophistication_cow(\n",
    "    content_forms: Iterable[str],\n",
    "    cow_top5000: set,\n",
    ") -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LS = |{ w in C : w not in R }| / |C|\n",
    "    where:\n",
    "        C = content-word tokens (surface forms, lowercased)\n",
    "        R = COW top-5000 content word forms (lowercased)\n",
    "    \"\"\"\n",
    "    forms = [f for f in content_forms if f]\n",
    "    if not forms:\n",
    "        return None\n",
    "\n",
    "    off_list = sum(1 for f in forms if f not in cow_top5000)\n",
    "    return off_list / len(forms)\n",
    "\n",
    "\n",
    "\n",
    "def lexical_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MTLD, LD, LS from a stanza Document.\n",
    "    \"\"\"\n",
    "    cow_top5000 = load_cow_top5000_en()\n",
    "\n",
    "    mtld_tokens = []\n",
    "    total_tokens = 0\n",
    "    content_tokens = 0\n",
    "    content_forms = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.upos == \"PUNCT\":\n",
    "                continue\n",
    "\n",
    "            lemma = (word.lemma or word.text or \"\").lower()\n",
    "            if not lemma:\n",
    "                continue\n",
    "\n",
    "            mtld_tokens.append(lemma)\n",
    "            total_tokens += 1\n",
    "\n",
    "            if word.upos in CONTENT_UPOS:\n",
    "                content_tokens += 1\n",
    "                form = (word.text or \"\").lower()\n",
    "                content_forms.append(form)\n",
    "\n",
    "    mtld = _compute_mtld(mtld_tokens) if mtld_tokens else None\n",
    "    ld = _compute_lexical_density(total_tokens, content_tokens)\n",
    "    ls = _compute_lexical_sophistication_cow(content_forms, cow_top5000)\n",
    "\n",
    "    return {\"MTLD\": mtld, \"LD\": ld, \"LS\": ls}\n",
    "\n",
    "\n",
    "def lexical_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute lexical measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return lexical_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute lexical measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = lexical_measures_from_text(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "\n",
    "    return {\"MTLD\": mtld_res, \"LD\": ld_res, \"LS\": ls_res}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43a1952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdd_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute Mean Dependency Distance (MDD) from a stanza Document.\n",
    "\n",
    "    For each sentence s_i with dependency set D_i:\n",
    "        MDD_i = (1 / |D_i|) * sum_{(h,d) in D_i} |h - d|\n",
    "    Then:\n",
    "        MDD = (1 / k) * sum_i MDD_i, over all sentences with at least one dependency.\n",
    "    \"\"\"\n",
    "    sentence_mdds = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        distances = []\n",
    "        for w in sent.words:\n",
    "            if w.head is None or w.head == 0:\n",
    "                continue\n",
    "            distances.append(abs(w.id - w.head))\n",
    "\n",
    "        if distances:\n",
    "            sentence_mdds.append(sum(distances) / len(distances))\n",
    "\n",
    "    if not sentence_mdds:\n",
    "        return None\n",
    "    return sum(sentence_mdds) / len(sentence_mdds)\n",
    "\n",
    "\n",
    "\n",
    "def _count_clauses_in_tree(tree) -> int:\n",
    "    \"\"\"\n",
    "    Count clause nodes in a constituency tree.\n",
    "\n",
    "    A simple and standard heuristic (PTB-style) is:\n",
    "        count all nodes whose label starts with 'S'\n",
    "        (S, SBAR, SBARQ, SINV, SQ, etc.).\n",
    "\n",
    "    This aligns with the idea of counting finite and subordinate clauses\n",
    "    as in Hunt (1965) and later complexity work.\n",
    "    \"\"\"\n",
    "    if tree is None:\n",
    "        return 0\n",
    "\n",
    "    # Stanza's constituency tree: tree.label, tree.children\n",
    "    count = 1 if getattr(tree, \"label\", \"\").startswith(\"S\") else 0\n",
    "\n",
    "    for child in getattr(tree, \"children\", []):\n",
    "        # leaves can be strings or terminals without 'label'\n",
    "        if hasattr(child, \"label\"):\n",
    "            count += _count_clauses_in_tree(child)\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def cs_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute CS (clauses per sentence) from a stanza Document.\n",
    "\n",
    "        CS = (1 / k) * sum_i L_i\n",
    "\n",
    "    where L_i is the number of clauses in sentence s_i, estimated by counting\n",
    "    all constituents whose label starts with 'S' in the constituency tree of s_i.\n",
    "    \"\"\"\n",
    "    clause_counts = []\n",
    "    for sent in doc.sentences:\n",
    "        tree = getattr(sent, \"constituency\", None)\n",
    "        if tree is None:\n",
    "            # No constituency tree available for this sentence\n",
    "            continue\n",
    "        num_clauses = _count_clauses_in_tree(tree)\n",
    "        clause_counts.append(num_clauses)\n",
    "\n",
    "    if not clause_counts:\n",
    "        return None\n",
    "\n",
    "    return sum(clause_counts) / len(clause_counts)\n",
    "\n",
    "\n",
    "\n",
    "def syntactic_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MDD and CS from a stanza Document.\n",
    "    \"\"\"\n",
    "    mdd = mdd_from_doc(doc)\n",
    "    cs = cs_from_doc(doc)\n",
    "    return {\"MDD\": mdd, \"CS\": cs}\n",
    "\n",
    "\n",
    "def syntactic_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute syntactic measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return syntactic_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "def compute_syntactic_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute syntactic measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MDD\": {index: value},\n",
    "            \"CS\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = syntactic_measures_from_text(text, lang=lang)\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "    return {\"MDD\": mdd_res, \"CS\": cs_res}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e2f2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate set of content POS tags (spaCy universal POS)\n",
    "CONTENT_POS =  {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "def is_content_token(tok):\n",
    "    \"\"\"\n",
    "    Return True if token is considered a content word.\n",
    "    We ignore stopwords, punctuation, and non-alphabetic tokens.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        tok.is_alpha\n",
    "        and not tok.is_stop\n",
    "        and tok.pos_ in CONTENT_POS\n",
    "    )\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=100000)\n",
    "def get_related_lemmas(lemma):\n",
    "    \"\"\"\n",
    "    Return a set of semantically related lemmas for the given lemma\n",
    "    using WordNet, including:\n",
    "      - synonyms\n",
    "      - antonyms\n",
    "      - hypernyms / hyponyms\n",
    "      - meronyms (part/member/substance)\n",
    "      - coordinate terms (siblings under the same hypernym)\n",
    "\n",
    "    NOTE: Some older examples mention 'troponyms', but in NLTK's\n",
    "    WordNet interface there is no 'troponyms()' method on Synset,\n",
    "    so we do NOT use it here.\n",
    "    \"\"\"\n",
    "    lemma = lemma.lower()\n",
    "    related = set()\n",
    "    synsets = wn.synsets(lemma)\n",
    "\n",
    "    for syn in synsets:\n",
    "        # Synonyms and antonyms\n",
    "        for l in syn.lemmas():\n",
    "            related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "            for ant in l.antonyms():\n",
    "                related.add(ant.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Hypernyms (more general) and hyponyms (more specific)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for l in hyper.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "        for hypo in syn.hyponyms():\n",
    "            for l in hypo.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Meronyms: part/member/substance\n",
    "        for mer in syn.part_meronyms() + syn.member_meronyms() + syn.substance_meronyms():\n",
    "            for l in mer.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Coordinate terms (siblings under same hypernym)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for sibling in hyper.hyponyms():\n",
    "                if sibling == syn:\n",
    "                    continue\n",
    "                for l in sibling.lemmas():\n",
    "                    related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "    # Remove the lemma itself if present\n",
    "    related.discard(lemma)\n",
    "    return related\n",
    "\n",
    "\n",
    "def lexical_cohesion_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Lexical Cohesion (LC) for a single document:\n",
    "\n",
    "        LC = |C| / m\n",
    "\n",
    "    where:\n",
    "      - |C| is the number of cohesive devices between sentences\n",
    "        (lexical repetition + semantic relations),\n",
    "      - m  is the total number of word tokens (alphabetic) in the document.\n",
    "\n",
    "    If the document has fewer than 2 sentences or no valid words,\n",
    "    LC is returned as 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Total number of alphabetic tokens (denominator m)\n",
    "    m = sum(1 for tok in doc if tok.is_alpha)\n",
    "    if m == 0:\n",
    "        return 0.0\n",
    "\n",
    "    sentences = list(doc.sents)\n",
    "    if len(sentences) < 2:\n",
    "        # With only one sentence, cross-sentence cohesion is not defined\n",
    "        return 0.0\n",
    "\n",
    "    # Collect sets of content lemmas per sentence\n",
    "    sent_lemmas = []\n",
    "    for sent in sentences:\n",
    "        lemmas = set(\n",
    "            tok.lemma_.lower()\n",
    "            for tok in sent\n",
    "            if is_content_token(tok)\n",
    "        )\n",
    "        if lemmas:\n",
    "            sent_lemmas.append(lemmas)\n",
    "\n",
    "    if len(sent_lemmas) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    cohesive_count = 0\n",
    "\n",
    "    for i in range(len(sent_lemmas) - 1):\n",
    "        for j in range(i + 1, len(sent_lemmas)):\n",
    "            li = sent_lemmas[i]\n",
    "            lj = sent_lemmas[j]\n",
    "\n",
    "            # 1) Lexical repetition: shared lemmas\n",
    "            shared = li & lj\n",
    "            cohesive_count += len(shared)\n",
    "\n",
    "            # 2) Semantic relations via WordNet\n",
    "            for lemma in li:\n",
    "                related = get_related_lemmas(lemma)\n",
    "                cohesive_count += len(related & lj)\n",
    "\n",
    "    return float(cohesive_count) / float(m)\n",
    "\n",
    "\n",
    "def sentence_vector(sent, vector_size):\n",
    "    \"\"\"\n",
    "    Represent a sentence as the average of token vectors.\n",
    "    If no token has a vector, return a zero vector.\n",
    "    \"\"\"\n",
    "    vecs = [\n",
    "        tok.vector\n",
    "        for tok in sent\n",
    "        if tok.has_vector and not tok.is_punct and not tok.is_space\n",
    "    ]\n",
    "    if not vecs:\n",
    "        return np.zeros(vector_size, dtype=\"float32\")\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "def coherence_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Coherence (CoH) for a single document as the average\n",
    "    cosine similarity between adjacent sentence vectors:\n",
    "\n",
    "        CoH = (1 / (k-1)) * sum_{i=1}^{k-1} cos(h_i, h_{i+1})\n",
    "\n",
    "    where h_i is the sentence/topic vector for sentence i.\n",
    "\n",
    "    If the document has fewer than 2 sentences, CoH = 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    if nlp.vocab.vectors_length == 0:\n",
    "        raise ValueError(\n",
    "            \"The loaded spaCy model does not contain word vectors \"\n",
    "            \"(nlp.vocab.vectors_length == 0). \"\n",
    "            \"Use a model like 'en_core_web_md' or similar.\"\n",
    "        )\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    k = len(sentences)\n",
    "\n",
    "    if k < 2:\n",
    "        # Only one sentence: no adjacent pair, coherence = 0.0\n",
    "        return 0.0\n",
    "\n",
    "    vector_size = nlp.vocab.vectors_length\n",
    "    sent_vectors = [\n",
    "        sentence_vector(sent, vector_size)\n",
    "        for sent in sentences\n",
    "    ]\n",
    "\n",
    "    sims = []\n",
    "    for i in range(k - 1):\n",
    "        v1 = sent_vectors[i]\n",
    "        v2 = sent_vectors[i + 1]\n",
    "        norm1 = np.linalg.norm(v1)\n",
    "        norm2 = np.linalg.norm(v2)\n",
    "        denom = norm1 * norm2\n",
    "        if denom == 0.0:\n",
    "            # Skip pairs where at least one sentence vector is zero\n",
    "            continue\n",
    "        cos_sim = float(np.dot(v1, v2) / denom)\n",
    "        sims.append(cos_sim)\n",
    "\n",
    "    if not sims:\n",
    "        return 0.0\n",
    "\n",
    "    return float(np.mean(sims))\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_cohesion_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute LC for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with lemmatizer, POS tagger, etc.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of LC scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [lexical_cohesion_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_coherence_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute CoH for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with word vectors.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of CoH scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [coherence_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_discourse_measures(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute both LC and CoH for each row of a DataFrame and return\n",
    "    them in a dictionary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"LC\":  np.ndarray of lexical cohesion scores,\n",
    "            \"CoH\": np.ndarray of coherence scores\n",
    "        }\n",
    "    \"\"\"\n",
    "    lc_vec = compute_lexical_cohesion_vector(df, nlp, column=column)\n",
    "    coh_vec = compute_coherence_vector(df, nlp, column=column)\n",
    "    return {\"LC\": lc_vec, \"CoH\": coh_vec}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fee2fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze_text_all(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Parse a text with stanza and compute all measures (lexical + syntactic)\n",
    "    in a single pass.\n",
    "\n",
    "    Returns a dict with keys:\n",
    "        \"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\"\n",
    "    (Discourse measures LC/CoH are added later at DataFrame level, via spaCy.)\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None, \"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    lex = lexical_measures_from_doc(doc)\n",
    "    syn = syntactic_measures_from_doc(doc)\n",
    "\n",
    "    out: Dict[str, Optional[float]] = {}\n",
    "    out.update(lex)\n",
    "    out.update(syn)\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_all_complexity_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    "    spacy_nlp=None,\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute all complexity measures for each row in df[column].\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with a text column.\n",
    "    column : str, default \"text\"\n",
    "        Name of the text column.\n",
    "    lang : str, default \"en\"\n",
    "        Language code for stanza.\n",
    "    n_jobs : int, default 1\n",
    "        Number of worker processes to use.\n",
    "            - 1  : sequential execution (no multiprocessing).\n",
    "            - >1 : multiprocessing with that many workers.\n",
    "            - 0 or None : use cpu_count() workers.\n",
    "    spacy_nlp : spaCy Language, required for LC / CoH\n",
    "        Pre-loaded spaCy pipeline with:\n",
    "            - POS / lemmatizer for LC\n",
    "            - word vectors for CoH (e.g. 'en_core_web_md').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "            \"MDD\":  {index: value},\n",
    "            \"CS\":   {index: value},\n",
    "            \"LC\":   {index: value},\n",
    "            \"CoH\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    items = list(df[column].items())  # list[(index, text)]\n",
    "    total_items = len(items)\n",
    "\n",
    "    # ---- Lexical + syntactic (stanza) ----\n",
    "    for idx, text in tqdm(\n",
    "        items,\n",
    "        total=total_items,\n",
    "        desc=\"Computing lexical & syntactic complexity (sequential)\",\n",
    "    ):\n",
    "        metrics = _analyze_text_all(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "\n",
    "    # ---- Discourse measures (spaCy: LC & CoH) ----\n",
    "    if spacy_nlp is None:\n",
    "        raise ValueError(\n",
    "            \"spacy_nlp must be provided to compute LC and CoH. \"\n",
    "            \"Load a spaCy model with vectors, e.g. 'en_core_web_md', and \"\n",
    "            \"pass it as spacy_nlp=...\"\n",
    "        )\n",
    "\n",
    "    discourse = compute_discourse_measures(df, spacy_nlp, column=column)\n",
    "    lc_vec = discourse[\"LC\"]\n",
    "    coh_vec = discourse[\"CoH\"]\n",
    "\n",
    "    lc_res: Dict[Any, float] = {}\n",
    "    coh_res: Dict[Any, float] = {}\n",
    "\n",
    "    # Map arrays back to DataFrame indices\n",
    "    for i, idx in enumerate(df.index):\n",
    "        lc_res[idx] = float(lc_vec[i])\n",
    "        coh_res[idx] = float(coh_vec[i])\n",
    "\n",
    "    return {\n",
    "        \"MTLD\": mtld_res,\n",
    "        \"LD\": ld_res,\n",
    "        \"LS\": ls_res,\n",
    "        \"MDD\": mdd_res,\n",
    "        \"CS\": cs_res,\n",
    "        \"LC\": lc_res,\n",
    "        \"CoH\": coh_res,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11af2ab",
   "metadata": {},
   "source": [
    "## Calcul métriques Simple/Complex (réutilise compute_all_complexity_measures_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63769d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_both_sides_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    col_simple: str = COL_SIMPLE,\n",
    "    col_complex: str = COL_COMPLEX,\n",
    "    lang: str = LANG,\n",
    "    spacy_nlp=None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule les 7 métriques de complexité pour chaque ligne, pour Simple et Complex,\n",
    "    en réutilisant strictement compute_all_complexity_measures_df (test.ipynb).\n",
    "\n",
    "    Retour : DataFrame original + 14 colonnes :\n",
    "      - simple_MTLD ... simple_CoH\n",
    "      - complex_MTLD ... complex_CoH\n",
    "    \"\"\"\n",
    "    required = {col_simple, col_complex}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}. Colonnes présentes: {list(df.columns)}\")\n",
    "\n",
    "    if spacy_nlp is None:\n",
    "        raise ValueError(\"spacy_nlp doit être fourni (modèle avec vecteurs)\")\n",
    "\n",
    "    # Mesures Simple\n",
    "    simple = compute_all_complexity_measures_df(df, column=col_simple, lang=lang, spacy_nlp=spacy_nlp)\n",
    "    # Mesures Complex\n",
    "    complex_ = compute_all_complexity_measures_df(df, column=col_complex, lang=lang, spacy_nlp=spacy_nlp)\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # On aligne explicitement sur l'index du DataFrame\n",
    "    for m in MEASURES:\n",
    "        out[f\"simple_{m}\"] = pd.Series(simple[m]).reindex(out.index)\n",
    "        out[f\"complex_{m}\"] = pd.Series(complex_[m]).reindex(out.index)\n",
    "\n",
    "    # Conversion numérique (sécurité)\n",
    "    metric_cols = [f\"simple_{m}\" for m in MEASURES] + [f\"complex_{m}\" for m in MEASURES]\n",
    "    for c in metric_cols:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e42537",
   "metadata": {},
   "source": [
    "## Ajout orig_id (numéro de ligne d’origine, 0-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e41759fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_orig_id(df: pd.DataFrame, id_col: str = ID_COL) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ajoute une colonne identifiant stable 'orig_id' basée sur le numéro de ligne d'origine (0-based).\n",
    "    Si la colonne existe déjà, on ne la modifie pas.\n",
    "    \"\"\"\n",
    "    if id_col in df.columns:\n",
    "        return df\n",
    "    df2 = df.reset_index(drop=True).copy()\n",
    "    df2.insert(0, id_col, df2.index.astype(int))\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3b6cd",
   "metadata": {},
   "source": [
    "## Calcul métriques Simple/Complex (réutilise compute_all_complexity_measures_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "636c2817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_both_sides_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    col_simple: str = COL_SIMPLE,\n",
    "    col_complex: str = COL_COMPLEX,\n",
    "    lang: str = LANG,\n",
    "    spacy_nlp=None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule les 7 métriques de complexité pour chaque ligne, pour Simple et Complex,\n",
    "    en réutilisant strictement compute_all_complexity_measures_df (test.ipynb).\n",
    "\n",
    "    Retour : DataFrame original + 14 colonnes :\n",
    "      - simple_MTLD ... simple_CoH\n",
    "      - complex_MTLD ... complex_CoH\n",
    "    \"\"\"\n",
    "    required = {col_simple, col_complex}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}. Colonnes présentes: {list(df.columns)}\")\n",
    "\n",
    "    if spacy_nlp is None:\n",
    "        raise ValueError(\"spacy_nlp doit être fourni (modèle avec vecteurs)\")\n",
    "\n",
    "    # Mesures Simple\n",
    "    simple = compute_all_complexity_measures_df(df, column=col_simple, lang=lang, spacy_nlp=spacy_nlp)\n",
    "    # Mesures Complex\n",
    "    complex_ = compute_all_complexity_measures_df(df, column=col_complex, lang=lang, spacy_nlp=spacy_nlp)\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # On aligne explicitement sur l'index du DataFrame\n",
    "    for m in MEASURES:\n",
    "        out[f\"simple_{m}\"] = pd.Series(simple[m]).reindex(out.index)\n",
    "        out[f\"complex_{m}\"] = pd.Series(complex_[m]).reindex(out.index)\n",
    "\n",
    "    # Conversion numérique (sécurité)\n",
    "    metric_cols = [f\"simple_{m}\" for m in MEASURES] + [f\"complex_{m}\" for m in MEASURES]\n",
    "    for c in metric_cols:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e59b8a",
   "metadata": {},
   "source": [
    "## Filtrage NaN (exigence “si NaN → ligne rejetée”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11abd1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_with_any_nan_metrics(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Supprime toute ligne pour laquelle au moins une des 14 métriques (Simple/Complex) est NaN.\n",
    "    Retourne :\n",
    "      - df_kept_nan : lignes conservées\n",
    "      - df_removed_nan : lignes supprimées à cause de NaN\n",
    "    \"\"\"\n",
    "    metric_cols = [f\"simple_{m}\" for m in MEASURES] + [f\"complex_{m}\" for m in MEASURES]\n",
    "    mask_ok = df[metric_cols].notna().all(axis=1)\n",
    "    return df[mask_ok].copy(), df[~mask_ok].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca6486",
   "metadata": {},
   "source": [
    "## Dominance (Simple > Complex sur ≥ MIN_METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be549f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominance_flags(df: pd.DataFrame, min_metrics: int = MIN_METRICS) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Une ligne est en dominance si Simple est strictement > Complex sur au moins `min_metrics` métriques.\n",
    "    (Ici, on suppose que les NaN ont déjà été supprimés.)\n",
    "    \"\"\"\n",
    "    wins = []\n",
    "    for m in MEASURES:\n",
    "        s = df[f\"simple_{m}\"]\n",
    "        c = df[f\"complex_{m}\"]\n",
    "        wins.append(s > c)\n",
    "\n",
    "    wins_count = pd.concat(wins, axis=1).sum(axis=1)\n",
    "    return wins_count >= min_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f058e4",
   "metadata": {},
   "source": [
    "## Export CSV (All / Kept / Removed / Removed IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ce55bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_csv_bundle(dataset_name: str, df_all: pd.DataFrame, df_kept: pd.DataFrame, df_removed: pd.DataFrame) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte les résultats en CSV (UTF-8), selon le format attendu.\n",
    "    \"\"\"\n",
    "    all_path = OUT_DIR / f\"{dataset_name}_with_metrics_all.csv\"\n",
    "    kept_path = OUT_DIR / f\"{dataset_name}_with_metrics_kept.csv\"\n",
    "    removed_path = OUT_DIR / f\"{dataset_name}_with_metrics_removed.csv\"\n",
    "    removed_ids_path = OUT_DIR / f\"{dataset_name}_removed_ids.csv\"\n",
    "\n",
    "    df_all.to_csv(all_path, index=False, encoding=\"utf-8\")\n",
    "    df_kept.to_csv(kept_path, index=False, encoding=\"utf-8\")\n",
    "    df_removed.to_csv(removed_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    pd.DataFrame({ID_COL: df_removed[ID_COL].astype(int).sort_values()}).to_csv(\n",
    "        removed_ids_path, index=False, encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"all\": str(all_path),\n",
    "        \"kept\": str(kept_path),\n",
    "        \"removed\": str(removed_path),\n",
    "        \"removed_ids\": str(removed_ids_path),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604e71e",
   "metadata": {},
   "source": [
    "## Pipeline dataset (16 colonnes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7653d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_offline(dataset_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Prépare un dataset :\n",
    "      1) charge les données via load_dataset (test.ipynb)\n",
    "      2) ajoute orig_id (numéro de ligne d'origine)\n",
    "      3) calcule les métriques offline (Simple & Complex)\n",
    "      4) supprime les lignes avec NaN sur une métrique\n",
    "      5) supprime les lignes en dominance\n",
    "      6) exporte en CSV\n",
    "\n",
    "    Sortie finale (df_kept) : 16 colonnes minimales :\n",
    "      orig_id, Simple, Complex, 7 métriques Simple, 7 métriques Complex\n",
    "    \"\"\"\n",
    "    df_raw = load_dataset(dataset_name)\n",
    "    df_raw = add_orig_id(df_raw, id_col=ID_COL)\n",
    "\n",
    "    # On conserve uniquement les colonnes texte + id (pour éviter des surprises)\n",
    "    df_base = df_raw[[ID_COL, COL_SIMPLE, COL_COMPLEX]].copy()\n",
    "\n",
    "    # Calcul des métriques (offline, coûteux)\n",
    "    df_all = compute_both_sides_metrics(df_base, spacy_nlp=spacy_nlp)\n",
    "\n",
    "    # Marquage NaN\n",
    "    df_no_nan, df_removed_nan = drop_rows_with_any_nan_metrics(df_all)\n",
    "    df_no_nan[\"removed_reason\"] = \"\"  # juste pour cohérence si besoin\n",
    "\n",
    "    # Dominance\n",
    "    dom = dominance_flags(df_no_nan, min_metrics=MIN_METRICS)\n",
    "    df_no_nan[\"simple_dominates_complex\"] = dom\n",
    "\n",
    "    df_removed_dom = df_no_nan[df_no_nan[\"simple_dominates_complex\"]].copy()\n",
    "    df_kept = df_no_nan[~df_no_nan[\"simple_dominates_complex\"]].copy()\n",
    "\n",
    "    # Bundle removed = NaN + dominance (avec raison)\n",
    "    df_removed_nan = df_removed_nan.copy()\n",
    "    if not df_removed_nan.empty:\n",
    "        df_removed_nan[\"simple_dominates_complex\"] = False\n",
    "        df_removed_nan[\"removed_reason\"] = \"nan_metrics\"\n",
    "\n",
    "    if not df_removed_dom.empty:\n",
    "        df_removed_dom[\"removed_reason\"] = \"dominance\"\n",
    "\n",
    "    df_removed = pd.concat([df_removed_nan, df_removed_dom], axis=0).sort_values(ID_COL)\n",
    "\n",
    "    # Garder exactement les 16 colonnes demandées (+ indicateur dominance optionnel dans df_all/removed)\n",
    "    metric_cols_simple = [f\"simple_{m}\" for m in MEASURES]\n",
    "    metric_cols_complex = [f\"complex_{m}\" for m in MEASURES]\n",
    "    cols_16 = [ID_COL, COL_SIMPLE, COL_COMPLEX] + metric_cols_simple + metric_cols_complex\n",
    "\n",
    "    df_all_export = df_all[cols_16].copy()\n",
    "    df_kept_export = df_kept[cols_16].copy()\n",
    "    df_removed_export = df_removed[cols_16 + [\"removed_reason\"]].copy() if \"removed_reason\" in df_removed.columns else df_removed[cols_16].copy()\n",
    "\n",
    "    paths = export_csv_bundle(dataset_name, df_all_export, df_kept_export, df_removed_export)\n",
    "\n",
    "    return {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"rows_all\": int(df_all_export.shape[0]),\n",
    "        \"rows_kept\": int(df_kept_export.shape[0]),\n",
    "        \"rows_removed\": int(df_removed_export.shape[0]),\n",
    "        \"paths\": paths,\n",
    "        \"df_all\": df_all_export,\n",
    "        \"df_kept\": df_kept_export,\n",
    "        \"df_removed\": df_removed_export,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16cb9e4",
   "metadata": {},
   "source": [
    "## Exécution sur tous les datasets + aperçu DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9244597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preparing: ose_adv_ele ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing lexical & syntactic complexity (sequential):   0%|          | 0/189 [00:00<?, ?it/s]2025-12-18 21:53:03 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 7.27MB/s]                    \n",
      "2025-12-18 21:53:04 INFO: Downloaded file to C:\\Users\\rroll\\stanza_resources\\resources.json\n",
      "2025-12-18 21:53:04 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-12-18 21:53:04 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "======================================\n",
      "\n",
      "2025-12-18 21:53:04 INFO: Using device: cpu\n",
      "2025-12-18 21:53:04 INFO: Loading: tokenize\n",
      "2025-12-18 21:53:06 INFO: Loading: mwt\n",
      "2025-12-18 21:53:06 INFO: Loading: pos\n",
      "2025-12-18 21:53:07 INFO: Loading: lemma\n",
      "2025-12-18 21:53:08 INFO: Loading: constituency\n",
      "2025-12-18 21:53:08 INFO: Loading: depparse\n",
      "2025-12-18 21:53:08 INFO: Done loading processors!\n",
      "Computing lexical & syntactic complexity (sequential): 100%|██████████| 189/189 [14:01<00:00,  4.45s/it]\n",
      "Computing lexical & syntactic complexity (sequential): 100%|██████████| 189/189 [21:10<00:00,  6.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: 189 | Kept: 188 | Removed: 1\n",
      "Saved to: {'all': 'data_prepared_csv\\\\ose_adv_ele_with_metrics_all.csv', 'kept': 'data_prepared_csv\\\\ose_adv_ele_with_metrics_kept.csv', 'removed': 'data_prepared_csv\\\\ose_adv_ele_with_metrics_removed.csv', 'removed_ids': 'data_prepared_csv\\\\ose_adv_ele_removed_ids.csv'}\n",
      "\n",
      "=== Preparing: ose_adv_int ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing lexical & syntactic complexity (sequential):  11%|█         | 20/189 [01:46<15:02,  5.34s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m datasets.keys():\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Preparing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     info = \u001b[43mprepare_dataset_offline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     results[name] = info\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAll: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo[\u001b[33m'\u001b[39m\u001b[33mrows_all\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Kept: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo[\u001b[33m'\u001b[39m\u001b[33mrows_kept\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Removed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo[\u001b[33m'\u001b[39m\u001b[33mrows_removed\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mprepare_dataset_offline\u001b[39m\u001b[34m(dataset_name)\u001b[39m\n\u001b[32m     18\u001b[39m df_base = df_raw[[ID_COL, COL_SIMPLE, COL_COMPLEX]].copy()\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Calcul des métriques (offline, coûteux)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m df_all = \u001b[43mcompute_both_sides_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspacy_nlp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspacy_nlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Marquage NaN\u001b[39;00m\n\u001b[32m     24\u001b[39m df_no_nan, df_removed_nan = drop_rows_with_any_nan_metrics(df_all)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mcompute_both_sides_metrics\u001b[39m\u001b[34m(df, col_simple, col_complex, lang, spacy_nlp)\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mspacy_nlp doit être fourni (modèle avec vecteurs)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Mesures Simple\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m simple = \u001b[43mcompute_all_complexity_measures_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcol_simple\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspacy_nlp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspacy_nlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Mesures Complex\u001b[39;00m\n\u001b[32m     27\u001b[39m complex_ = compute_all_complexity_measures_df(df, column=col_complex, lang=lang, spacy_nlp=spacy_nlp)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mcompute_all_complexity_measures_df\u001b[39m\u001b[34m(df, column, lang, spacy_nlp)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# ---- Lexical + syntactic (stanza) ----\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, text \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[32m     80\u001b[39m     items,\n\u001b[32m     81\u001b[39m     total=total_items,\n\u001b[32m     82\u001b[39m     desc=\u001b[33m\"\u001b[39m\u001b[33mComputing lexical & syntactic complexity (sequential)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     83\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     metrics = \u001b[43m_analyze_text_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     mtld_res[idx] = metrics[\u001b[33m\"\u001b[39m\u001b[33mMTLD\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     86\u001b[39m     ld_res[idx] = metrics[\u001b[33m\"\u001b[39m\u001b[33mLD\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36m_analyze_text_all\u001b[39m\u001b[34m(text, lang)\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mMTLD\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mLD\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mLS\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mMDD\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mCS\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[32m     17\u001b[39m nlp = get_stanza_pipeline(lang)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m doc = \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m lex = lexical_measures_from_doc(doc)\n\u001b[32m     21\u001b[39m syn = syntactic_measures_from_doc(doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\pipeline\\core.py:480\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, doc, processors)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\pipeline\\core.py:431\u001b[39m, in \u001b[36mPipeline.process\u001b[39m\u001b[34m(self, doc, processors)\u001b[39m\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processors.get(processor_name):\n\u001b[32m    430\u001b[39m         process = \u001b[38;5;28mself\u001b[39m.processors[processor_name].bulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processors[processor_name].process\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m         doc = \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\pipeline\\constituency_processor.py:69\u001b[39m, in \u001b[36mConstituencyProcessor.process\u001b[39m\u001b[34m(self, document)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tqdm:\n\u001b[32m     67\u001b[39m     words = tqdm(words)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m trees = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_tagged_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m trees = unsort(trees, original_indices)\n\u001b[32m     71\u001b[39m document.set(CONSTITUENCY, trees, to_sentence=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\models\\constituency\\base_model.py:399\u001b[39m, in \u001b[36mBaseModel.parse_tagged_words\u001b[39m\u001b[34m(self, words, batch_size)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28mself\u001b[39m.eval()\n\u001b[32m    398\u001b[39m sentence_iterator = \u001b[38;5;28miter\u001b[39m(words)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m treebank = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_sentences_no_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_batch_from_tagged_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_constituents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m results = [t.predictions[\u001b[32m0\u001b[39m].tree \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m treebank]\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\models\\constituency\\base_model.py:364\u001b[39m, in \u001b[36mBaseModel.parse_sentences_no_grad\u001b[39m\u001b[34m(self, data_iterator, build_batch_fn, batch_size, transition_choice, keep_state, keep_constituents, keep_scores)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    358\u001b[39m \u001b[33;03mGiven an iterator over the data and a method for building batches, returns a list of parse trees.\u001b[39;00m\n\u001b[32m    359\u001b[39m \n\u001b[32m    360\u001b[39m \u001b[33;03mno_grad() is so that gradients aren't kept, which makes the model\u001b[39;00m\n\u001b[32m    361\u001b[39m \u001b[33;03mrun faster and use less memory at inference time\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuild_batch_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransition_choice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_constituents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_scores\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\models\\constituency\\base_model.py:296\u001b[39m, in \u001b[36mBaseModel.parse_sentences\u001b[39m\u001b[34m(self, data_iterator, build_batch_fn, batch_size, transition_choice, keep_state, keep_constituents, keep_scores)\u001b[39m\n\u001b[32m    294\u001b[39m treebank = []\n\u001b[32m    295\u001b[39m treebank_indices = []\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m state_batch = \u001b[43mbuild_batch_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# used to track which indices we are currently parsing\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;66;03m# since the parses get finished at different times, this will let us unsort after\u001b[39;00m\n\u001b[32m    299\u001b[39m batch_indices = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(state_batch)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\models\\constituency\\base_model.py:273\u001b[39m, in \u001b[36mBaseModel.build_batch_from_tagged_words\u001b[39m\u001b[34m(self, batch_size, data_iterator)\u001b[39m\n\u001b[32m    270\u001b[39m     state_batch.append(sentence)\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state_batch) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     state_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitial_state_from_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\models\\constituency\\base_model.py:224\u001b[39m, in \u001b[36mBaseModel.initial_state_from_words\u001b[39m\u001b[34m(self, word_lists)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minitial_state_from_words\u001b[39m(\u001b[38;5;28mself\u001b[39m, word_lists):\n\u001b[32m    222\u001b[39m     preterminal_lists = [[Tree(tag, Tree(word)) \u001b[38;5;28;01mfor\u001b[39;00m word, tag \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[32m    223\u001b[39m                          \u001b[38;5;28;01mfor\u001b[39;00m words \u001b[38;5;129;01min\u001b[39;00m word_lists]\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitial_state_from_preterminals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreterminal_lists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgold_trees\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgold_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\models\\constituency\\base_model.py:200\u001b[39m, in \u001b[36mBaseModel.initial_state_from_preterminals\u001b[39m\u001b[34m(self, preterminal_lists, gold_trees, gold_sequences)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minitial_state_from_preterminals\u001b[39m(\u001b[38;5;28mself\u001b[39m, preterminal_lists, gold_trees, gold_sequences):\n\u001b[32m    197\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03m    what is passed in should be a list of list of preterminals\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     word_queues = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitial_word_queues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreterminal_lists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     \u001b[38;5;66;03m# this is the bottom of the TreeStack and will be the same for each State\u001b[39;00m\n\u001b[32m    202\u001b[39m     transitions = \u001b[38;5;28mself\u001b[39m.initial_transitions()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\models\\constituency\\lstm_model.py:779\u001b[39m, in \u001b[36mLSTMModel.initial_word_queues\u001b[39m\u001b[34m(self, tagged_word_lists)\u001b[39m\n\u001b[32m    776\u001b[39m     all_word_inputs.append(word_inputs)\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.forward_charlm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m     all_forward_chars = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_charlm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_char_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_word_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m word_inputs, forward_chars \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_word_inputs, all_forward_chars):\n\u001b[32m    781\u001b[39m         word_inputs.append(forward_chars)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:222\u001b[39m, in \u001b[36mCharacterLanguageModel.build_char_representation\u001b[39m\u001b[34m(self, sentences)\u001b[39m\n\u001b[32m    219\u001b[39m chars = get_long_tensor(chars, \u001b[38;5;28mlen\u001b[39m(all_data), pad_id=vocab.unit2id(CHARLM_END)).to(device=device)\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     output, _, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m     res = [output[i, offsets] \u001b[38;5;28;01mfor\u001b[39;00m i, offsets \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(char_offsets)]\n\u001b[32m    224\u001b[39m     res = unsort(res, orig_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:153\u001b[39m, in \u001b[36mCharacterLanguageModel.forward\u001b[39m\u001b[34m(self, chars, charlens, hidden)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hidden \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[32m    151\u001b[39m     hidden = (\u001b[38;5;28mself\u001b[39m.charlstm_h_init.expand(\u001b[38;5;28mself\u001b[39m.args[\u001b[33m'\u001b[39m\u001b[33mchar_num_layers\u001b[39m\u001b[33m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m.args[\u001b[33m'\u001b[39m\u001b[33mchar_hidden_dim\u001b[39m\u001b[33m'\u001b[39m]).contiguous(),\n\u001b[32m    152\u001b[39m               \u001b[38;5;28mself\u001b[39m.charlstm_c_init.expand(\u001b[38;5;28mself\u001b[39m.args[\u001b[33m'\u001b[39m\u001b[33mchar_num_layers\u001b[39m\u001b[33m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m.args[\u001b[33m'\u001b[39m\u001b[33mchar_hidden_dim\u001b[39m\u001b[33m'\u001b[39m]).contiguous())\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m output, hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcharlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m output = \u001b[38;5;28mself\u001b[39m.dropout(pad_packed_sequence(output, batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m])\n\u001b[32m    155\u001b[39m decoded = \u001b[38;5;28mself\u001b[39m.decoder(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\models\\common\\packed_lstm.py:22\u001b[39m, in \u001b[36mPackedLSTM.forward\u001b[39m\u001b[34m(self, input, lengths, hx)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, PackedSequence):\n\u001b[32m     20\u001b[39m     \u001b[38;5;28minput\u001b[39m = pack_padded_sequence(\u001b[38;5;28minput\u001b[39m, lengths, batch_first=\u001b[38;5;28mself\u001b[39m.batch_first)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pad:\n\u001b[32m     24\u001b[39m     res = (pad_packed_sequence(res[\u001b[32m0\u001b[39m], batch_first=\u001b[38;5;28mself\u001b[39m.batch_first)[\u001b[32m0\u001b[39m], res[\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1139\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1127\u001b[39m     result = _VF.lstm(\n\u001b[32m   1128\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1129\u001b[39m         hx,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1136\u001b[39m         \u001b[38;5;28mself\u001b[39m.batch_first,\n\u001b[32m   1137\u001b[39m     )\n\u001b[32m   1138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1139\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1150\u001b[39m output = result[\u001b[32m0\u001b[39m]\n\u001b[32m   1151\u001b[39m hidden = result[\u001b[32m1\u001b[39m:]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name in datasets.keys():\n",
    "    print(f\"\\n=== Preparing: {name} ===\")\n",
    "    info = prepare_dataset_offline(name)\n",
    "    results[name] = info\n",
    "    print(f\"All: {info['rows_all']} | Kept: {info['rows_kept']} | Removed: {info['rows_removed']}\")\n",
    "    print(\"Saved to:\", info[\"paths\"])\n",
    "\n",
    "# Aperçu du dataset préparé (kept)\n",
    "example_name = next(iter(results.keys()))\n",
    "df_prepared = results[example_name][\"df_kept\"]\n",
    "df_prepared.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
