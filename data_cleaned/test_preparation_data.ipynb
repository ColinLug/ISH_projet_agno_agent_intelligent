{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fe89001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 7.72MB/s]                    \n",
      "2025-12-18 23:28:08 INFO: Downloaded file to C:\\Users\\rroll\\stanza_resources\\resources.json\n",
      "2025-12-18 23:28:08 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-12-18 23:28:09 INFO: File exists: C:\\Users\\rroll\\stanza_resources\\en\\default.zip\n",
      "2025-12-18 23:28:11 INFO: Finished downloading models and saved to C:\\Users\\rroll\\stanza_resources\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rroll\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rroll\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x2401cb86910>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "from pprint import pprint\n",
    "from typing import Dict, Set, Iterable, Optional, Any, Tuple\n",
    "import importlib.resources as pkg_resources\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "import stanza\n",
    "import textcomplexity  # only used to access en.json\n",
    "from tqdm.auto import tqdm  \n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Download required resources\n",
    "stanza.download('en')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Make sure WordNet is available; if not, download it.\n",
    "try:\n",
    "    _ = wn.synsets(\"dog\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "spacy_nlp = nlp\n",
    "spacy_nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed542395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Colonnes attendues\n",
    "COL_SIMPLE = \"Simple\"\n",
    "COL_COMPLEX = \"Complex\"\n",
    "ID_COL = \"orig_id\"\n",
    "\n",
    "# Sorties\n",
    "OUT_DIR = Path(\"data_prepared\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Langue / modèles\n",
    "LANG = \"en\"\n",
    "SPACY_MODEL = \"en_core_web_md\"\n",
    "\n",
    "# Dominance : Simple > Complex sur au moins X métriques\n",
    "MIN_METRICS = 5\n",
    "\n",
    "# Liste officielle des métriques (test.ipynb)\n",
    "MEASURES = [\"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\", \"LC\", \"CoH\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e044f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement spaCy (doit contenir des vecteurs pour CoH)\n",
    "nlp = spacy.load(SPACY_MODEL)\n",
    "spacy_nlp = nlp\n",
    "\n",
    "# Important : segmentation en phrases\n",
    "if \"sentencizer\" not in spacy_nlp.pipe_names:\n",
    "    spacy_nlp.add_pipe(\"sentencizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff4f785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire des datasets\n",
    "datasets ={'ose_adv_ele':'data_sampled/OSE_adv_ele.csv', \n",
    "           'ose_adv_int':'data_sampled/OSE_adv_int.csv',\n",
    "           'swipe': 'data_sampled/swipe.csv',\n",
    "           'vikidia':'data_sampled/vikidia.csv'}\n",
    "\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path, sep='\\t')\n",
    "    \n",
    "\n",
    "def load_dataset(name):\n",
    "    if name not in datasets:\n",
    "        raise ValueError(f\"Dataset {name} not found\")\n",
    "    return load_data(datasets[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e76a962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Simple</th>\n",
       "      <th>Complex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿To tourists, Amsterdam still seems very liber...</td>\n",
       "      <td>﻿Amsterdam still looks liberal to tourists, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>﻿Anitta, a music star from Brazil, has million...</td>\n",
       "      <td>﻿Brazil’s latest funk sensation, Anitta, has w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Simple  \\\n",
       "0  ﻿When you see the word Amazon, what’s the firs...   \n",
       "1  ﻿To tourists, Amsterdam still seems very liber...   \n",
       "2  ﻿Anitta, a music star from Brazil, has million...   \n",
       "\n",
       "                                             Complex  \n",
       "0  ﻿When you see the word Amazon, what’s the firs...  \n",
       "1  ﻿Amsterdam still looks liberal to tourists, wh...  \n",
       "2  ﻿Brazil’s latest funk sensation, Anitta, has w...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('ose_adv_ele')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "127c2dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE TEXT\n",
      "﻿Life isn’t fair sometimes. Mark Zuckerberg created Facebook and is now worth $48 billion. James Goodfellow also invented something that millions of people around the world use every day – the cash machine – but it didn’t make him rich. In fact, he earned just £10 from the patent and has not made any money from it since.\n",
      "Who is the inventor of the ATM? People have argued for years over this question. In 2005, a man called John Shepherd-Barron received a UK honour as the “inventor of the automatic cash dispenser”. But, the UK government is now saying it was Goodfellow who invented the ATM.\n",
      "In the mid-1960s, Goodfellow’s managers asked him to think of a way to allow customers to withdraw cash from banks on Saturdays. “Most people worked during the week and couldn’t go to the bank. They wanted a solution. The solution was a machine which would give cash to a customer,” he says. “I wanted to develop a cash machine and, to make this happen, I invented the PIN [personal identification number] and a coded token.”\n",
      "Goodfellow’s first machines were installed in 1967. At around the same time, Shepherd-Barron was developing a similar machine. His machine didn’t use plastic cards – it used cheques.\n",
      "Most people agree that Shepherd-Barron’s ATM was the “world’s first” to be installed and used by the public. The first one was at a bank in north London. It was opened on 27 June, 1967 – a month before Goodfellow’s ATM appeared. But, Goodfellow registered the patent for his machine on 2 May, 1966, 14 months before Shepherd-Barron’s ATM machine was first used.\n",
      "Shepherd-Barron received an official honour for his invention and Goodfellow says: “My one big regret is that I never said anything about it until John Shepherd-Barron received the honour in 2005. The Queen gave him this honour for inventing the automatic cash dispenser. That really annoyed me and I complained about it.”\n",
      "Shepherd-Barron is dead now but, in a 2005 interview, he criticized Goodfellow. He said Goodfellow’s invention was a failure.\n",
      "The cash machine is now used all over the world and, every year, there are more and more: there are now three million ATMs worldwide and there will be four million by 2020. The good news for Goodfellow is that people are beginning to recognize him for his invention. The website ATMInventor.com says: “Who invented the idea of an ATM? We believe it was Luther George Simjian. Who invented the ATM as we know it? It was James Goodfellow for holding a patent date of 1966.”\n",
      "Even better for Goodfellow, his invention is in a 180-page guidebook called Life in the United Kingdom. In the section about “great British inventions of the twentieth century”, it says: “In the 1960s, James Goodfellow (1937-) invented the automatic teller machine (ATM) or ‘cashpoint’.”\n",
      "So after all these years, Goodfellow is finally among a group of famous British inventors with John Logie Baird (the television), Alan Turing (the Turing machine), Sir Frank Whittle (the jet engine) and Sir Tim Berners-Lee (the World Wide Web). When he was asked what he did with the £10 he received in the 1960s, Goodfellow said he spent it on a night out. “It didn’t change my life,” he said.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "COMPLEX TEXT\n",
      "﻿Sometimes life just isn’t fair. Mark Zuckerberg created Facebook and is now worth an estimated $48bn. James Goodfellow also invented something used by millions of people around the world every day – the cash machine – but it didn’t make him rich. In fact, he earned just £10 from the patent and has not made a penny more from it since.\n",
      "“You can imagine how I feel when I see bankers getting £1m bonuses. I wonder what they contributed to the banking industry more than I did to merit a £1m bonus. It doesn’t make much sense to me but that’s the way of the world,” Goodfellow says.\n",
      "It also annoys him that he’s not seen as a good role model for inventors and engineers. He came up with a groundbreaking invention that generated billions of pounds “and I got nothing, so who’s going to want to follow in James Goodfellow’s footsteps and get £10 if they have a fantastic success?”\n",
      "There have been arguments for years over who should officially go down in history as “the inventor of the ATM” and, in 2005, a man called John Shepherd-Barron received a UK honour for services to banking as the “inventor of the automatic cash dispenser”. But, the UK government is now saying it was Goodfellow who invented the ATM – so it would seem that, after all the squabbling, his place in history is now assured.\n",
      "Back in the mid-1960s, Goodfellow was working as a development engineer and had been asked to devise a way to enable customers to withdraw cash from banks on Saturdays. “Most people working during the week couldn’t get to the bank. They wanted a solution. The solution was a machine which would issue cash on demand to a recognized customer,” he recalls. “I set out to develop a cash-issuing machine and, to make this a reality, I invented the PIN [personal identification number] and an associated coded token.”\n",
      "This token took the form of a plastic card with holes punched in it. The patent documents proposed a system incorporating a card reader and buttons mounted in an external wall of the bank and stated: “When the customer wishes to withdraw a pack of banknotes from the system, he simply inserts his punched card in the card reader of the system and operates the set of ten push-buttons in accordance with his personal identification number.”\n",
      "Aside from the cards with punched holes, that pretty much describes today’s ATM. After Goodfellow successfully demonstrated the methodology by producing a model, prototypes were built and the first machines were installed in 1967.\n",
      "At around the same time, Shepherd-Barron was developing a rival cash-dispensing device. His machine didn’t use plastic cards – instead, it used cheques impregnated with carbon-14, a mildly radioactive substance. The machine detected the carbon-14, matched the cheque against a PIN and paid out the cash.\n",
      "It is widely accepted that the Shepherd-Barron ATM was the “world’s first” when it comes to being installed and used by the public; the first one, at a bank in north London, was opened on 27 June, 1967 – a month before Goodfellow’s ATM made its public debut. However, the patent for Goodfellow’s machine was lodged on 2 May, 1966, 14 months before the London ATM machine came into service.\n",
      "The rivalry between the two men bubbled up when Shepherd-Barron received an official honour for his achievement. Goodfellow says: “My one big regret is that I never said anything about it until John Shepherd-Barron received the OBE in 2005. This honour was granted on the basis that he was the inventor of the automatic cash dispenser. That really stuck in my throat and I kicked up a bit of a fuss.” Shepherd-Barron is no longer alive to put his case but, in a 2005 interview, he was fairly withering about his rival: “I don’t know him but it’s clear that the difference between Goodfellow and us was that we thought through the whole system concept and that was important to the banks who bought it. His invention reminds me of the hovercraft, an elegant failure.”\n",
      "The cash machine has become a world-conquering piece of technology and nothing – the contactless revolution, bitcoin, wearable technology, etc – seems to be slowing its growth: there are now 3m ATMs worldwide, with the number forecast to hit 4m by 2020.\n",
      "Goodfellow accepts he didn’t invent the concept of a cash-issuing machine “but I did invent a way of doing it. When people talk about the Wright brothers, they didn’t invent the concept of flying – everyone was trying to do it – but they did it and got the credit for inventing the aeroplane so I think I should get the credit for inventing the cash dispenser.”\n",
      "The good news for Goodfellow is that this is now starting to happen. The website ATMInventor.com concludes: “Who invented the idea of an ATM? We believe it was Luther George Simjian. Who invented the ATM as we know it? We have to think it was James Goodfellow in Scotland for holding a patent date of 1966. Who invented the ATM design we recognize today? We think it was John D White for Docutel in the US.”\n",
      "Even better for Goodfellow, his achievement has been officially recognized in the latest edition of a 180-page guidebook called Life in the United Kingdom. In the section about “great British inventions of the twentieth century ”, it states: “In the 1960s, James Goodfellow (1937-) invented the cash-dispensing automatic teller machine (ATM) or 'cashpoint'.”\n",
      "So after all these years, Goodfellow finally finds himself being talked about in the same breath as John Logie Baird (the television), Alan Turing (the Turing machine), Sir Frank Whittle (the jet engine) and Sir Tim Berners-Lee (the World Wide Web). Asked what he did with the £10 he received back in the 1960s, Goodfellow says he thinks he blew it on a wild night out, adding: “It didn’t change my life.” But, he concludes, it’s been a good working life: “I was very happy doing the job I was doing.”\n"
     ]
    }
   ],
   "source": [
    "row = df.sample(1)\n",
    "\n",
    "print('SIMPLE TEXT')\n",
    "print(row['Simple'].iloc[0])\n",
    "print('-'*100)\n",
    "print('COMPLEX TEXT')\n",
    "print(row['Complex'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e0dfffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ose_adv_ele: 189 rows\n",
      "ose_adv_int: 189 rows\n",
      "swipe: 1233 rows\n",
      "vikidia: 1233 rows\n",
      "Total: 2844 rows\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for name, path in datasets.items():\n",
    "    df = load_dataset(name)\n",
    "    print(f\"{name}: {df.shape[0]} rows\")\n",
    "    cnt += df.shape[0]\n",
    "print(f\"Total: {cnt} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "afca2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset('ose_adv_ele')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02a669",
   "metadata": {},
   "source": [
    "## Complexity measures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa4b934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache stanza pipelines to avoid re-loading models\n",
    "_STANZA_PIPELINES: Dict[str, stanza.Pipeline] = {}\n",
    "\n",
    "# UPOS tags considered content words (C)\n",
    "CONTENT_UPOS = {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def load_cow_top5000_en() -> Set[str]:\n",
    "    \"\"\"\n",
    "    Load the COW-based list of the 5,000 most frequent English content words\n",
    "    from textcomplexity's English language definition file (en.json).\n",
    "\n",
    "    We ignore POS tags and keep only lowercased word forms.\n",
    "    \"\"\"\n",
    "    with pkg_resources.files(textcomplexity).joinpath(\"en.json\").open(\n",
    "        \"r\", encoding=\"utf-8\"\n",
    "    ) as f:\n",
    "        lang_def = json.load(f)\n",
    "\n",
    "    most_common = lang_def[\"most_common\"]  # list of [word, xpos]\n",
    "    cow_top5000 = {w.lower() for w, xpos in most_common}\n",
    "    return cow_top5000\n",
    "\n",
    "\n",
    "def get_stanza_pipeline(lang: str = \"en\", use_gpu: bool = False) -> stanza.Pipeline:\n",
    "    \"\"\"\n",
    "    Get (or create) a cached stanza Pipeline for a given language.\n",
    "\n",
    "    NOTE: You must have downloaded the models beforehand, e.g.:\n",
    "        import stanza\n",
    "        stanza.download('en')\n",
    "    \"\"\"\n",
    "    if lang not in _STANZA_PIPELINES:\n",
    "        _STANZA_PIPELINES[lang] = stanza.Pipeline(\n",
    "            lang=lang,\n",
    "            processors=\"tokenize,pos,lemma,depparse,constituency\",\n",
    "            use_gpu=use_gpu,\n",
    "            tokenize_no_ssplit=False,\n",
    "        )\n",
    "    return _STANZA_PIPELINES[lang]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d67b5",
   "metadata": {},
   "source": [
    "### Lexical complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8ee6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_mtld(tokens: Iterable[str], ttr_threshold: float = 0.72) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute MTLD (Measure of Textual Lexical Diversity) for a list of tokens.\n",
    "\n",
    "    MTLD = total_number_of_tokens / number_of_factors\n",
    "\n",
    "    A factor is a contiguous segment where the running TTR stays >= threshold.\n",
    "    When the TTR drops below the threshold, we close a factor (at the previous\n",
    "    token) and start a new one. At the end, the remaining partial segment is\n",
    "    counted as a fractional factor, with weight proportional to how close the\n",
    "    final TTR is to the threshold.\n",
    "    \"\"\"\n",
    "    tokens = [tok for tok in tokens if tok]\n",
    "    if not tokens:\n",
    "        return None\n",
    "\n",
    "    types = set()\n",
    "    factor_count = 0.0\n",
    "    token_count_in_factor = 0\n",
    "\n",
    "    for tok in tokens:\n",
    "        token_count_in_factor += 1\n",
    "        types.add(tok)\n",
    "        ttr = len(types) / token_count_in_factor\n",
    "\n",
    "        if ttr < ttr_threshold:\n",
    "            factor_count += 1.0\n",
    "            types = set()\n",
    "            token_count_in_factor = 0\n",
    "\n",
    "    # final partial factor\n",
    "    if token_count_in_factor > 0:\n",
    "        final_ttr = len(types) / token_count_in_factor\n",
    "        if final_ttr < 1.0:\n",
    "            fractional = (1.0 - final_ttr) / (1.0 - ttr_threshold)\n",
    "            fractional = max(0.0, min(1.0, fractional))\n",
    "            factor_count += fractional\n",
    "\n",
    "    if factor_count == 0:\n",
    "        return None\n",
    "\n",
    "    return len(tokens) / factor_count\n",
    "\n",
    "\n",
    "\n",
    "def _compute_lexical_density(total_tokens: int, content_tokens: int) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LD = |C| / |T|\n",
    "    where:\n",
    "        |C| = number of content-word tokens\n",
    "        |T| = total number of non-punctuation tokens\n",
    "    \"\"\"\n",
    "    if total_tokens == 0:\n",
    "        return None\n",
    "    return content_tokens / total_tokens\n",
    "\n",
    "\n",
    "def _compute_lexical_sophistication_cow(\n",
    "    content_forms: Iterable[str],\n",
    "    cow_top5000: set,\n",
    ") -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LS = |{ w in C : w not in R }| / |C|\n",
    "    where:\n",
    "        C = content-word tokens (surface forms, lowercased)\n",
    "        R = COW top-5000 content word forms (lowercased)\n",
    "    \"\"\"\n",
    "    forms = [f for f in content_forms if f]\n",
    "    if not forms:\n",
    "        return None\n",
    "\n",
    "    off_list = sum(1 for f in forms if f not in cow_top5000)\n",
    "    return off_list / len(forms)\n",
    "\n",
    "\n",
    "\n",
    "def lexical_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MTLD, LD, LS from a stanza Document.\n",
    "    \"\"\"\n",
    "    cow_top5000 = load_cow_top5000_en()\n",
    "\n",
    "    mtld_tokens = []\n",
    "    total_tokens = 0\n",
    "    content_tokens = 0\n",
    "    content_forms = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.upos == \"PUNCT\":\n",
    "                continue\n",
    "\n",
    "            lemma = (word.lemma or word.text or \"\").lower()\n",
    "            if not lemma:\n",
    "                continue\n",
    "\n",
    "            mtld_tokens.append(lemma)\n",
    "            total_tokens += 1\n",
    "\n",
    "            if word.upos in CONTENT_UPOS:\n",
    "                content_tokens += 1\n",
    "                form = (word.text or \"\").lower()\n",
    "                content_forms.append(form)\n",
    "\n",
    "    mtld = _compute_mtld(mtld_tokens) if mtld_tokens else None\n",
    "    ld = _compute_lexical_density(total_tokens, content_tokens)\n",
    "    ls = _compute_lexical_sophistication_cow(content_forms, cow_top5000)\n",
    "\n",
    "    return {\"MTLD\": mtld, \"LD\": ld, \"LS\": ls}\n",
    "\n",
    "\n",
    "def lexical_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute lexical measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return lexical_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute lexical measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = lexical_measures_from_text(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "\n",
    "    return {\"MTLD\": mtld_res, \"LD\": ld_res, \"LS\": ls_res}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60121915",
   "metadata": {},
   "source": [
    "### Syntactic complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "43a1952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdd_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute Mean Dependency Distance (MDD) from a stanza Document.\n",
    "\n",
    "    For each sentence s_i with dependency set D_i:\n",
    "        MDD_i = (1 / |D_i|) * sum_{(h,d) in D_i} |h - d|\n",
    "    Then:\n",
    "        MDD = (1 / k) * sum_i MDD_i, over all sentences with at least one dependency.\n",
    "    \"\"\"\n",
    "    sentence_mdds = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        distances = []\n",
    "        for w in sent.words:\n",
    "            if w.head is None or w.head == 0:\n",
    "                continue\n",
    "            distances.append(abs(w.id - w.head))\n",
    "\n",
    "        if distances:\n",
    "            sentence_mdds.append(sum(distances) / len(distances))\n",
    "\n",
    "    if not sentence_mdds:\n",
    "        return None\n",
    "    return sum(sentence_mdds) / len(sentence_mdds)\n",
    "\n",
    "\n",
    "\n",
    "def _count_clauses_in_tree(tree) -> int:\n",
    "    \"\"\"\n",
    "    Count clause nodes in a constituency tree.\n",
    "\n",
    "    A simple and standard heuristic (PTB-style) is:\n",
    "        count all nodes whose label starts with 'S'\n",
    "        (S, SBAR, SBARQ, SINV, SQ, etc.).\n",
    "\n",
    "    This aligns with the idea of counting finite and subordinate clauses\n",
    "    as in Hunt (1965) and later complexity work.\n",
    "    \"\"\"\n",
    "    if tree is None:\n",
    "        return 0\n",
    "\n",
    "    # Stanza's constituency tree: tree.label, tree.children\n",
    "    count = 1 if getattr(tree, \"label\", \"\").startswith(\"S\") else 0\n",
    "\n",
    "    for child in getattr(tree, \"children\", []):\n",
    "        # leaves can be strings or terminals without 'label'\n",
    "        if hasattr(child, \"label\"):\n",
    "            count += _count_clauses_in_tree(child)\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def cs_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute CS (clauses per sentence) from a stanza Document.\n",
    "\n",
    "        CS = (1 / k) * sum_i L_i\n",
    "\n",
    "    where L_i is the number of clauses in sentence s_i, estimated by counting\n",
    "    all constituents whose label starts with 'S' in the constituency tree of s_i.\n",
    "    \"\"\"\n",
    "    clause_counts = []\n",
    "    for sent in doc.sentences:\n",
    "        tree = getattr(sent, \"constituency\", None)\n",
    "        if tree is None:\n",
    "            # No constituency tree available for this sentence\n",
    "            continue\n",
    "        num_clauses = _count_clauses_in_tree(tree)\n",
    "        clause_counts.append(num_clauses)\n",
    "\n",
    "    if not clause_counts:\n",
    "        return None\n",
    "\n",
    "    return sum(clause_counts) / len(clause_counts)\n",
    "\n",
    "\n",
    "\n",
    "def syntactic_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MDD and CS from a stanza Document.\n",
    "    \"\"\"\n",
    "    mdd = mdd_from_doc(doc)\n",
    "    cs = cs_from_doc(doc)\n",
    "    return {\"MDD\": mdd, \"CS\": cs}\n",
    "\n",
    "\n",
    "def syntactic_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute syntactic measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return syntactic_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "def compute_syntactic_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute syntactic measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MDD\": {index: value},\n",
    "            \"CS\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = syntactic_measures_from_text(text, lang=lang)\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "    return {\"MDD\": mdd_res, \"CS\": cs_res}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9abca82",
   "metadata": {},
   "source": [
    "### Discourse complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e2f2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate set of content POS tags (spaCy universal POS)\n",
    "CONTENT_POS =  {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "def is_content_token(tok):\n",
    "    \"\"\"\n",
    "    Return True if token is considered a content word.\n",
    "    We ignore stopwords, punctuation, and non-alphabetic tokens.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        tok.is_alpha\n",
    "        and not tok.is_stop\n",
    "        and tok.pos_ in CONTENT_POS\n",
    "    )\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=100000)\n",
    "def get_related_lemmas(lemma):\n",
    "    \"\"\"\n",
    "    Return a set of semantically related lemmas for the given lemma\n",
    "    using WordNet, including:\n",
    "      - synonyms\n",
    "      - antonyms\n",
    "      - hypernyms / hyponyms\n",
    "      - meronyms (part/member/substance)\n",
    "      - coordinate terms (siblings under the same hypernym)\n",
    "\n",
    "    NOTE: Some older examples mention 'troponyms', but in NLTK's\n",
    "    WordNet interface there is no 'troponyms()' method on Synset,\n",
    "    so we do NOT use it here.\n",
    "    \"\"\"\n",
    "    lemma = lemma.lower()\n",
    "    related = set()\n",
    "    synsets = wn.synsets(lemma)\n",
    "\n",
    "    for syn in synsets:\n",
    "        # Synonyms and antonyms\n",
    "        for l in syn.lemmas():\n",
    "            related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "            for ant in l.antonyms():\n",
    "                related.add(ant.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Hypernyms (more general) and hyponyms (more specific)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for l in hyper.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "        for hypo in syn.hyponyms():\n",
    "            for l in hypo.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Meronyms: part/member/substance\n",
    "        for mer in syn.part_meronyms() + syn.member_meronyms() + syn.substance_meronyms():\n",
    "            for l in mer.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Coordinate terms (siblings under same hypernym)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for sibling in hyper.hyponyms():\n",
    "                if sibling == syn:\n",
    "                    continue\n",
    "                for l in sibling.lemmas():\n",
    "                    related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "    # Remove the lemma itself if present\n",
    "    related.discard(lemma)\n",
    "    return related\n",
    "\n",
    "\n",
    "def lexical_cohesion_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Lexical Cohesion (LC) for a single document:\n",
    "\n",
    "        LC = |C| / m\n",
    "\n",
    "    where:\n",
    "      - |C| is the number of cohesive devices between sentences\n",
    "        (lexical repetition + semantic relations),\n",
    "      - m  is the total number of word tokens (alphabetic) in the document.\n",
    "\n",
    "    If the document has fewer than 2 sentences or no valid words,\n",
    "    LC is returned as 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Total number of alphabetic tokens (denominator m)\n",
    "    m = sum(1 for tok in doc if tok.is_alpha)\n",
    "    if m == 0:\n",
    "        return 0.0\n",
    "\n",
    "    sentences = list(doc.sents)\n",
    "    if len(sentences) < 2:\n",
    "        # With only one sentence, cross-sentence cohesion is not defined\n",
    "        return 0.0\n",
    "\n",
    "    # Collect sets of content lemmas per sentence\n",
    "    sent_lemmas = []\n",
    "    for sent in sentences:\n",
    "        lemmas = set(\n",
    "            tok.lemma_.lower()\n",
    "            for tok in sent\n",
    "            if is_content_token(tok)\n",
    "        )\n",
    "        if lemmas:\n",
    "            sent_lemmas.append(lemmas)\n",
    "\n",
    "    if len(sent_lemmas) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    cohesive_count = 0\n",
    "\n",
    "    for i in range(len(sent_lemmas) - 1):\n",
    "        for j in range(i + 1, len(sent_lemmas)):\n",
    "            li = sent_lemmas[i]\n",
    "            lj = sent_lemmas[j]\n",
    "\n",
    "            # 1) Lexical repetition: shared lemmas\n",
    "            shared = li & lj\n",
    "            cohesive_count += len(shared)\n",
    "\n",
    "            # 2) Semantic relations via WordNet\n",
    "            for lemma in li:\n",
    "                related = get_related_lemmas(lemma)\n",
    "                cohesive_count += len(related & lj)\n",
    "\n",
    "    return float(cohesive_count) / float(m)\n",
    "\n",
    "\n",
    "def sentence_vector(sent, vector_size):\n",
    "    \"\"\"\n",
    "    Represent a sentence as the average of token vectors.\n",
    "    If no token has a vector, return a zero vector.\n",
    "    \"\"\"\n",
    "    vecs = [\n",
    "        tok.vector\n",
    "        for tok in sent\n",
    "        if tok.has_vector and not tok.is_punct and not tok.is_space\n",
    "    ]\n",
    "    if not vecs:\n",
    "        return np.zeros(vector_size, dtype=\"float32\")\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "def coherence_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Coherence (CoH) for a single document as the average\n",
    "    cosine similarity between adjacent sentence vectors:\n",
    "\n",
    "        CoH = (1 / (k-1)) * sum_{i=1}^{k-1} cos(h_i, h_{i+1})\n",
    "\n",
    "    where h_i is the sentence/topic vector for sentence i.\n",
    "\n",
    "    If the document has fewer than 2 sentences, CoH = 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    if nlp.vocab.vectors_length == 0:\n",
    "        raise ValueError(\n",
    "            \"The loaded spaCy model does not contain word vectors \"\n",
    "            \"(nlp.vocab.vectors_length == 0). \"\n",
    "            \"Use a model like 'en_core_web_md' or similar.\"\n",
    "        )\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    k = len(sentences)\n",
    "\n",
    "    if k < 2:\n",
    "        # Only one sentence: no adjacent pair, coherence = 0.0\n",
    "        return 0.0\n",
    "\n",
    "    vector_size = nlp.vocab.vectors_length\n",
    "    sent_vectors = [\n",
    "        sentence_vector(sent, vector_size)\n",
    "        for sent in sentences\n",
    "    ]\n",
    "\n",
    "    sims = []\n",
    "    for i in range(k - 1):\n",
    "        v1 = sent_vectors[i]\n",
    "        v2 = sent_vectors[i + 1]\n",
    "        norm1 = np.linalg.norm(v1)\n",
    "        norm2 = np.linalg.norm(v2)\n",
    "        denom = norm1 * norm2\n",
    "        if denom == 0.0:\n",
    "            # Skip pairs where at least one sentence vector is zero\n",
    "            continue\n",
    "        cos_sim = float(np.dot(v1, v2) / denom)\n",
    "        sims.append(cos_sim)\n",
    "\n",
    "    if not sims:\n",
    "        return 0.0\n",
    "\n",
    "    return float(np.mean(sims))\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_cohesion_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute LC for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with lemmatizer, POS tagger, etc.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of LC scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [lexical_cohesion_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_coherence_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute CoH for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with word vectors.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of CoH scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [coherence_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_discourse_measures(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute both LC and CoH for each row of a DataFrame and return\n",
    "    them in a dictionary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"LC\":  np.ndarray of lexical cohesion scores,\n",
    "            \"CoH\": np.ndarray of coherence scores\n",
    "        }\n",
    "    \"\"\"\n",
    "    lc_vec = compute_lexical_cohesion_vector(df, nlp, column=column)\n",
    "    coh_vec = compute_coherence_vector(df, nlp, column=column)\n",
    "    return {\"LC\": lc_vec, \"CoH\": coh_vec}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57355785",
   "metadata": {},
   "source": [
    "### Text complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fee2fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze_text_all(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Parse a text with stanza and compute all measures (lexical + syntactic)\n",
    "    in a single pass.\n",
    "\n",
    "    Returns a dict with keys:\n",
    "        \"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\"\n",
    "    (Discourse measures LC/CoH are added later at DataFrame level, via spaCy.)\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None, \"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    lex = lexical_measures_from_doc(doc)\n",
    "    syn = syntactic_measures_from_doc(doc)\n",
    "\n",
    "    out: Dict[str, Optional[float]] = {}\n",
    "    out.update(lex)\n",
    "    out.update(syn)\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_all_complexity_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    "    spacy_nlp=None,\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute all complexity measures for each row in df[column].\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with a text column.\n",
    "    column : str, default \"text\"\n",
    "        Name of the text column.\n",
    "    lang : str, default \"en\"\n",
    "        Language code for stanza.\n",
    "    n_jobs : int, default 1\n",
    "        Number of worker processes to use.\n",
    "            - 1  : sequential execution (no multiprocessing).\n",
    "            - >1 : multiprocessing with that many workers.\n",
    "            - 0 or None : use cpu_count() workers.\n",
    "    spacy_nlp : spaCy Language, required for LC / CoH\n",
    "        Pre-loaded spaCy pipeline with:\n",
    "            - POS / lemmatizer for LC\n",
    "            - word vectors for CoH (e.g. 'en_core_web_md').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "            \"MDD\":  {index: value},\n",
    "            \"CS\":   {index: value},\n",
    "            \"LC\":   {index: value},\n",
    "            \"CoH\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    items = list(df[column].items())  # list[(index, text)]\n",
    "    total_items = len(items)\n",
    "\n",
    "    # ---- Lexical + syntactic (stanza) ----\n",
    "    for idx, text in tqdm(\n",
    "        items,\n",
    "        total=total_items,\n",
    "        desc=\"Computing lexical & syntactic complexity (sequential)\",\n",
    "    ):\n",
    "        metrics = _analyze_text_all(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "\n",
    "    # ---- Discourse measures (spaCy: LC & CoH) ----\n",
    "    if spacy_nlp is None:\n",
    "        raise ValueError(\n",
    "            \"spacy_nlp must be provided to compute LC and CoH. \"\n",
    "            \"Load a spaCy model with vectors, e.g. 'en_core_web_md', and \"\n",
    "            \"pass it as spacy_nlp=...\"\n",
    "        )\n",
    "\n",
    "    discourse = compute_discourse_measures(df, spacy_nlp, column=column)\n",
    "    lc_vec = discourse[\"LC\"]\n",
    "    coh_vec = discourse[\"CoH\"]\n",
    "\n",
    "    lc_res: Dict[Any, float] = {}\n",
    "    coh_res: Dict[Any, float] = {}\n",
    "\n",
    "    # Map arrays back to DataFrame indices\n",
    "    for i, idx in enumerate(df.index):\n",
    "        lc_res[idx] = float(lc_vec[i])\n",
    "        coh_res[idx] = float(coh_vec[i])\n",
    "\n",
    "    return {\n",
    "        \"MTLD\": mtld_res,\n",
    "        \"LD\": ld_res,\n",
    "        \"LS\": ls_res,\n",
    "        \"MDD\": mdd_res,\n",
    "        \"CS\": cs_res,\n",
    "        \"LC\": lc_res,\n",
    "        \"CoH\": coh_res,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11af2ab",
   "metadata": {},
   "source": [
    "## Calcul métriques Simple/Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63769d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_both_sides_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    col_simple: str = COL_SIMPLE,\n",
    "    col_complex: str = COL_COMPLEX,\n",
    "    lang: str = LANG,\n",
    "    spacy_nlp=None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule les 7 métriques de complexité pour chaque ligne, pour Simple et Complex,\n",
    "    en réutilisant strictement compute_all_complexity_measures_df (test.ipynb).\n",
    "\n",
    "    Retour : DataFrame original + 14 colonnes :\n",
    "      - simple_MTLD ... simple_CoH\n",
    "      - complex_MTLD ... complex_CoH\n",
    "    \"\"\"\n",
    "    required = {col_simple, col_complex}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}. Colonnes présentes: {list(df.columns)}\")\n",
    "\n",
    "    if spacy_nlp is None:\n",
    "        raise ValueError(\"spacy_nlp doit être fourni (modèle avec vecteurs)\")\n",
    "\n",
    "    # Mesures Simple\n",
    "    simple = compute_all_complexity_measures_df(df, column=col_simple, lang=lang, spacy_nlp=spacy_nlp)\n",
    "    # Mesures Complex\n",
    "    complex_ = compute_all_complexity_measures_df(df, column=col_complex, lang=lang, spacy_nlp=spacy_nlp)\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # On aligne explicitement sur l'index du DataFrame\n",
    "    for m in MEASURES:\n",
    "        out[f\"simple_{m}\"] = pd.Series(simple[m]).reindex(out.index)\n",
    "        out[f\"complex_{m}\"] = pd.Series(complex_[m]).reindex(out.index)\n",
    "\n",
    "    # Conversion numérique (sécurité)\n",
    "    metric_cols = [f\"simple_{m}\" for m in MEASURES] + [f\"complex_{m}\" for m in MEASURES]\n",
    "    for c in metric_cols:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e42537",
   "metadata": {},
   "source": [
    "## Ajout orig_id (numéro de ligne d’origine, 0-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e41759fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_orig_id(df: pd.DataFrame, id_col: str = ID_COL) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ajoute une colonne identifiant stable 'orig_id' basée sur le numéro de ligne d'origine (0-based).\n",
    "    Si la colonne existe déjà, on ne la modifie pas.\n",
    "    \"\"\"\n",
    "    if id_col in df.columns:\n",
    "        return df\n",
    "    df2 = df.reset_index(drop=True).copy()\n",
    "    df2.insert(0, id_col, df2.index.astype(int))\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e59b8a",
   "metadata": {},
   "source": [
    "## Filtrage NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "11abd1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_with_any_nan_metrics(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Supprime toute ligne pour laquelle au moins une des 14 métriques (Simple/Complex) est NaN.\n",
    "    Retourne :\n",
    "      - df_kept_nan : lignes conservées\n",
    "      - df_removed_nan : lignes supprimées à cause de NaN\n",
    "    \"\"\"\n",
    "    metric_cols = [f\"simple_{m}\" for m in MEASURES] + [f\"complex_{m}\" for m in MEASURES]\n",
    "    mask_ok = df[metric_cols].notna().all(axis=1)\n",
    "    return df[mask_ok].copy(), df[~mask_ok].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca6486",
   "metadata": {},
   "source": [
    "## Dominance (Simple > Complex sur ≥ MIN_METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be549f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominance_flags(df: pd.DataFrame, min_metrics: int = MIN_METRICS) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Une ligne est en dominance si Simple est strictement > Complex sur au moins `min_metrics` métriques.\n",
    "    \n",
    "    \"\"\"\n",
    "    wins = []\n",
    "    for m in MEASURES:\n",
    "        s = df[f\"simple_{m}\"]\n",
    "        c = df[f\"complex_{m}\"]\n",
    "        wins.append(s > c)\n",
    "\n",
    "    wins_count = pd.concat(wins, axis=1).sum(axis=1)\n",
    "    return wins_count >= min_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f058e4",
   "metadata": {},
   "source": [
    "## Export CSV (All / Kept / Removed / Removed IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ce55bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_csv_bundle(dataset_name: str, df_all: pd.DataFrame, df_kept: pd.DataFrame, df_removed: pd.DataFrame) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte les résultats en CSV (UTF-8), selon le format attendu.\n",
    "    \"\"\"\n",
    "    all_path = OUT_DIR / f\"{dataset_name}_with_metrics_all.csv\"\n",
    "    kept_path = OUT_DIR / f\"{dataset_name}_with_metrics_kept.csv\"\n",
    "    removed_path = OUT_DIR / f\"{dataset_name}_with_metrics_removed.csv\"\n",
    "    removed_ids_path = OUT_DIR / f\"{dataset_name}_removed_ids.csv\"\n",
    "\n",
    "    df_all.to_csv(all_path, index=False, encoding=\"utf-8\")\n",
    "    df_kept.to_csv(kept_path, index=False, encoding=\"utf-8\")\n",
    "    df_removed.to_csv(removed_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    pd.DataFrame({ID_COL: df_removed[ID_COL].astype(int).sort_values()}).to_csv(\n",
    "        removed_ids_path, index=False, encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"all\": str(all_path),\n",
    "        \"kept\": str(kept_path),\n",
    "        \"removed\": str(removed_path),\n",
    "        \"removed_ids\": str(removed_ids_path),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604e71e",
   "metadata": {},
   "source": [
    "## Pipeline dataset (16 colonnes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7653d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_offline(dataset_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Prépare un dataset :\n",
    "      1) charge les données via load_dataset (test.ipynb)\n",
    "      2) ajoute orig_id (numéro de ligne d'origine)\n",
    "      3) calcule les métriques offline (Simple & Complex)\n",
    "      4) supprime les lignes avec NaN sur une métrique\n",
    "      5) supprime les lignes en dominance\n",
    "      6) exporte en CSV\n",
    "\n",
    "    Sortie finale (df_kept) : 16 colonnes minimales :\n",
    "      orig_id, Simple, Complex, 7 métriques Simple, 7 métriques Complex\n",
    "    \"\"\"\n",
    "    df_raw = load_dataset(dataset_name)\n",
    "    df_raw = add_orig_id(df_raw, id_col=ID_COL)\n",
    "\n",
    "    # On conserve uniquement les colonnes texte + id\n",
    "    df_base = df_raw[[ID_COL, COL_SIMPLE, COL_COMPLEX]].copy()\n",
    "\n",
    "    # Calcul des métriques\n",
    "    df_all = compute_both_sides_metrics(df_base, spacy_nlp=spacy_nlp)\n",
    "\n",
    "    # Marquage NaN\n",
    "    df_no_nan, df_removed_nan = drop_rows_with_any_nan_metrics(df_all)\n",
    "    df_no_nan[\"removed_reason\"] = \"\"  # juste pour cohérence si besoin\n",
    "\n",
    "    # Dominance\n",
    "    dom = dominance_flags(df_no_nan, min_metrics=MIN_METRICS)\n",
    "    df_no_nan[\"simple_dominates_complex\"] = dom\n",
    "\n",
    "    df_removed_dom = df_no_nan[df_no_nan[\"simple_dominates_complex\"]].copy()\n",
    "    df_kept = df_no_nan[~df_no_nan[\"simple_dominates_complex\"]].copy()\n",
    "\n",
    "    # Bundle removed = NaN + dominance \n",
    "    df_removed_nan = df_removed_nan.copy()\n",
    "    if not df_removed_nan.empty:\n",
    "        df_removed_nan[\"simple_dominates_complex\"] = False\n",
    "        df_removed_nan[\"removed_reason\"] = \"nan_metrics\"\n",
    "\n",
    "    if not df_removed_dom.empty:\n",
    "        df_removed_dom[\"removed_reason\"] = \"dominance\"\n",
    "\n",
    "    df_removed = pd.concat([df_removed_nan, df_removed_dom], axis=0).sort_values(ID_COL)\n",
    "\n",
    "    # Création rendu metric\n",
    "    metric_cols_simple = [f\"simple_{m}\" for m in MEASURES]\n",
    "    metric_cols_complex = [f\"complex_{m}\" for m in MEASURES]\n",
    "    cols_16 = [ID_COL, COL_SIMPLE, COL_COMPLEX] + metric_cols_simple + metric_cols_complex\n",
    "\n",
    "    df_all_export = df_all[cols_16].copy()\n",
    "    df_kept_export = df_kept[cols_16].copy()\n",
    "    df_removed_export = df_removed[cols_16 + [\"removed_reason\"]].copy() if \"removed_reason\" in df_removed.columns else df_removed[cols_16].copy()\n",
    "\n",
    "    paths = export_csv_bundle(dataset_name, df_all_export, df_kept_export, df_removed_export)\n",
    "\n",
    "    return {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"rows_all\": int(df_all_export.shape[0]),\n",
    "        \"rows_kept\": int(df_kept_export.shape[0]),\n",
    "        \"rows_removed\": int(df_removed_export.shape[0]),\n",
    "        \"paths\": paths,\n",
    "        \"df_all\": df_all_export,\n",
    "        \"df_kept\": df_kept_export,\n",
    "        \"df_removed\": df_removed_export,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16cb9e4",
   "metadata": {},
   "source": [
    "## Exécution sur tous les datasets + aperçu DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9244597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preparing: ose_adv_ele ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing lexical & syntactic complexity (sequential):   0%|          | 0/189 [00:00<?, ?it/s]2025-12-18 23:29:32 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 7.35MB/s]                    \n",
      "2025-12-18 23:29:33 INFO: Downloaded file to C:\\Users\\rroll\\stanza_resources\\resources.json\n",
      "2025-12-18 23:29:33 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-12-18 23:29:33 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "======================================\n",
      "\n",
      "2025-12-18 23:29:33 INFO: Using device: cpu\n",
      "2025-12-18 23:29:33 INFO: Loading: tokenize\n",
      "2025-12-18 23:29:33 INFO: Loading: mwt\n",
      "2025-12-18 23:29:33 INFO: Loading: pos\n",
      "2025-12-18 23:29:34 INFO: Loading: lemma\n",
      "2025-12-18 23:29:35 INFO: Loading: constituency\n",
      "2025-12-18 23:29:35 INFO: Loading: depparse\n",
      "2025-12-18 23:29:35 INFO: Done loading processors!\n",
      "Computing lexical & syntactic complexity (sequential):   2%|▏         | 3/189 [00:14<15:20,  4.95s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m datasets.keys():\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Preparing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     info = \u001b[43mprepare_dataset_offline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     results[name] = info\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAll: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo[\u001b[33m'\u001b[39m\u001b[33mrows_all\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Kept: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo[\u001b[33m'\u001b[39m\u001b[33mrows_kept\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Removed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo[\u001b[33m'\u001b[39m\u001b[33mrows_removed\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mprepare_dataset_offline\u001b[39m\u001b[34m(dataset_name)\u001b[39m\n\u001b[32m     18\u001b[39m df_base = df_raw[[ID_COL, COL_SIMPLE, COL_COMPLEX]].copy()\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Calcul des métriques\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m df_all = \u001b[43mcompute_both_sides_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspacy_nlp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspacy_nlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Marquage NaN\u001b[39;00m\n\u001b[32m     24\u001b[39m df_no_nan, df_removed_nan = drop_rows_with_any_nan_metrics(df_all)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mcompute_both_sides_metrics\u001b[39m\u001b[34m(df, col_simple, col_complex, lang, spacy_nlp)\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mspacy_nlp doit être fourni (modèle avec vecteurs)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Mesures Simple\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m simple = \u001b[43mcompute_all_complexity_measures_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcol_simple\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspacy_nlp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspacy_nlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Mesures Complex\u001b[39;00m\n\u001b[32m     27\u001b[39m complex_ = compute_all_complexity_measures_df(df, column=col_complex, lang=lang, spacy_nlp=spacy_nlp)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mcompute_all_complexity_measures_df\u001b[39m\u001b[34m(df, column, lang, spacy_nlp)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# ---- Lexical + syntactic (stanza) ----\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, text \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[32m     80\u001b[39m     items,\n\u001b[32m     81\u001b[39m     total=total_items,\n\u001b[32m     82\u001b[39m     desc=\u001b[33m\"\u001b[39m\u001b[33mComputing lexical & syntactic complexity (sequential)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     83\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     metrics = \u001b[43m_analyze_text_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     mtld_res[idx] = metrics[\u001b[33m\"\u001b[39m\u001b[33mMTLD\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     86\u001b[39m     ld_res[idx] = metrics[\u001b[33m\"\u001b[39m\u001b[33mLD\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36m_analyze_text_all\u001b[39m\u001b[34m(text, lang)\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mMTLD\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mLD\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mLS\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mMDD\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mCS\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[32m     17\u001b[39m nlp = get_stanza_pipeline(lang)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m doc = \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m lex = lexical_measures_from_doc(doc)\n\u001b[32m     21\u001b[39m syn = syntactic_measures_from_doc(doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\pipeline\\core.py:480\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, doc, processors)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\pipeline\\core.py:431\u001b[39m, in \u001b[36mPipeline.process\u001b[39m\u001b[34m(self, doc, processors)\u001b[39m\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processors.get(processor_name):\n\u001b[32m    430\u001b[39m         process = \u001b[38;5;28mself\u001b[39m.processors[processor_name].bulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processors[processor_name].process\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m         doc = \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\pipeline\\constituency_processor.py:69\u001b[39m, in \u001b[36mConstituencyProcessor.process\u001b[39m\u001b[34m(self, document)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tqdm:\n\u001b[32m     67\u001b[39m     words = tqdm(words)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m trees = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_tagged_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m trees = unsort(trees, original_indices)\n\u001b[32m     71\u001b[39m document.set(CONSTITUENCY, trees, to_sentence=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\models\\constituency\\base_model.py:399\u001b[39m, in \u001b[36mBaseModel.parse_tagged_words\u001b[39m\u001b[34m(self, words, batch_size)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28mself\u001b[39m.eval()\n\u001b[32m    398\u001b[39m sentence_iterator = \u001b[38;5;28miter\u001b[39m(words)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m treebank = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_sentences_no_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_batch_from_tagged_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_constituents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m results = [t.predictions[\u001b[32m0\u001b[39m].tree \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m treebank]\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\models\\constituency\\base_model.py:364\u001b[39m, in \u001b[36mBaseModel.parse_sentences_no_grad\u001b[39m\u001b[34m(self, data_iterator, build_batch_fn, batch_size, transition_choice, keep_state, keep_constituents, keep_scores)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    358\u001b[39m \u001b[33;03mGiven an iterator over the data and a method for building batches, returns a list of parse trees.\u001b[39;00m\n\u001b[32m    359\u001b[39m \n\u001b[32m    360\u001b[39m \u001b[33;03mno_grad() is so that gradients aren't kept, which makes the model\u001b[39;00m\n\u001b[32m    361\u001b[39m \u001b[33;03mrun faster and use less memory at inference time\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuild_batch_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransition_choice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_constituents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_scores\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\models\\constituency\\base_model.py:306\u001b[39m, in \u001b[36mBaseModel.parse_sentences\u001b[39m\u001b[34m(self, data_iterator, build_batch_fn, batch_size, transition_choice, keep_state, keep_constituents, keep_scores)\u001b[39m\n\u001b[32m    303\u001b[39m     constituents = defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state_batch) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     pred_scores, transitions, scores = \u001b[43mtransition_choice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m keep_scores \u001b[38;5;129;01mand\u001b[39;00m scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    308\u001b[39m         state_batch = [state._replace(score=state.score + score) \u001b[38;5;28;01mfor\u001b[39;00m state, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(state_batch, scores)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\models\\constituency\\lstm_model.py:1152\u001b[39m, in \u001b[36mLSTMModel.predict\u001b[39m\u001b[34m(self, states, is_legal)\u001b[39m\n\u001b[32m   1139\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, states, is_legal=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1140\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1141\u001b[39m \u001b[33;03m    Generate and return predictions, along with the transitions those predictions represent\u001b[39;00m\n\u001b[32m   1142\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1150\u001b[39m \u001b[33;03m      tensor(batch_size) - the final output specifically for the chosen transition\u001b[39;00m\n\u001b[32m   1151\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1152\u001b[39m     predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1153\u001b[39m     pred_max = torch.argmax(predictions, dim=\u001b[32m1\u001b[39m)\n\u001b[32m   1154\u001b[39m     scores = torch.take_along_dim(predictions, pred_max.unsqueeze(\u001b[32m1\u001b[39m), dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stanza\\models\\constituency\\lstm_model.py:1136\u001b[39m, in \u001b[36mLSTMModel.forward\u001b[39m\u001b[34m(self, states)\u001b[39m\n\u001b[32m   1134\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maxout_k \u001b[38;5;129;01mand\u001b[39;00m idx < \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.output_layers) - \u001b[32m1\u001b[39m:\n\u001b[32m   1135\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.nonlinearity(hx)\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     hx = \u001b[43moutput_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hx\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name in datasets.keys():\n",
    "    print(f\"\\n=== Preparing: {name} ===\")\n",
    "    info = prepare_dataset_offline(name)\n",
    "    results[name] = info\n",
    "    print(f\"All: {info['rows_all']} | Kept: {info['rows_kept']} | Removed: {info['rows_removed']}\")\n",
    "    print(\"Saved to:\", info[\"paths\"])\n",
    "\n",
    "# Aperçu du dataset préparé (kept)\n",
    "example_name = next(iter(results.keys()))\n",
    "df_prepared = results[example_name][\"df_kept\"]\n",
    "df_prepared.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
