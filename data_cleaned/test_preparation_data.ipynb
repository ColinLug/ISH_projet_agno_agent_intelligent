{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe89001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 7.34MB/s]                    \n",
      "2025-12-19 09:04:42 INFO: Downloaded file to C:\\Users\\rroll\\stanza_resources\\resources.json\n",
      "2025-12-19 09:04:42 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-12-19 09:04:43 INFO: File exists: C:\\Users\\rroll\\stanza_resources\\en\\default.zip\n",
      "2025-12-19 09:04:46 INFO: Finished downloading models and saved to C:\\Users\\rroll\\stanza_resources\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rroll\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rroll\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x17e2e8d55d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "from pprint import pprint\n",
    "from typing import Dict, Set, Iterable, Optional, Any, Tuple\n",
    "import importlib.resources as pkg_resources\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "import stanza\n",
    "import textcomplexity  # only used to access en.json\n",
    "from tqdm.auto import tqdm  \n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Download required resources\n",
    "stanza.download('en')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Make sure WordNet is available; if not, download it.\n",
    "try:\n",
    "    _ = wn.synsets(\"dog\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "spacy_nlp = nlp\n",
    "spacy_nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed542395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Colonnes attendues\n",
    "COL_SIMPLE = \"Simple\"\n",
    "COL_COMPLEX = \"Complex\"\n",
    "ID_COL = \"orig_id\"\n",
    "\n",
    "# Sorties\n",
    "OUT_DIR = Path(\"data_prepared\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Langue / modèles\n",
    "LANG = \"en\"\n",
    "SPACY_MODEL = \"en_core_web_md\"\n",
    "\n",
    "# Dominance : Simple > Complex sur au moins X métriques\n",
    "MIN_METRICS = 5\n",
    "\n",
    "# Liste officielle des métriques (test.ipynb)\n",
    "MEASURES = [\"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\", \"LC\", \"CoH\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e044f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement spaCy (doit contenir des vecteurs pour CoH)\n",
    "nlp = spacy.load(SPACY_MODEL)\n",
    "spacy_nlp = nlp\n",
    "\n",
    "# Important : segmentation en phrases\n",
    "if \"sentencizer\" not in spacy_nlp.pipe_names:\n",
    "    spacy_nlp.add_pipe(\"sentencizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4f785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire des datasets\n",
    "datasets ={'ose_adv_ele':'data_sampled/OSE_adv_ele.csv', \n",
    "           'ose_adv_int':'data_sampled/OSE_adv_int.csv',\n",
    "           'swipe': 'data_sampled/swipe.csv',\n",
    "           'vikidia':'data_sampled/vikidia.csv'}\n",
    "\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path, sep='\\t')\n",
    "    \n",
    "\n",
    "def load_dataset(name):\n",
    "    if name not in datasets:\n",
    "        raise ValueError(f\"Dataset {name} not found\")\n",
    "    return load_data(datasets[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e76a962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Simple</th>\n",
       "      <th>Complex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿To tourists, Amsterdam still seems very liber...</td>\n",
       "      <td>﻿Amsterdam still looks liberal to tourists, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>﻿Anitta, a music star from Brazil, has million...</td>\n",
       "      <td>﻿Brazil’s latest funk sensation, Anitta, has w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Simple  \\\n",
       "0  ﻿When you see the word Amazon, what’s the firs...   \n",
       "1  ﻿To tourists, Amsterdam still seems very liber...   \n",
       "2  ﻿Anitta, a music star from Brazil, has million...   \n",
       "\n",
       "                                             Complex  \n",
       "0  ﻿When you see the word Amazon, what’s the firs...  \n",
       "1  ﻿Amsterdam still looks liberal to tourists, wh...  \n",
       "2  ﻿Brazil’s latest funk sensation, Anitta, has w...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('ose_adv_ele')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "127c2dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE TEXT\n",
      "﻿Scarlett Johansson is suing a French novelist for €50,000. She says that he wrote things about her personal life that are not true.\n",
      "La premiere chose qu’on regarde (The First Thing We Look At) by Grégoire Delacourt is the story of a French model who looks so similar to the American actor that the book’s main male character thinks she is Johansson. In the novel, the model’s beauty means that men see her only as a sex object and women are jealous of her. She has many adventures as Johansson and, in the end, dies in a car crash.\n",
      "Johansson does not feel flattered by the best- seller. Her lawyer, Vincent Toledano, told Le Figaro that Delacourt ’s novel illegally used Ms Johansson’s name. He has now gone to court because Johansson does not want the book to be translated or to become a film.\n",
      "Delacourt said that he chose to mention  Johansson because she is famous for her beauty. He said: “I wrote a work of fiction. My character is not Scarlett Johansson. ”\n",
      "On French radio, the author recently said the legal action was “sad.”\n",
      "Delacourt is one of France’s best-loved authors; his last novel, My List of Desires, was translated into 47 languages and they are making a film of it. But he said he was “speechless” when he found out Johansson was suing him.\n",
      "“I thought she would ask me to go for a coffee with her. I didn’t write a novel about a celebrity,” he said. “I wrote a real love story about women’s beauty, especially interior beauty.\n",
      "“If an author can no longer write about the things that surround us – a brand of beer, a monument,  an actor – it’s going to be difficult to write fiction.\n",
      "“I’m not sure she’s read the novel because it hasn’t been translated yet.”\n",
      "Emmanuelle Allibert, spokeswoman for publisher JC Lattès, said taking legal action was “crazy”. “We have never known anything like it. It is very surprising because the novel is not even about Scarlett Johansson. It is about a woman who is Scarlett Johansson’s double.”\n",
      "The author ’s legal situation would be easier if he had published the book in the USA and not in France. Lloyd Jassin, a New York lawyer, said that the case would probably not go to court in the United States.\n",
      "“I thought she might send me flowers because the book was a declaration of love for her, but she didn’t understand,” Delacourt said.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "COMPLEX TEXT\n",
      "﻿Scarlett Johansson is suing a French novelist for €50,000 in damages, alleging that his work of fiction makes fraudulent claims about her personal life.\n",
      "La Premiere chose qu’on regarde (The First Thing We Look At) by Grégoire Delacourt tells the story of a French model who looks so similar to the American actor that the book’s lead male character thinks it is Johansson herself. In the novel, the model’s looks mean that men see her only as a sex object, while women are jealous of her. She has a series of adventures as Johansson until she is eventually found out and, in the end, dies in a car crash.\n",
      "Johansson herself is not flattered by the best- selling literary work. Her lawyer, Vincent Toledano, told Le Figaro that Delacourt’s novel constituted a “violation and fraudulent and illegal exploitation of her name, her reputation and her image.” He said the novel contains “defamatory claims about her private life” and has now gone to court to try to stop the book being translated or adapted for cinema. The court case began in Paris on Wednesday afternoon, though neither Johansson nor Delacourt was present.\n",
      "“The freedom of expression that she defends as an artist is not in question,” Toledano said. “Such activities for purely mercantile ends have nothing to do with creativity.”\n",
      "Delacourt has tried explaining that he chose to reference Johansson because she is “the archetype of beauty today.” He said: “I wrote a work of fiction. My character is not Scarlett Johansson.”\n",
      "The author recently hit out against the actor on French radio, saying the legal action was “rather sad”. He said: “It freaks me out to think that, when you talk of a character in a novel, judges can get involved.”\n",
      "Delacourt has become one of France’s best-loved authors; his previous novel, My List of Desires, was translated into 47 languages and is now being adapted into a film. But he said he was “speechless” when he found out Johansson was suing him.\n",
      "“I thought she’d get in contact to ask me to go for a coffee with her. I didn’t write a novel about a celebrity,” he said. “I wrote a real love story and a homage to feminine beauty, especially interior beauty.\n",
      "“If an author can no longer mention the things that surround us – a brand of beer, a monument, an actor – it’s going to be complicated to produce fiction.\n",
      "“It’s stupefying, especially as I’m not sure she’s even read the novel, since it hasn’t been translated yet.”\n",
      "Emmanuelle Allibert, spokeswoman for publisher JC Lattès, said taking legal action was “crazy”. “We have never known anything like it. It is all the more surprising for the fact that the novel is not even about Scarlett Johansson. It is about a woman who is Scarlett Johansson’s double.”\n",
      "Delacourt’s lawyer, Anne Veil, who is also representing publisher JC Lattès, said the allegations were “totally scandalous”. “This is a literary, not commercial, approach. She has not been used as a product,” she said. “Grégoire Delacourt is not a paparazzo; he’s a writer!”\n",
      "Ironically, the author’s legal situation would be far easier had he published the book in Johansson’s home country, rather than France. Lloyd Jassin, a New York intellectual property lawyer, told Time that the case would be unlikely to be considered in the United States because the book would be protected by the First Amendment.\n",
      "“The First Amendment doesn’t look at most books as commercial uses or commercial propositions,” he said. “If her name or likeness is relevant, literarily, if there’s significance and literary merit to using her name between the covers, the First Amendment steps in.” However, in France, the legal position is more complicated and personality rights are taken “much more seriously,” Jassin says.\n",
      "“I thought she might send me flowers as it was a declaration of love for her, but she didn’t understand,” Delacourt said. “It’s a strange paradox – but a very American one.”\n"
     ]
    }
   ],
   "source": [
    "row = df.sample(1)\n",
    "\n",
    "print('SIMPLE TEXT')\n",
    "print(row['Simple'].iloc[0])\n",
    "print('-'*100)\n",
    "print('COMPLEX TEXT')\n",
    "print(row['Complex'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e0dfffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ose_adv_ele: 189 rows\n",
      "ose_adv_int: 189 rows\n",
      "swipe: 1233 rows\n",
      "vikidia: 1233 rows\n",
      "Total: 2844 rows\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for name, path in datasets.items():\n",
    "    df = load_dataset(name)\n",
    "    print(f\"{name}: {df.shape[0]} rows\")\n",
    "    cnt += df.shape[0]\n",
    "print(f\"Total: {cnt} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afca2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset('ose_adv_ele')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02a669",
   "metadata": {},
   "source": [
    "## Complexity measures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa4b934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache stanza pipelines to avoid re-loading models\n",
    "_STANZA_PIPELINES: Dict[str, stanza.Pipeline] = {}\n",
    "\n",
    "# UPOS tags considered content words (C)\n",
    "CONTENT_UPOS = {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def load_cow_top5000_en() -> Set[str]:\n",
    "    \"\"\"\n",
    "    Load the COW-based list of the 5,000 most frequent English content words\n",
    "    from textcomplexity's English language definition file (en.json).\n",
    "\n",
    "    We ignore POS tags and keep only lowercased word forms.\n",
    "    \"\"\"\n",
    "    with pkg_resources.files(textcomplexity).joinpath(\"en.json\").open(\n",
    "        \"r\", encoding=\"utf-8\"\n",
    "    ) as f:\n",
    "        lang_def = json.load(f)\n",
    "\n",
    "    most_common = lang_def[\"most_common\"]  # list of [word, xpos]\n",
    "    cow_top5000 = {w.lower() for w, xpos in most_common}\n",
    "    return cow_top5000\n",
    "\n",
    "\n",
    "def get_stanza_pipeline(lang: str = \"en\", use_gpu: bool = False) -> stanza.Pipeline:\n",
    "    \"\"\"\n",
    "    Get (or create) a cached stanza Pipeline for a given language.\n",
    "\n",
    "    NOTE: You must have downloaded the models beforehand, e.g.:\n",
    "        import stanza\n",
    "        stanza.download('en')\n",
    "    \"\"\"\n",
    "    if lang not in _STANZA_PIPELINES:\n",
    "        _STANZA_PIPELINES[lang] = stanza.Pipeline(\n",
    "            lang=lang,\n",
    "            processors=\"tokenize,pos,lemma,depparse,constituency\",\n",
    "            use_gpu=use_gpu,\n",
    "            tokenize_no_ssplit=False,\n",
    "        )\n",
    "    return _STANZA_PIPELINES[lang]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d67b5",
   "metadata": {},
   "source": [
    "### Lexical complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8ee6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_mtld(tokens: Iterable[str], ttr_threshold: float = 0.72) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute MTLD (Measure of Textual Lexical Diversity) for a list of tokens.\n",
    "\n",
    "    MTLD = total_number_of_tokens / number_of_factors\n",
    "\n",
    "    A factor is a contiguous segment where the running TTR stays >= threshold.\n",
    "    When the TTR drops below the threshold, we close a factor (at the previous\n",
    "    token) and start a new one. At the end, the remaining partial segment is\n",
    "    counted as a fractional factor, with weight proportional to how close the\n",
    "    final TTR is to the threshold.\n",
    "    \"\"\"\n",
    "    tokens = [tok for tok in tokens if tok]\n",
    "    if not tokens:\n",
    "        return None\n",
    "\n",
    "    types = set()\n",
    "    factor_count = 0.0\n",
    "    token_count_in_factor = 0\n",
    "\n",
    "    for tok in tokens:\n",
    "        token_count_in_factor += 1\n",
    "        types.add(tok)\n",
    "        ttr = len(types) / token_count_in_factor\n",
    "\n",
    "        if ttr < ttr_threshold:\n",
    "            factor_count += 1.0\n",
    "            types = set()\n",
    "            token_count_in_factor = 0\n",
    "\n",
    "    # final partial factor\n",
    "    if token_count_in_factor > 0:\n",
    "        final_ttr = len(types) / token_count_in_factor\n",
    "        if final_ttr < 1.0:\n",
    "            fractional = (1.0 - final_ttr) / (1.0 - ttr_threshold)\n",
    "            fractional = max(0.0, min(1.0, fractional))\n",
    "            factor_count += fractional\n",
    "\n",
    "    if factor_count == 0:\n",
    "        return None\n",
    "\n",
    "    return len(tokens) / factor_count\n",
    "\n",
    "\n",
    "\n",
    "def _compute_lexical_density(total_tokens: int, content_tokens: int) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LD = |C| / |T|\n",
    "    where:\n",
    "        |C| = number of content-word tokens\n",
    "        |T| = total number of non-punctuation tokens\n",
    "    \"\"\"\n",
    "    if total_tokens == 0:\n",
    "        return None\n",
    "    return content_tokens / total_tokens\n",
    "\n",
    "\n",
    "def _compute_lexical_sophistication_cow(\n",
    "    content_forms: Iterable[str],\n",
    "    cow_top5000: set,\n",
    ") -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LS = |{ w in C : w not in R }| / |C|\n",
    "    where:\n",
    "        C = content-word tokens (surface forms, lowercased)\n",
    "        R = COW top-5000 content word forms (lowercased)\n",
    "    \"\"\"\n",
    "    forms = [f for f in content_forms if f]\n",
    "    if not forms:\n",
    "        return None\n",
    "\n",
    "    off_list = sum(1 for f in forms if f not in cow_top5000)\n",
    "    return off_list / len(forms)\n",
    "\n",
    "\n",
    "\n",
    "def lexical_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MTLD, LD, LS from a stanza Document.\n",
    "    \"\"\"\n",
    "    cow_top5000 = load_cow_top5000_en()\n",
    "\n",
    "    mtld_tokens = []\n",
    "    total_tokens = 0\n",
    "    content_tokens = 0\n",
    "    content_forms = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.upos == \"PUNCT\":\n",
    "                continue\n",
    "\n",
    "            lemma = (word.lemma or word.text or \"\").lower()\n",
    "            if not lemma:\n",
    "                continue\n",
    "\n",
    "            mtld_tokens.append(lemma)\n",
    "            total_tokens += 1\n",
    "\n",
    "            if word.upos in CONTENT_UPOS:\n",
    "                content_tokens += 1\n",
    "                form = (word.text or \"\").lower()\n",
    "                content_forms.append(form)\n",
    "\n",
    "    mtld = _compute_mtld(mtld_tokens) if mtld_tokens else None\n",
    "    ld = _compute_lexical_density(total_tokens, content_tokens)\n",
    "    ls = _compute_lexical_sophistication_cow(content_forms, cow_top5000)\n",
    "\n",
    "    return {\"MTLD\": mtld, \"LD\": ld, \"LS\": ls}\n",
    "\n",
    "\n",
    "def lexical_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute lexical measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return lexical_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute lexical measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = lexical_measures_from_text(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "\n",
    "    return {\"MTLD\": mtld_res, \"LD\": ld_res, \"LS\": ls_res}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60121915",
   "metadata": {},
   "source": [
    "### Syntactic complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43a1952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdd_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute Mean Dependency Distance (MDD) from a stanza Document.\n",
    "\n",
    "    For each sentence s_i with dependency set D_i:\n",
    "        MDD_i = (1 / |D_i|) * sum_{(h,d) in D_i} |h - d|\n",
    "    Then:\n",
    "        MDD = (1 / k) * sum_i MDD_i, over all sentences with at least one dependency.\n",
    "    \"\"\"\n",
    "    sentence_mdds = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        distances = []\n",
    "        for w in sent.words:\n",
    "            if w.head is None or w.head == 0:\n",
    "                continue\n",
    "            distances.append(abs(w.id - w.head))\n",
    "\n",
    "        if distances:\n",
    "            sentence_mdds.append(sum(distances) / len(distances))\n",
    "\n",
    "    if not sentence_mdds:\n",
    "        return None\n",
    "    return sum(sentence_mdds) / len(sentence_mdds)\n",
    "\n",
    "\n",
    "\n",
    "def _count_clauses_in_tree(tree) -> int:\n",
    "    \"\"\"\n",
    "    Count clause nodes in a constituency tree.\n",
    "\n",
    "    A simple and standard heuristic (PTB-style) is:\n",
    "        count all nodes whose label starts with 'S'\n",
    "        (S, SBAR, SBARQ, SINV, SQ, etc.).\n",
    "\n",
    "    This aligns with the idea of counting finite and subordinate clauses\n",
    "    as in Hunt (1965) and later complexity work.\n",
    "    \"\"\"\n",
    "    if tree is None:\n",
    "        return 0\n",
    "\n",
    "    # Stanza's constituency tree: tree.label, tree.children\n",
    "    count = 1 if getattr(tree, \"label\", \"\").startswith(\"S\") else 0\n",
    "\n",
    "    for child in getattr(tree, \"children\", []):\n",
    "        # leaves can be strings or terminals without 'label'\n",
    "        if hasattr(child, \"label\"):\n",
    "            count += _count_clauses_in_tree(child)\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def cs_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute CS (clauses per sentence) from a stanza Document.\n",
    "\n",
    "        CS = (1 / k) * sum_i L_i\n",
    "\n",
    "    where L_i is the number of clauses in sentence s_i, estimated by counting\n",
    "    all constituents whose label starts with 'S' in the constituency tree of s_i.\n",
    "    \"\"\"\n",
    "    clause_counts = []\n",
    "    for sent in doc.sentences:\n",
    "        tree = getattr(sent, \"constituency\", None)\n",
    "        if tree is None:\n",
    "            # No constituency tree available for this sentence\n",
    "            continue\n",
    "        num_clauses = _count_clauses_in_tree(tree)\n",
    "        clause_counts.append(num_clauses)\n",
    "\n",
    "    if not clause_counts:\n",
    "        return None\n",
    "\n",
    "    return sum(clause_counts) / len(clause_counts)\n",
    "\n",
    "\n",
    "\n",
    "def syntactic_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MDD and CS from a stanza Document.\n",
    "    \"\"\"\n",
    "    mdd = mdd_from_doc(doc)\n",
    "    cs = cs_from_doc(doc)\n",
    "    return {\"MDD\": mdd, \"CS\": cs}\n",
    "\n",
    "\n",
    "def syntactic_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute syntactic measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return syntactic_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "def compute_syntactic_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute syntactic measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MDD\": {index: value},\n",
    "            \"CS\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = syntactic_measures_from_text(text, lang=lang)\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "    return {\"MDD\": mdd_res, \"CS\": cs_res}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9abca82",
   "metadata": {},
   "source": [
    "### Discourse complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e2f2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate set of content POS tags (spaCy universal POS)\n",
    "CONTENT_POS =  {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "def is_content_token(tok):\n",
    "    \"\"\"\n",
    "    Return True if token is considered a content word.\n",
    "    We ignore stopwords, punctuation, and non-alphabetic tokens.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        tok.is_alpha\n",
    "        and not tok.is_stop\n",
    "        and tok.pos_ in CONTENT_POS\n",
    "    )\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=100000)\n",
    "def get_related_lemmas(lemma):\n",
    "    \"\"\"\n",
    "    Return a set of semantically related lemmas for the given lemma\n",
    "    using WordNet, including:\n",
    "      - synonyms\n",
    "      - antonyms\n",
    "      - hypernyms / hyponyms\n",
    "      - meronyms (part/member/substance)\n",
    "      - coordinate terms (siblings under the same hypernym)\n",
    "\n",
    "    NOTE: Some older examples mention 'troponyms', but in NLTK's\n",
    "    WordNet interface there is no 'troponyms()' method on Synset,\n",
    "    so we do NOT use it here.\n",
    "    \"\"\"\n",
    "    lemma = lemma.lower()\n",
    "    related = set()\n",
    "    synsets = wn.synsets(lemma)\n",
    "\n",
    "    for syn in synsets:\n",
    "        # Synonyms and antonyms\n",
    "        for l in syn.lemmas():\n",
    "            related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "            for ant in l.antonyms():\n",
    "                related.add(ant.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Hypernyms (more general) and hyponyms (more specific)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for l in hyper.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "        for hypo in syn.hyponyms():\n",
    "            for l in hypo.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Meronyms: part/member/substance\n",
    "        for mer in syn.part_meronyms() + syn.member_meronyms() + syn.substance_meronyms():\n",
    "            for l in mer.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Coordinate terms (siblings under same hypernym)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for sibling in hyper.hyponyms():\n",
    "                if sibling == syn:\n",
    "                    continue\n",
    "                for l in sibling.lemmas():\n",
    "                    related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "    # Remove the lemma itself if present\n",
    "    related.discard(lemma)\n",
    "    return related\n",
    "\n",
    "\n",
    "def lexical_cohesion_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Lexical Cohesion (LC) for a single document:\n",
    "\n",
    "        LC = |C| / m\n",
    "\n",
    "    where:\n",
    "      - |C| is the number of cohesive devices between sentences\n",
    "        (lexical repetition + semantic relations),\n",
    "      - m  is the total number of word tokens (alphabetic) in the document.\n",
    "\n",
    "    If the document has fewer than 2 sentences or no valid words,\n",
    "    LC is returned as 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Total number of alphabetic tokens (denominator m)\n",
    "    m = sum(1 for tok in doc if tok.is_alpha)\n",
    "    if m == 0:\n",
    "        return 0.0\n",
    "\n",
    "    sentences = list(doc.sents)\n",
    "    if len(sentences) < 2:\n",
    "        # With only one sentence, cross-sentence cohesion is not defined\n",
    "        return 0.0\n",
    "\n",
    "    # Collect sets of content lemmas per sentence\n",
    "    sent_lemmas = []\n",
    "    for sent in sentences:\n",
    "        lemmas = set(\n",
    "            tok.lemma_.lower()\n",
    "            for tok in sent\n",
    "            if is_content_token(tok)\n",
    "        )\n",
    "        if lemmas:\n",
    "            sent_lemmas.append(lemmas)\n",
    "\n",
    "    if len(sent_lemmas) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    cohesive_count = 0\n",
    "\n",
    "    for i in range(len(sent_lemmas) - 1):\n",
    "        for j in range(i + 1, len(sent_lemmas)):\n",
    "            li = sent_lemmas[i]\n",
    "            lj = sent_lemmas[j]\n",
    "\n",
    "            # 1) Lexical repetition: shared lemmas\n",
    "            shared = li & lj\n",
    "            cohesive_count += len(shared)\n",
    "\n",
    "            # 2) Semantic relations via WordNet\n",
    "            for lemma in li:\n",
    "                related = get_related_lemmas(lemma)\n",
    "                cohesive_count += len(related & lj)\n",
    "\n",
    "    return float(cohesive_count) / float(m)\n",
    "\n",
    "\n",
    "def sentence_vector(sent, vector_size):\n",
    "    \"\"\"\n",
    "    Represent a sentence as the average of token vectors.\n",
    "    If no token has a vector, return a zero vector.\n",
    "    \"\"\"\n",
    "    vecs = [\n",
    "        tok.vector\n",
    "        for tok in sent\n",
    "        if tok.has_vector and not tok.is_punct and not tok.is_space\n",
    "    ]\n",
    "    if not vecs:\n",
    "        return np.zeros(vector_size, dtype=\"float32\")\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "def coherence_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Coherence (CoH) for a single document as the average\n",
    "    cosine similarity between adjacent sentence vectors:\n",
    "\n",
    "        CoH = (1 / (k-1)) * sum_{i=1}^{k-1} cos(h_i, h_{i+1})\n",
    "\n",
    "    where h_i is the sentence/topic vector for sentence i.\n",
    "\n",
    "    If the document has fewer than 2 sentences, CoH = 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    if nlp.vocab.vectors_length == 0:\n",
    "        raise ValueError(\n",
    "            \"The loaded spaCy model does not contain word vectors \"\n",
    "            \"(nlp.vocab.vectors_length == 0). \"\n",
    "            \"Use a model like 'en_core_web_md' or similar.\"\n",
    "        )\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    k = len(sentences)\n",
    "\n",
    "    if k < 2:\n",
    "        # Only one sentence: no adjacent pair, coherence = 0.0\n",
    "        return 0.0\n",
    "\n",
    "    vector_size = nlp.vocab.vectors_length\n",
    "    sent_vectors = [\n",
    "        sentence_vector(sent, vector_size)\n",
    "        for sent in sentences\n",
    "    ]\n",
    "\n",
    "    sims = []\n",
    "    for i in range(k - 1):\n",
    "        v1 = sent_vectors[i]\n",
    "        v2 = sent_vectors[i + 1]\n",
    "        norm1 = np.linalg.norm(v1)\n",
    "        norm2 = np.linalg.norm(v2)\n",
    "        denom = norm1 * norm2\n",
    "        if denom == 0.0:\n",
    "            # Skip pairs where at least one sentence vector is zero\n",
    "            continue\n",
    "        cos_sim = float(np.dot(v1, v2) / denom)\n",
    "        sims.append(cos_sim)\n",
    "\n",
    "    if not sims:\n",
    "        return 0.0\n",
    "\n",
    "    return float(np.mean(sims))\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_cohesion_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute LC for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with lemmatizer, POS tagger, etc.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of LC scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [lexical_cohesion_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_coherence_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute CoH for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with word vectors.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of CoH scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [coherence_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_discourse_measures(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute both LC and CoH for each row of a DataFrame and return\n",
    "    them in a dictionary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"LC\":  np.ndarray of lexical cohesion scores,\n",
    "            \"CoH\": np.ndarray of coherence scores\n",
    "        }\n",
    "    \"\"\"\n",
    "    lc_vec = compute_lexical_cohesion_vector(df, nlp, column=column)\n",
    "    coh_vec = compute_coherence_vector(df, nlp, column=column)\n",
    "    return {\"LC\": lc_vec, \"CoH\": coh_vec}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57355785",
   "metadata": {},
   "source": [
    "### Text complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fee2fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze_text_all(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Parse a text with stanza and compute all measures (lexical + syntactic)\n",
    "    in a single pass.\n",
    "\n",
    "    Returns a dict with keys:\n",
    "        \"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\"\n",
    "    (Discourse measures LC/CoH are added later at DataFrame level, via spaCy.)\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None, \"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    lex = lexical_measures_from_doc(doc)\n",
    "    syn = syntactic_measures_from_doc(doc)\n",
    "\n",
    "    out: Dict[str, Optional[float]] = {}\n",
    "    out.update(lex)\n",
    "    out.update(syn)\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_all_complexity_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    "    spacy_nlp=None,\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute all complexity measures for each row in df[column].\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with a text column.\n",
    "    column : str, default \"text\"\n",
    "        Name of the text column.\n",
    "    lang : str, default \"en\"\n",
    "        Language code for stanza.\n",
    "    n_jobs : int, default 1\n",
    "        Number of worker processes to use.\n",
    "            - 1  : sequential execution (no multiprocessing).\n",
    "            - >1 : multiprocessing with that many workers.\n",
    "            - 0 or None : use cpu_count() workers.\n",
    "    spacy_nlp : spaCy Language, required for LC / CoH\n",
    "        Pre-loaded spaCy pipeline with:\n",
    "            - POS / lemmatizer for LC\n",
    "            - word vectors for CoH (e.g. 'en_core_web_md').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "            \"MDD\":  {index: value},\n",
    "            \"CS\":   {index: value},\n",
    "            \"LC\":   {index: value},\n",
    "            \"CoH\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    items = list(df[column].items())  # list[(index, text)]\n",
    "    total_items = len(items)\n",
    "\n",
    "    # ---- Lexical + syntactic (stanza) ----\n",
    "    for idx, text in tqdm(\n",
    "        items,\n",
    "        total=total_items,\n",
    "        desc=\"Computing lexical & syntactic complexity (sequential)\",\n",
    "    ):\n",
    "        metrics = _analyze_text_all(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "\n",
    "    # ---- Discourse measures (spaCy: LC & CoH) ----\n",
    "    if spacy_nlp is None:\n",
    "        raise ValueError(\n",
    "            \"spacy_nlp must be provided to compute LC and CoH. \"\n",
    "            \"Load a spaCy model with vectors, e.g. 'en_core_web_md', and \"\n",
    "            \"pass it as spacy_nlp=...\"\n",
    "        )\n",
    "\n",
    "    discourse = compute_discourse_measures(df, spacy_nlp, column=column)\n",
    "    lc_vec = discourse[\"LC\"]\n",
    "    coh_vec = discourse[\"CoH\"]\n",
    "\n",
    "    lc_res: Dict[Any, float] = {}\n",
    "    coh_res: Dict[Any, float] = {}\n",
    "\n",
    "    # Map arrays back to DataFrame indices\n",
    "    for i, idx in enumerate(df.index):\n",
    "        lc_res[idx] = float(lc_vec[i])\n",
    "        coh_res[idx] = float(coh_vec[i])\n",
    "\n",
    "    return {\n",
    "        \"MTLD\": mtld_res,\n",
    "        \"LD\": ld_res,\n",
    "        \"LS\": ls_res,\n",
    "        \"MDD\": mdd_res,\n",
    "        \"CS\": cs_res,\n",
    "        \"LC\": lc_res,\n",
    "        \"CoH\": coh_res,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11af2ab",
   "metadata": {},
   "source": [
    "## Calcul métriques Simple/Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63769d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_both_sides_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    col_simple: str = COL_SIMPLE,\n",
    "    col_complex: str = COL_COMPLEX,\n",
    "    lang: str = LANG,\n",
    "    spacy_nlp=None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule les 7 métriques de complexité pour chaque ligne, pour Simple et Complex,\n",
    "    en réutilisant strictement compute_all_complexity_measures_df (test.ipynb).\n",
    "\n",
    "    Retour : DataFrame original + 14 colonnes :\n",
    "      - simple_MTLD ... simple_CoH\n",
    "      - complex_MTLD ... complex_CoH\n",
    "    \"\"\"\n",
    "    required = {col_simple, col_complex}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}. Colonnes présentes: {list(df.columns)}\")\n",
    "\n",
    "    if spacy_nlp is None:\n",
    "        raise ValueError(\"spacy_nlp doit être fourni (modèle avec vecteurs)\")\n",
    "\n",
    "    # Mesures Simple\n",
    "    simple = compute_all_complexity_measures_df(df, column=col_simple, lang=lang, spacy_nlp=spacy_nlp)\n",
    "    # Mesures Complex\n",
    "    complex_ = compute_all_complexity_measures_df(df, column=col_complex, lang=lang, spacy_nlp=spacy_nlp)\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # On aligne explicitement sur l'index du DataFrame\n",
    "    for m in MEASURES:\n",
    "        out[f\"simple_{m}\"] = pd.Series(simple[m]).reindex(out.index)\n",
    "        out[f\"complex_{m}\"] = pd.Series(complex_[m]).reindex(out.index)\n",
    "\n",
    "    # Conversion numérique (sécurité)\n",
    "    metric_cols = [f\"simple_{m}\" for m in MEASURES] + [f\"complex_{m}\" for m in MEASURES]\n",
    "    for c in metric_cols:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e42537",
   "metadata": {},
   "source": [
    "## Ajout orig_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41759fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_orig_id(df: pd.DataFrame, id_col: str = ID_COL) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ajoute une colonne identifiant stable 'orig_id' basée sur le numéro de ligne d'origine (0-based).\n",
    "    Si la colonne existe déjà, on ne la modifie pas.\n",
    "    \"\"\"\n",
    "    if id_col in df.columns:\n",
    "        return df\n",
    "    df2 = df.reset_index(drop=True).copy()\n",
    "    df2.insert(0, id_col, df2.index.astype(int))\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e59b8a",
   "metadata": {},
   "source": [
    "## Filtrage NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11abd1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_with_any_nan_metrics(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Supprime toute ligne pour laquelle au moins une des 14 métriques (Simple/Complex) est NaN.\n",
    "    Retourne :\n",
    "      - df_kept_nan : lignes conservées\n",
    "      - df_removed_nan : lignes supprimées à cause de NaN\n",
    "    \"\"\"\n",
    "    metric_cols = [f\"simple_{m}\" for m in MEASURES] + [f\"complex_{m}\" for m in MEASURES]\n",
    "    mask_ok = df[metric_cols].notna().all(axis=1)\n",
    "    return df[mask_ok].copy(), df[~mask_ok].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca6486",
   "metadata": {},
   "source": [
    "## Dominance (Simple > Complex sur ≥ MIN_METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be549f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominance_flags(df: pd.DataFrame, min_metrics: int = MIN_METRICS) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Une ligne est en dominance si Simple est strictement > Complex sur au moins `min_metrics` métriques.\n",
    "    \n",
    "    \"\"\"\n",
    "    wins = []\n",
    "    for m in MEASURES:\n",
    "        s = df[f\"simple_{m}\"]\n",
    "        c = df[f\"complex_{m}\"]\n",
    "        wins.append(s > c)\n",
    "\n",
    "    wins_count = pd.concat(wins, axis=1).sum(axis=1)\n",
    "    return wins_count >= min_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f058e4",
   "metadata": {},
   "source": [
    "## Export CSV (All / Kept / Removed / Removed IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ce55bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_csv_bundle(dataset_name: str, df_all: pd.DataFrame, df_kept: pd.DataFrame, df_removed: pd.DataFrame) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte les résultats en CSV (UTF-8), selon le format attendu.\n",
    "    \"\"\"\n",
    "    all_path = OUT_DIR / f\"{dataset_name}_with_metrics_all.csv\"\n",
    "    kept_path = OUT_DIR / f\"{dataset_name}_with_metrics_kept.csv\"\n",
    "    removed_path = OUT_DIR / f\"{dataset_name}_with_metrics_removed.csv\"\n",
    "    removed_ids_path = OUT_DIR / f\"{dataset_name}_removed_ids.csv\"\n",
    "\n",
    "    df_all.to_csv(all_path, index=False, encoding=\"utf-8\")\n",
    "    df_kept.to_csv(kept_path, index=False, encoding=\"utf-8\")\n",
    "    df_removed.to_csv(removed_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    pd.DataFrame({ID_COL: df_removed[ID_COL].astype(int).sort_values()}).to_csv(\n",
    "        removed_ids_path, index=False, encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"all\": str(all_path),\n",
    "        \"kept\": str(kept_path),\n",
    "        \"removed\": str(removed_path),\n",
    "        \"removed_ids\": str(removed_ids_path),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604e71e",
   "metadata": {},
   "source": [
    "## Pipeline dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7653d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_offline(dataset_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Prépare un dataset :\n",
    "      1) charge les données via load_dataset (test.ipynb)\n",
    "      2) ajoute orig_id (numéro de ligne d'origine)\n",
    "      3) calcule les métriques offline (Simple & Complex)\n",
    "      4) supprime les lignes avec NaN sur une métrique\n",
    "      5) supprime les lignes en dominance\n",
    "      6) exporte en CSV\n",
    "\n",
    "    Sortie finale (df_kept) : 16 colonnes minimales :\n",
    "      orig_id, Simple, Complex, 7 métriques Simple, 7 métriques Complex\n",
    "    \"\"\"\n",
    "    df_raw = load_dataset(dataset_name)\n",
    "    df_raw = add_orig_id(df_raw, id_col=ID_COL)\n",
    "\n",
    "    # On conserve uniquement les colonnes texte + id\n",
    "    df_base = df_raw[[ID_COL, COL_SIMPLE, COL_COMPLEX]].copy()\n",
    "\n",
    "    # Calcul des métriques\n",
    "    df_all = compute_both_sides_metrics(df_base, spacy_nlp=spacy_nlp)\n",
    "\n",
    "    # Marquage NaN\n",
    "    df_no_nan, df_removed_nan = drop_rows_with_any_nan_metrics(df_all)\n",
    "    df_no_nan[\"removed_reason\"] = \"\"  # juste pour cohérence si besoin\n",
    "\n",
    "    # Dominance\n",
    "    dom = dominance_flags(df_no_nan, min_metrics=MIN_METRICS)\n",
    "    df_no_nan[\"simple_dominates_complex\"] = dom\n",
    "\n",
    "    df_removed_dom = df_no_nan[df_no_nan[\"simple_dominates_complex\"]].copy()\n",
    "    df_kept = df_no_nan[~df_no_nan[\"simple_dominates_complex\"]].copy()\n",
    "\n",
    "    # Bundle removed = NaN + dominance \n",
    "    df_removed_nan = df_removed_nan.copy()\n",
    "    if not df_removed_nan.empty:\n",
    "        df_removed_nan[\"simple_dominates_complex\"] = False\n",
    "        df_removed_nan[\"removed_reason\"] = \"nan_metrics\"\n",
    "\n",
    "    if not df_removed_dom.empty:\n",
    "        df_removed_dom[\"removed_reason\"] = \"dominance\"\n",
    "\n",
    "    df_removed = pd.concat([df_removed_nan, df_removed_dom], axis=0).sort_values(ID_COL)\n",
    "\n",
    "    # Création rendu metric\n",
    "    metric_cols_simple = [f\"simple_{m}\" for m in MEASURES]\n",
    "    metric_cols_complex = [f\"complex_{m}\" for m in MEASURES]\n",
    "    cols_16 = [ID_COL, COL_SIMPLE, COL_COMPLEX] + metric_cols_simple + metric_cols_complex\n",
    "\n",
    "    df_all_export = df_all[cols_16].copy()\n",
    "    df_kept_export = df_kept[cols_16].copy()\n",
    "    df_removed_export = df_removed[cols_16 + [\"removed_reason\"]].copy() if \"removed_reason\" in df_removed.columns else df_removed[cols_16].copy()\n",
    "\n",
    "    paths = export_csv_bundle(dataset_name, df_all_export, df_kept_export, df_removed_export)\n",
    "\n",
    "    return {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"rows_all\": int(df_all_export.shape[0]),\n",
    "        \"rows_kept\": int(df_kept_export.shape[0]),\n",
    "        \"rows_removed\": int(df_removed_export.shape[0]),\n",
    "        \"paths\": paths,\n",
    "        \"df_all\": df_all_export,\n",
    "        \"df_kept\": df_kept_export,\n",
    "        \"df_removed\": df_removed_export,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16cb9e4",
   "metadata": {},
   "source": [
    "## Exécution sur tous les datasets + aperçu DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9244597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preparing: ose_adv_ele.csv ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing lexical & syntactic complexity (sequential):   0%|          | 0/189 [00:00<?, ?it/s]2025-12-19 09:10:21 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 8.16MB/s]                    \n",
      "2025-12-19 09:10:21 INFO: Downloaded file to C:\\Users\\rroll\\stanza_resources\\resources.json\n",
      "2025-12-19 09:10:21 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-12-19 09:10:22 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "======================================\n",
      "\n",
      "2025-12-19 09:10:22 INFO: Using device: cpu\n",
      "2025-12-19 09:10:22 INFO: Loading: tokenize\n",
      "2025-12-19 09:10:24 INFO: Loading: mwt\n",
      "2025-12-19 09:10:24 INFO: Loading: pos\n",
      "2025-12-19 09:10:25 INFO: Loading: lemma\n",
      "2025-12-19 09:10:25 INFO: Loading: constituency\n",
      "2025-12-19 09:10:26 INFO: Loading: depparse\n",
      "2025-12-19 09:10:26 INFO: Done loading processors!\n",
      "Computing lexical & syntactic complexity (sequential): 100%|██████████| 189/189 [15:15<00:00,  4.85s/it]\n",
      "Computing lexical & syntactic complexity (sequential): 100%|██████████| 189/189 [24:50<00:00,  7.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: 189 | Kept: 188 | Removed: 1\n",
      "Saved to: {'all': 'data_prepared\\\\ose_adv_ele_with_metrics_all.csv', 'kept': 'data_prepared\\\\ose_adv_ele_with_metrics_kept.csv', 'removed': 'data_prepared\\\\ose_adv_ele_with_metrics_removed.csv', 'removed_ids': 'data_prepared\\\\ose_adv_ele_removed_ids.csv'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_id</th>\n",
       "      <th>Simple</th>\n",
       "      <th>Complex</th>\n",
       "      <th>simple_MTLD</th>\n",
       "      <th>simple_LD</th>\n",
       "      <th>simple_LS</th>\n",
       "      <th>simple_MDD</th>\n",
       "      <th>simple_CS</th>\n",
       "      <th>simple_LC</th>\n",
       "      <th>simple_CoH</th>\n",
       "      <th>complex_MTLD</th>\n",
       "      <th>complex_LD</th>\n",
       "      <th>complex_LS</th>\n",
       "      <th>complex_MDD</th>\n",
       "      <th>complex_CS</th>\n",
       "      <th>complex_LC</th>\n",
       "      <th>complex_CoH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "      <td>73.840000</td>\n",
       "      <td>0.577465</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>3.470719</td>\n",
       "      <td>3.190476</td>\n",
       "      <td>1.009926</td>\n",
       "      <td>0.829960</td>\n",
       "      <td>96.971182</td>\n",
       "      <td>0.564593</td>\n",
       "      <td>0.313559</td>\n",
       "      <td>3.658385</td>\n",
       "      <td>3.826087</td>\n",
       "      <td>1.618182</td>\n",
       "      <td>0.886974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>﻿To tourists, Amsterdam still seems very liber...</td>\n",
       "      <td>﻿Amsterdam still looks liberal to tourists, wh...</td>\n",
       "      <td>68.795500</td>\n",
       "      <td>0.542222</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>3.287793</td>\n",
       "      <td>4.636364</td>\n",
       "      <td>1.305430</td>\n",
       "      <td>0.855846</td>\n",
       "      <td>91.400000</td>\n",
       "      <td>0.533917</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>3.397510</td>\n",
       "      <td>4.684211</td>\n",
       "      <td>1.164811</td>\n",
       "      <td>0.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>﻿Anitta, a music star from Brazil, has million...</td>\n",
       "      <td>﻿Brazil’s latest funk sensation, Anitta, has w...</td>\n",
       "      <td>69.193644</td>\n",
       "      <td>0.521097</td>\n",
       "      <td>0.242915</td>\n",
       "      <td>3.213359</td>\n",
       "      <td>3.423077</td>\n",
       "      <td>1.951860</td>\n",
       "      <td>0.824020</td>\n",
       "      <td>92.232609</td>\n",
       "      <td>0.527691</td>\n",
       "      <td>0.316832</td>\n",
       "      <td>3.109058</td>\n",
       "      <td>4.104167</td>\n",
       "      <td>3.159700</td>\n",
       "      <td>0.822927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Google has made maps of the world’s highest mo...</td>\n",
       "      <td>﻿It has charted the world’s highest peaks, the...</td>\n",
       "      <td>70.260708</td>\n",
       "      <td>0.573469</td>\n",
       "      <td>0.185053</td>\n",
       "      <td>3.123752</td>\n",
       "      <td>3.782609</td>\n",
       "      <td>1.937630</td>\n",
       "      <td>0.873518</td>\n",
       "      <td>108.234454</td>\n",
       "      <td>0.556650</td>\n",
       "      <td>0.309735</td>\n",
       "      <td>3.386614</td>\n",
       "      <td>5.037037</td>\n",
       "      <td>2.083650</td>\n",
       "      <td>0.882598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>﻿The auction of a Banksy painting that disappe...</td>\n",
       "      <td>﻿The controversial auction of a Banksy mural t...</td>\n",
       "      <td>53.250000</td>\n",
       "      <td>0.481221</td>\n",
       "      <td>0.282927</td>\n",
       "      <td>3.093187</td>\n",
       "      <td>4.904762</td>\n",
       "      <td>1.612591</td>\n",
       "      <td>0.841078</td>\n",
       "      <td>78.500000</td>\n",
       "      <td>0.494268</td>\n",
       "      <td>0.296392</td>\n",
       "      <td>3.488317</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>1.922876</td>\n",
       "      <td>0.883444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>﻿The huge fortunes made by the world’s richest...</td>\n",
       "      <td>﻿The vast fortunes made by the world’s richest...</td>\n",
       "      <td>59.419741</td>\n",
       "      <td>0.520202</td>\n",
       "      <td>0.218447</td>\n",
       "      <td>3.343531</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>1.323120</td>\n",
       "      <td>0.878235</td>\n",
       "      <td>89.698761</td>\n",
       "      <td>0.543372</td>\n",
       "      <td>0.262048</td>\n",
       "      <td>3.404561</td>\n",
       "      <td>4.458333</td>\n",
       "      <td>1.977153</td>\n",
       "      <td>0.869142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>In 2005, BlackBerry brought instant messaging ...</td>\n",
       "      <td>﻿Back in 2005, when BlackBerry brought instant...</td>\n",
       "      <td>76.704428</td>\n",
       "      <td>0.565414</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>2.961620</td>\n",
       "      <td>3.090909</td>\n",
       "      <td>1.355346</td>\n",
       "      <td>0.788039</td>\n",
       "      <td>121.664773</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.330827</td>\n",
       "      <td>3.221481</td>\n",
       "      <td>2.981132</td>\n",
       "      <td>2.238967</td>\n",
       "      <td>0.809628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>﻿A big international disagreement has started ...</td>\n",
       "      <td>﻿A major international row with wide-ranging i...</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.561224</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>3.198370</td>\n",
       "      <td>4.193548</td>\n",
       "      <td>1.754386</td>\n",
       "      <td>0.849859</td>\n",
       "      <td>63.952921</td>\n",
       "      <td>0.543584</td>\n",
       "      <td>0.369710</td>\n",
       "      <td>3.417543</td>\n",
       "      <td>4.212121</td>\n",
       "      <td>1.825871</td>\n",
       "      <td>0.864913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>﻿Male bosses are paid bonuses double the size ...</td>\n",
       "      <td>﻿Male bosses are being paid bonuses double the...</td>\n",
       "      <td>72.973333</td>\n",
       "      <td>0.551069</td>\n",
       "      <td>0.168103</td>\n",
       "      <td>3.187893</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.583333</td>\n",
       "      <td>0.872890</td>\n",
       "      <td>95.714286</td>\n",
       "      <td>0.538806</td>\n",
       "      <td>0.224377</td>\n",
       "      <td>3.393355</td>\n",
       "      <td>5.318182</td>\n",
       "      <td>3.143317</td>\n",
       "      <td>0.871018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>﻿More than 100,000 people went onto the street...</td>\n",
       "      <td>﻿Brazil experienced one of its biggest nights ...</td>\n",
       "      <td>100.275000</td>\n",
       "      <td>0.582897</td>\n",
       "      <td>0.323353</td>\n",
       "      <td>3.097560</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.793594</td>\n",
       "      <td>0.810610</td>\n",
       "      <td>116.480666</td>\n",
       "      <td>0.588964</td>\n",
       "      <td>0.365201</td>\n",
       "      <td>3.211171</td>\n",
       "      <td>3.846154</td>\n",
       "      <td>2.972509</td>\n",
       "      <td>0.837656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   orig_id                                             Simple  \\\n",
       "0        0  ﻿When you see the word Amazon, what’s the firs...   \n",
       "1        1  ﻿To tourists, Amsterdam still seems very liber...   \n",
       "2        2  ﻿Anitta, a music star from Brazil, has million...   \n",
       "3        3  Google has made maps of the world’s highest mo...   \n",
       "4        4  ﻿The auction of a Banksy painting that disappe...   \n",
       "5        5  ﻿The huge fortunes made by the world’s richest...   \n",
       "6        6  In 2005, BlackBerry brought instant messaging ...   \n",
       "7        7  ﻿A big international disagreement has started ...   \n",
       "8        8  ﻿Male bosses are paid bonuses double the size ...   \n",
       "9        9  ﻿More than 100,000 people went onto the street...   \n",
       "\n",
       "                                             Complex  simple_MTLD  simple_LD  \\\n",
       "0  ﻿When you see the word Amazon, what’s the firs...    73.840000   0.577465   \n",
       "1  ﻿Amsterdam still looks liberal to tourists, wh...    68.795500   0.542222   \n",
       "2  ﻿Brazil’s latest funk sensation, Anitta, has w...    69.193644   0.521097   \n",
       "3  ﻿It has charted the world’s highest peaks, the...    70.260708   0.573469   \n",
       "4  ﻿The controversial auction of a Banksy mural t...    53.250000   0.481221   \n",
       "5  ﻿The vast fortunes made by the world’s richest...    59.419741   0.520202   \n",
       "6  ﻿Back in 2005, when BlackBerry brought instant...    76.704428   0.565414   \n",
       "7  ﻿A major international row with wide-ranging i...    66.000000   0.561224   \n",
       "8  ﻿Male bosses are being paid bonuses double the...    72.973333   0.551069   \n",
       "9  ﻿Brazil experienced one of its biggest nights ...   100.275000   0.582897   \n",
       "\n",
       "   simple_LS  simple_MDD  simple_CS  simple_LC  simple_CoH  complex_MTLD  \\\n",
       "0   0.268293    3.470719   3.190476   1.009926    0.829960     96.971182   \n",
       "1   0.262295    3.287793   4.636364   1.305430    0.855846     91.400000   \n",
       "2   0.242915    3.213359   3.423077   1.951860    0.824020     92.232609   \n",
       "3   0.185053    3.123752   3.782609   1.937630    0.873518    108.234454   \n",
       "4   0.282927    3.093187   4.904762   1.612591    0.841078     78.500000   \n",
       "5   0.218447    3.343531   4.333333   1.323120    0.878235     89.698761   \n",
       "6   0.297872    2.961620   3.090909   1.355346    0.788039    121.664773   \n",
       "7   0.303030    3.198370   4.193548   1.754386    0.849859     63.952921   \n",
       "8   0.168103    3.187893   4.000000   2.583333    0.872890     95.714286   \n",
       "9   0.323353    3.097560   3.500000   1.793594    0.810610    116.480666   \n",
       "\n",
       "   complex_LD  complex_LS  complex_MDD  complex_CS  complex_LC  complex_CoH  \n",
       "0    0.564593    0.313559     3.658385    3.826087    1.618182     0.886974  \n",
       "1    0.533917    0.360656     3.397510    4.684211    1.164811     0.869300  \n",
       "2    0.527691    0.316832     3.109058    4.104167    3.159700     0.822927  \n",
       "3    0.556650    0.309735     3.386614    5.037037    2.083650     0.882598  \n",
       "4    0.494268    0.296392     3.488317    5.166667    1.922876     0.883444  \n",
       "5    0.543372    0.262048     3.404561    4.458333    1.977153     0.869142  \n",
       "6    0.550725    0.330827     3.221481    2.981132    2.238967     0.809628  \n",
       "7    0.543584    0.369710     3.417543    4.212121    1.825871     0.864913  \n",
       "8    0.538806    0.224377     3.393355    5.318182    3.143317     0.871018  \n",
       "9    0.588964    0.365201     3.211171    3.846154    2.972509     0.837656  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results = {}\n",
    "\n",
    "DATA_DIR = Path(\"data_sampled\")\n",
    "\n",
    "# Liste des CSV à traiter\n",
    "CSV_FILES_TO_RUN = [\n",
    "    \"ose_adv_ele.csv\",\n",
    "    # \"ose_adv_int.csv\",\n",
    "    # \"swipe.csv\",\n",
    "    # 'vikidia.csv'\n",
    "]\n",
    "\n",
    "for csv_name in CSV_FILES_TO_RUN:\n",
    "    csv_path = DATA_DIR / csv_name\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Fichier introuvable : {csv_path.resolve()}\")\n",
    "\n",
    "    dataset_name = csv_path.stem\n",
    "    print(f\"\\n=== Preparing: {csv_path.name} ===\")\n",
    "\n",
    "    info = prepare_dataset_offline(dataset_name)\n",
    "    results[dataset_name] = info\n",
    "\n",
    "    print(f\"All: {info['rows_all']} | Kept: {info['rows_kept']} | Removed: {info['rows_removed']}\")\n",
    "    print(\"Saved to:\", info[\"paths\"])\n",
    "\n",
    "# Aperçu\n",
    "example_name = next(iter(results.keys()))\n",
    "df_prepared = results[example_name][\"df_kept\"]\n",
    "df_prepared.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
