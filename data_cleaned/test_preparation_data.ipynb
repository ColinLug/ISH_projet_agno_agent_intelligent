{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fe89001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 8.23MB/s]                    \n",
      "2025-12-19 22:10:00 INFO: Downloaded file to C:\\Users\\rroll\\stanza_resources\\resources.json\n",
      "2025-12-19 22:10:00 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-12-19 22:10:00 INFO: File exists: C:\\Users\\rroll\\stanza_resources\\en\\default.zip\n",
      "2025-12-19 22:10:03 INFO: Finished downloading models and saved to C:\\Users\\rroll\\stanza_resources\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rroll\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rroll\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x22ef2614050>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "from pprint import pprint\n",
    "from typing import Dict, Set, Iterable, Optional, Any, Tuple\n",
    "import importlib.resources as pkg_resources\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "import stanza\n",
    "import textcomplexity  # only used to access en.json\n",
    "from tqdm.auto import tqdm  \n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Download required resources\n",
    "stanza.download('en')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Make sure WordNet is available; if not, download it.\n",
    "try:\n",
    "    _ = wn.synsets(\"dog\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "spacy_nlp = nlp\n",
    "spacy_nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed542395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Colonnes attendues\n",
    "COL_SIMPLE = \"Simple\"\n",
    "COL_COMPLEX = \"Complex\"\n",
    "ID_COL = \"orig_id\"\n",
    "\n",
    "# Sorties\n",
    "OUT_DIR = Path(\"data_prepared\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Langue / modèles\n",
    "LANG = \"en\"\n",
    "SPACY_MODEL = \"en_core_web_md\"\n",
    "\n",
    "\n",
    "# Liste officielle des métriques (test.ipynb)\n",
    "MEASURES = [\"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\", \"LC\", \"CoH\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e044f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement spaCy (doit contenir des vecteurs pour CoH)\n",
    "nlp = spacy.load(SPACY_MODEL)\n",
    "spacy_nlp = nlp\n",
    "\n",
    "# Important : segmentation en phrases\n",
    "if \"sentencizer\" not in spacy_nlp.pipe_names:\n",
    "    spacy_nlp.add_pipe(\"sentencizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff4f785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire des datasets\n",
    "datasets ={'ose_adv_ele':'data_sampled/OSE_adv_ele.csv', \n",
    "           'ose_adv_int':'data_sampled/OSE_adv_int.csv',\n",
    "           'swipe': 'data_sampled/swipe.csv',\n",
    "           'vikidia':'data_sampled/vikidia.csv'}\n",
    "\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path, sep='\\t')\n",
    "    \n",
    "\n",
    "def load_dataset(name):\n",
    "    if name not in datasets:\n",
    "        raise ValueError(f\"Dataset {name} not found\")\n",
    "    return load_data(datasets[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e76a962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Simple</th>\n",
       "      <th>Complex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When you see the word Amazon, whats the first ...</td>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To tourists, Amsterdam still seems very libera...</td>\n",
       "      <td>﻿Amsterdam still looks liberal to tourists, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brazils latest funk sensation, Anitta, has won...</td>\n",
       "      <td>﻿Brazil’s latest funk sensation, Anitta, has w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Simple  \\\n",
       "0  When you see the word Amazon, whats the first ...   \n",
       "1  To tourists, Amsterdam still seems very libera...   \n",
       "2  Brazils latest funk sensation, Anitta, has won...   \n",
       "\n",
       "                                             Complex  \n",
       "0  ﻿When you see the word Amazon, what’s the firs...  \n",
       "1  ﻿Amsterdam still looks liberal to tourists, wh...  \n",
       "2  ﻿Brazil’s latest funk sensation, Anitta, has w...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('ose_adv_int')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "127c2dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE TEXT\n",
      "A car with a top speed of 25mph, two seats and no pedals or steering wheel might not make much of an impression at a motor show. But Google, in the US, sent a shock through the car and taxi industries as it unveiled the latest version of its driverless car.\n",
      "The electrically powered vehicle, which Google has begun testing around its headquarters in Mountain View, California, dispenses with all the normal controls, including foot pedals. Instead, it has a smartphone app that calls it and tells it the destination, and a single STOP button between the two front-facing seats in case the passengers need to override the computer.\n",
      "The car, in fact, does all the tasks of navigation, steering, acceleration and braking.\n",
      "The company is building about 100 prototypes for a two-year test. The companys co-founder, Sergey Brin, told a conference in California that the vehicle was still in the prototype stage but that the project was about changing the world for people who do not have good transportation today.\n",
      "He said of the car: Youre just sitting there; no steering wheel, no pedals. For me, it was very relaxing. About ten seconds after getting in, I forgot I was there. I found it really enjoyable.\n",
      "Google says that the aim of the project is to improve safety and that, because the car is constructed with impact-absorbing foam at the front and a plastic windscreen, it should be far safer than any other car for pedestrians.\n",
      "The cars, which have been built specially by an unnamed company in Detroit, will be used to find out how best to make driverless vehicles work. Google will run a pilot programme using the cars, which are not yet for sale.\n",
      "One challenge is creating high-definition scans of the roads and surroundings before the cars can drive along them because they cannot collect and process enough information in real time.\n",
      "So far, there are high-detail maps of about 2,000 miles of Californias roads, but the state has more than 170,000 miles of public roads.\n",
      "Google says it is interested in licensing the technology to traditional vehicle manufacturers once it has been refined.\n",
      "But the idea of driverless cars replacing humandriven taxis has been the cause of some alarm.\n",
      "Dennis Conyon of the UK National Taxi Association says that drivers will become unemployed.\n",
      "London has about 22,000 licensed black cabs and Conyon estimates that the total number of people who drive taxis for hire in the UK is about 100,000.\n",
      "Other car makers, including Volvo, Ford and Mercedes, are working on driver-assisted vehicles, which, unlike Googles version, do not dispense with the driver controls.\n",
      "But Chris Urmson, director of the self-driving car project at Google, said that the new prototypes dispensed with the steering wheel and brakes because a human passenger might not be able to take over in an emergency, and that it was simpler just to have an emergency stop button.\n",
      "Urmson said: The vehicles will be very basic. We want to learn from them and adapt them as quickly as possible. But they will take you where you want to go at the push of a button. And, thats an important step towards improving road safety and improving mobility for millions of people.\n",
      "So far, the Google versions of the self-driving cars have driven 700,000 miles without an accident caused by the computer. The company says that thousands of people die each year on the roads and that about 80% of crashes are caused by human error.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "COMPLEX TEXT\n",
      "﻿Unveiling a car with a top speed of 25mph, two seats and no pedals or steering wheel might not make much of an impression at a motor show. But Google, in the US, sent a minor earthquake through the car and taxi industries as it unveiled the latest version of its driverless car.\n",
      "The electrically powered vehicle, which Google has begun testing around its headquarters in Mountain View, California, dispenses with all the normal controls, including foot pedals. Instead, it has a smartphone app that summons it and tells it the destination, and a single STOP button mounted between the two front-facing seats in case the occupants need to override the computer.\n",
      "The car, in fact, takes over all the tasks of navigation, steering, acceleration and braking.\n",
      "The company is building about 100 prototypes for a two-year test. The company’s co-founder, Sergey Brin, told a conference in California that the vehicle was “still in the prototype stage” but that the project was “about changing the world for people who are not well served by transportation today ”.\n",
      "He said of the car: “You’re just sitting there; no steering wheel, no pedals. For me, it was very relaxing. About ten seconds after getting in, I forgot I was there. It reminded me of catching a chairlift by yourself – a bit of solitude I found really enjoyable.”\n",
      "Google says that the principal aim of the project is to improve safety and that, because the car is constructed with impact-absorbing foam at the front and a plastic windscreen, “it should be far safer than any other car for pedestrians”.\n",
      "The cars, which have been built specially by a company (as yet unnamed) in Detroit, will be used to investigate further how best to make driverless vehicles work. Google will run a pilot programme using the cars, which are not yet for sale.\n",
      "One challenge is creating high-definition scans of the roads and surroundings before the cars can drive along them because they cannot gather and process enough information in real time.\n",
      "So far, there are high-detail maps of about 2,000 miles of California’s roads, but the state has more than 170,000 miles of public roads.\n",
      "Google says it is interested in licensing the technology to traditional vehicle manufacturers once it has refined it sufficiently. Members of the team had been working on the project even before joining Google, for more than a decade.\n",
      "But the prospect of driverless cars replacing human-driven taxis has been the cause of some alarm.\n",
      "“If you get rid of the driver, then they’re unemployed,” said Dennis Conyon, the south- east director for the UK National Taxi Association. “It would have a major impact on the labour force.”\n",
      "London has about 22,000 licensed black cabs and Conyon estimates that the total number of people who drive taxis for hire in the UK is about 100,000.\n",
      "However, Steve McNamara, general secretary of the 10,500-strong London Taxi Drivers’ Association, said: “You won’t get these driverless cars in London for 20 or 25 years. Maybe, by then, they’ll have a charge point – because there isn’t a single one in London now.”\n",
      "Other car makers, including Volvo, Ford and Mercedes, are working on driver-assisted vehicles, which, unlike Google’s version, do not dispense with the driver controls.\n",
      "But Chris Urmson, director of the self-driving car project at Google, said that the new prototypes dispensed with the steering wheel and brakes because there was no guarantee that a human occupant would be able to take over in an emergency, and that it was simpler just to have an emergency stop button.\n",
      "Urmson said: “The vehicles will be very basic. We want to learn from them and adapt them as quickly as possible. But they will take you where you want to go at the push of a button. And, that’s an important step towards improving road safety and transforming mobility for millions of people.”\n",
      "So far, the Google versions of the self-driving cars have covered 700,000 miles without an accident caused by the computer. The company points out that thousands of people die each year on the roads and that about 80% of crashes can be ascribed to human error.\n",
      "But, they could have some way to go to match Conyon at the National Taxi Association. Aged 79, he has been driving a taxi for 50 years and claims never to have had an accident.\n"
     ]
    }
   ],
   "source": [
    "row = df.sample(1)\n",
    "\n",
    "print('SIMPLE TEXT')\n",
    "print(row['Simple'].iloc[0])\n",
    "print('-'*100)\n",
    "print('COMPLEX TEXT')\n",
    "print(row['Complex'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e0dfffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ose_adv_ele: 189 rows\n",
      "ose_adv_int: 189 rows\n",
      "swipe: 1233 rows\n",
      "vikidia: 1233 rows\n",
      "Total: 2844 rows\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for name, path in datasets.items():\n",
    "    df = load_dataset(name)\n",
    "    print(f\"{name}: {df.shape[0]} rows\")\n",
    "    cnt += df.shape[0]\n",
    "print(f\"Total: {cnt} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afca2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset('ose_adv_int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02a669",
   "metadata": {},
   "source": [
    "## Complexity measures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa4b934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache stanza pipelines to avoid re-loading models\n",
    "_STANZA_PIPELINES: Dict[str, stanza.Pipeline] = {}\n",
    "\n",
    "# UPOS tags considered content words (C)\n",
    "CONTENT_UPOS = {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def load_cow_top5000_en() -> Set[str]:\n",
    "    \"\"\"\n",
    "    Load the COW-based list of the 5,000 most frequent English content words\n",
    "    from textcomplexity's English language definition file (en.json).\n",
    "\n",
    "    We ignore POS tags and keep only lowercased word forms.\n",
    "    \"\"\"\n",
    "    with pkg_resources.files(textcomplexity).joinpath(\"en.json\").open(\n",
    "        \"r\", encoding=\"utf-8\"\n",
    "    ) as f:\n",
    "        lang_def = json.load(f)\n",
    "\n",
    "    most_common = lang_def[\"most_common\"]  # list of [word, xpos]\n",
    "    cow_top5000 = {w.lower() for w, xpos in most_common}\n",
    "    return cow_top5000\n",
    "\n",
    "\n",
    "def get_stanza_pipeline(lang: str = \"en\", use_gpu: bool = False) -> stanza.Pipeline:\n",
    "    \"\"\"\n",
    "    Get (or create) a cached stanza Pipeline for a given language.\n",
    "\n",
    "    NOTE: You must have downloaded the models beforehand, e.g.:\n",
    "        import stanza\n",
    "        stanza.download('en')\n",
    "    \"\"\"\n",
    "    if lang not in _STANZA_PIPELINES:\n",
    "        _STANZA_PIPELINES[lang] = stanza.Pipeline(\n",
    "            lang=lang,\n",
    "            processors=\"tokenize,pos,lemma,depparse,constituency\",\n",
    "            use_gpu=use_gpu,\n",
    "            tokenize_no_ssplit=False,\n",
    "        )\n",
    "    return _STANZA_PIPELINES[lang]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d67b5",
   "metadata": {},
   "source": [
    "### Lexical complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8ee6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_mtld(tokens: Iterable[str], ttr_threshold: float = 0.72) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute MTLD (Measure of Textual Lexical Diversity) for a list of tokens.\n",
    "\n",
    "    MTLD = total_number_of_tokens / number_of_factors\n",
    "\n",
    "    A factor is a contiguous segment where the running TTR stays >= threshold.\n",
    "    When the TTR drops below the threshold, we close a factor (at the previous\n",
    "    token) and start a new one. At the end, the remaining partial segment is\n",
    "    counted as a fractional factor, with weight proportional to how close the\n",
    "    final TTR is to the threshold.\n",
    "    \"\"\"\n",
    "    tokens = [tok for tok in tokens if tok]\n",
    "    if not tokens:\n",
    "        return None\n",
    "\n",
    "    types = set()\n",
    "    factor_count = 0.0\n",
    "    token_count_in_factor = 0\n",
    "\n",
    "    for tok in tokens:\n",
    "        token_count_in_factor += 1\n",
    "        types.add(tok)\n",
    "        ttr = len(types) / token_count_in_factor\n",
    "\n",
    "        if ttr < ttr_threshold:\n",
    "            factor_count += 1.0\n",
    "            types = set()\n",
    "            token_count_in_factor = 0\n",
    "\n",
    "    # final partial factor\n",
    "    if token_count_in_factor > 0:\n",
    "        final_ttr = len(types) / token_count_in_factor\n",
    "        if final_ttr < 1.0:\n",
    "            fractional = (1.0 - final_ttr) / (1.0 - ttr_threshold)\n",
    "            fractional = max(0.0, min(1.0, fractional))\n",
    "            factor_count += fractional\n",
    "\n",
    "    if factor_count == 0:\n",
    "        return None\n",
    "\n",
    "    return len(tokens) / factor_count\n",
    "\n",
    "\n",
    "\n",
    "def _compute_lexical_density(total_tokens: int, content_tokens: int) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LD = |C| / |T|\n",
    "    where:\n",
    "        |C| = number of content-word tokens\n",
    "        |T| = total number of non-punctuation tokens\n",
    "    \"\"\"\n",
    "    if total_tokens == 0:\n",
    "        return None\n",
    "    return content_tokens / total_tokens\n",
    "\n",
    "\n",
    "def _compute_lexical_sophistication_cow(\n",
    "    content_forms: Iterable[str],\n",
    "    cow_top5000: set,\n",
    ") -> Optional[float]:\n",
    "    \"\"\"\n",
    "    LS = |{ w in C : w not in R }| / |C|\n",
    "    where:\n",
    "        C = content-word tokens (surface forms, lowercased)\n",
    "        R = COW top-5000 content word forms (lowercased)\n",
    "    \"\"\"\n",
    "    forms = [f for f in content_forms if f]\n",
    "    if not forms:\n",
    "        return None\n",
    "\n",
    "    off_list = sum(1 for f in forms if f not in cow_top5000)\n",
    "    return off_list / len(forms)\n",
    "\n",
    "\n",
    "\n",
    "def lexical_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MTLD, LD, LS from a stanza Document.\n",
    "    \"\"\"\n",
    "    cow_top5000 = load_cow_top5000_en()\n",
    "\n",
    "    mtld_tokens = []\n",
    "    total_tokens = 0\n",
    "    content_tokens = 0\n",
    "    content_forms = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.upos == \"PUNCT\":\n",
    "                continue\n",
    "\n",
    "            lemma = (word.lemma or word.text or \"\").lower()\n",
    "            if not lemma:\n",
    "                continue\n",
    "\n",
    "            mtld_tokens.append(lemma)\n",
    "            total_tokens += 1\n",
    "\n",
    "            if word.upos in CONTENT_UPOS:\n",
    "                content_tokens += 1\n",
    "                form = (word.text or \"\").lower()\n",
    "                content_forms.append(form)\n",
    "\n",
    "    mtld = _compute_mtld(mtld_tokens) if mtld_tokens else None\n",
    "    ld = _compute_lexical_density(total_tokens, content_tokens)\n",
    "    ls = _compute_lexical_sophistication_cow(content_forms, cow_top5000)\n",
    "\n",
    "    return {\"MTLD\": mtld, \"LD\": ld, \"LS\": ls}\n",
    "\n",
    "\n",
    "def lexical_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute lexical measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return lexical_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute lexical measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = lexical_measures_from_text(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "\n",
    "    return {\"MTLD\": mtld_res, \"LD\": ld_res, \"LS\": ls_res}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60121915",
   "metadata": {},
   "source": [
    "### Syntactic complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43a1952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdd_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute Mean Dependency Distance (MDD) from a stanza Document.\n",
    "\n",
    "    For each sentence s_i with dependency set D_i:\n",
    "        MDD_i = (1 / |D_i|) * sum_{(h,d) in D_i} |h - d|\n",
    "    Then:\n",
    "        MDD = (1 / k) * sum_i MDD_i, over all sentences with at least one dependency.\n",
    "    \"\"\"\n",
    "    sentence_mdds = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        distances = []\n",
    "        for w in sent.words:\n",
    "            if w.head is None or w.head == 0:\n",
    "                continue\n",
    "            distances.append(abs(w.id - w.head))\n",
    "\n",
    "        if distances:\n",
    "            sentence_mdds.append(sum(distances) / len(distances))\n",
    "\n",
    "    if not sentence_mdds:\n",
    "        return None\n",
    "    return sum(sentence_mdds) / len(sentence_mdds)\n",
    "\n",
    "\n",
    "\n",
    "def _count_clauses_in_tree(tree) -> int:\n",
    "    \"\"\"\n",
    "    Count clause nodes in a constituency tree.\n",
    "\n",
    "    A simple and standard heuristic (PTB-style) is:\n",
    "        count all nodes whose label starts with 'S'\n",
    "        (S, SBAR, SBARQ, SINV, SQ, etc.).\n",
    "\n",
    "    This aligns with the idea of counting finite and subordinate clauses\n",
    "    as in Hunt (1965) and later complexity work.\n",
    "    \"\"\"\n",
    "    if tree is None:\n",
    "        return 0\n",
    "\n",
    "    # Stanza's constituency tree: tree.label, tree.children\n",
    "    count = 1 if getattr(tree, \"label\", \"\").startswith(\"S\") else 0\n",
    "\n",
    "    for child in getattr(tree, \"children\", []):\n",
    "        # leaves can be strings or terminals without 'label'\n",
    "        if hasattr(child, \"label\"):\n",
    "            count += _count_clauses_in_tree(child)\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def cs_from_doc(doc) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute CS (clauses per sentence) from a stanza Document.\n",
    "\n",
    "        CS = (1 / k) * sum_i L_i\n",
    "\n",
    "    where L_i is the number of clauses in sentence s_i, estimated by counting\n",
    "    all constituents whose label starts with 'S' in the constituency tree of s_i.\n",
    "    \"\"\"\n",
    "    clause_counts = []\n",
    "    for sent in doc.sentences:\n",
    "        tree = getattr(sent, \"constituency\", None)\n",
    "        if tree is None:\n",
    "            # No constituency tree available for this sentence\n",
    "            continue\n",
    "        num_clauses = _count_clauses_in_tree(tree)\n",
    "        clause_counts.append(num_clauses)\n",
    "\n",
    "    if not clause_counts:\n",
    "        return None\n",
    "\n",
    "    return sum(clause_counts) / len(clause_counts)\n",
    "\n",
    "\n",
    "\n",
    "def syntactic_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute MDD and CS from a stanza Document.\n",
    "    \"\"\"\n",
    "    mdd = mdd_from_doc(doc)\n",
    "    cs = cs_from_doc(doc)\n",
    "    return {\"MDD\": mdd, \"CS\": cs}\n",
    "\n",
    "\n",
    "def syntactic_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: parse a single text and compute syntactic measures.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    return syntactic_measures_from_doc(doc)\n",
    "\n",
    "\n",
    "def compute_syntactic_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute syntactic measures for each row in df[column].\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"MDD\": {index: value},\n",
    "            \"CS\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    for idx, text in df[column].items():\n",
    "        metrics = syntactic_measures_from_text(text, lang=lang)\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "    return {\"MDD\": mdd_res, \"CS\": cs_res}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9abca82",
   "metadata": {},
   "source": [
    "### Discourse complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e2f2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate set of content POS tags (spaCy universal POS)\n",
    "CONTENT_POS =  {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "\n",
    "def is_content_token(tok):\n",
    "    \"\"\"\n",
    "    Return True if token is considered a content word.\n",
    "    We ignore stopwords, punctuation, and non-alphabetic tokens.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        tok.is_alpha\n",
    "        and not tok.is_stop\n",
    "        and tok.pos_ in CONTENT_POS\n",
    "    )\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=100000)\n",
    "def get_related_lemmas(lemma):\n",
    "    \"\"\"\n",
    "    Return a set of semantically related lemmas for the given lemma\n",
    "    using WordNet, including:\n",
    "      - synonyms\n",
    "      - antonyms\n",
    "      - hypernyms / hyponyms\n",
    "      - meronyms (part/member/substance)\n",
    "      - coordinate terms (siblings under the same hypernym)\n",
    "\n",
    "    NOTE: Some older examples mention 'troponyms', but in NLTK's\n",
    "    WordNet interface there is no 'troponyms()' method on Synset,\n",
    "    so we do NOT use it here.\n",
    "    \"\"\"\n",
    "    lemma = lemma.lower()\n",
    "    related = set()\n",
    "    synsets = wn.synsets(lemma)\n",
    "\n",
    "    for syn in synsets:\n",
    "        # Synonyms and antonyms\n",
    "        for l in syn.lemmas():\n",
    "            related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "            for ant in l.antonyms():\n",
    "                related.add(ant.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Hypernyms (more general) and hyponyms (more specific)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for l in hyper.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "        for hypo in syn.hyponyms():\n",
    "            for l in hypo.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Meronyms: part/member/substance\n",
    "        for mer in syn.part_meronyms() + syn.member_meronyms() + syn.substance_meronyms():\n",
    "            for l in mer.lemmas():\n",
    "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "        # Coordinate terms (siblings under same hypernym)\n",
    "        for hyper in syn.hypernyms():\n",
    "            for sibling in hyper.hyponyms():\n",
    "                if sibling == syn:\n",
    "                    continue\n",
    "                for l in sibling.lemmas():\n",
    "                    related.add(l.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "    # Remove the lemma itself if present\n",
    "    related.discard(lemma)\n",
    "    return related\n",
    "\n",
    "\n",
    "def lexical_cohesion_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Lexical Cohesion (LC) for a single document:\n",
    "\n",
    "        LC = |C| / m\n",
    "\n",
    "    where:\n",
    "      - |C| is the number of cohesive devices between sentences\n",
    "        (lexical repetition + semantic relations),\n",
    "      - m  is the total number of word tokens (alphabetic) in the document.\n",
    "\n",
    "    If the document has fewer than 2 sentences or no valid words,\n",
    "    LC is returned as 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Total number of alphabetic tokens (denominator m)\n",
    "    m = sum(1 for tok in doc if tok.is_alpha)\n",
    "    if m == 0:\n",
    "        return 0.0\n",
    "\n",
    "    sentences = list(doc.sents)\n",
    "    if len(sentences) < 2:\n",
    "        # With only one sentence, cross-sentence cohesion is not defined\n",
    "        return 0.0\n",
    "\n",
    "    # Collect sets of content lemmas per sentence\n",
    "    sent_lemmas = []\n",
    "    for sent in sentences:\n",
    "        lemmas = set(\n",
    "            tok.lemma_.lower()\n",
    "            for tok in sent\n",
    "            if is_content_token(tok)\n",
    "        )\n",
    "        if lemmas:\n",
    "            sent_lemmas.append(lemmas)\n",
    "\n",
    "    if len(sent_lemmas) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    cohesive_count = 0\n",
    "\n",
    "    for i in range(len(sent_lemmas) - 1):\n",
    "        for j in range(i + 1, len(sent_lemmas)):\n",
    "            li = sent_lemmas[i]\n",
    "            lj = sent_lemmas[j]\n",
    "\n",
    "            # 1) Lexical repetition: shared lemmas\n",
    "            shared = li & lj\n",
    "            cohesive_count += len(shared)\n",
    "\n",
    "            # 2) Semantic relations via WordNet\n",
    "            for lemma in li:\n",
    "                related = get_related_lemmas(lemma)\n",
    "                cohesive_count += len(related & lj)\n",
    "\n",
    "    return float(cohesive_count) / float(m)\n",
    "\n",
    "\n",
    "def sentence_vector(sent, vector_size):\n",
    "    \"\"\"\n",
    "    Represent a sentence as the average of token vectors.\n",
    "    If no token has a vector, return a zero vector.\n",
    "    \"\"\"\n",
    "    vecs = [\n",
    "        tok.vector\n",
    "        for tok in sent\n",
    "        if tok.has_vector and not tok.is_punct and not tok.is_space\n",
    "    ]\n",
    "    if not vecs:\n",
    "        return np.zeros(vector_size, dtype=\"float32\")\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "def coherence_single(text, nlp):\n",
    "    \"\"\"\n",
    "    Compute Coherence (CoH) for a single document as the average\n",
    "    cosine similarity between adjacent sentence vectors:\n",
    "\n",
    "        CoH = (1 / (k-1)) * sum_{i=1}^{k-1} cos(h_i, h_{i+1})\n",
    "\n",
    "    where h_i is the sentence/topic vector for sentence i.\n",
    "\n",
    "    If the document has fewer than 2 sentences, CoH = 0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    if nlp.vocab.vectors_length == 0:\n",
    "        raise ValueError(\n",
    "            \"The loaded spaCy model does not contain word vectors \"\n",
    "            \"(nlp.vocab.vectors_length == 0). \"\n",
    "            \"Use a model like 'en_core_web_md' or similar.\"\n",
    "        )\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    k = len(sentences)\n",
    "\n",
    "    if k < 2:\n",
    "        # Only one sentence: no adjacent pair, coherence = 0.0\n",
    "        return 0.0\n",
    "\n",
    "    vector_size = nlp.vocab.vectors_length\n",
    "    sent_vectors = [\n",
    "        sentence_vector(sent, vector_size)\n",
    "        for sent in sentences\n",
    "    ]\n",
    "\n",
    "    sims = []\n",
    "    for i in range(k - 1):\n",
    "        v1 = sent_vectors[i]\n",
    "        v2 = sent_vectors[i + 1]\n",
    "        norm1 = np.linalg.norm(v1)\n",
    "        norm2 = np.linalg.norm(v2)\n",
    "        denom = norm1 * norm2\n",
    "        if denom == 0.0:\n",
    "            # Skip pairs where at least one sentence vector is zero\n",
    "            continue\n",
    "        cos_sim = float(np.dot(v1, v2) / denom)\n",
    "        sims.append(cos_sim)\n",
    "\n",
    "    if not sims:\n",
    "        return 0.0\n",
    "\n",
    "    return float(np.mean(sims))\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_cohesion_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute LC for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with lemmatizer, POS tagger, etc.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of LC scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [lexical_cohesion_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_coherence_vector(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute CoH for each row of a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the texts.\n",
    "    nlp : spaCy Language object\n",
    "        Pre-loaded spaCy pipeline with word vectors.\n",
    "    column : str, default \"text\"\n",
    "        Name of the column that contains the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D array of CoH scores, length == len(df).\n",
    "    \"\"\"\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "    scores = [coherence_single(t, nlp) for t in texts]\n",
    "    return np.array(scores, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def compute_discourse_measures(df, nlp, column=\"text\"):\n",
    "    \"\"\"\n",
    "    Compute both LC and CoH for each row of a DataFrame and return\n",
    "    them in a dictionary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"LC\":  np.ndarray of lexical cohesion scores,\n",
    "            \"CoH\": np.ndarray of coherence scores\n",
    "        }\n",
    "    \"\"\"\n",
    "    lc_vec = compute_lexical_cohesion_vector(df, nlp, column=column)\n",
    "    coh_vec = compute_coherence_vector(df, nlp, column=column)\n",
    "    return {\"LC\": lc_vec, \"CoH\": coh_vec}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57355785",
   "metadata": {},
   "source": [
    "### Text complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fee2fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze_text_all(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Parse a text with stanza and compute all measures (lexical + syntactic)\n",
    "    in a single pass.\n",
    "\n",
    "    Returns a dict with keys:\n",
    "        \"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\"\n",
    "    (Discourse measures LC/CoH are added later at DataFrame level, via spaCy.)\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    if not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None, \"MDD\": None, \"CS\": None}\n",
    "\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    lex = lexical_measures_from_doc(doc)\n",
    "    syn = syntactic_measures_from_doc(doc)\n",
    "\n",
    "    out: Dict[str, Optional[float]] = {}\n",
    "    out.update(lex)\n",
    "    out.update(syn)\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_all_complexity_measures_df(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"text\",\n",
    "    lang: str = \"en\",\n",
    "    spacy_nlp=None,\n",
    ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
    "    \"\"\"\n",
    "    Compute all complexity measures for each row in df[column].\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with a text column.\n",
    "    column : str, default \"text\"\n",
    "        Name of the text column.\n",
    "    lang : str, default \"en\"\n",
    "        Language code for stanza.\n",
    "    n_jobs : int, default 1\n",
    "        Number of worker processes to use.\n",
    "            - 1  : sequential execution (no multiprocessing).\n",
    "            - >1 : multiprocessing with that many workers.\n",
    "            - 0 or None : use cpu_count() workers.\n",
    "    spacy_nlp : spaCy Language, required for LC / CoH\n",
    "        Pre-loaded spaCy pipeline with:\n",
    "            - POS / lemmatizer for LC\n",
    "            - word vectors for CoH (e.g. 'en_core_web_md').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"MTLD\": {index: value},\n",
    "            \"LD\":   {index: value},\n",
    "            \"LS\":   {index: value},\n",
    "            \"MDD\":  {index: value},\n",
    "            \"CS\":   {index: value},\n",
    "            \"LC\":   {index: value},\n",
    "            \"CoH\":  {index: value},\n",
    "        }\n",
    "    \"\"\"\n",
    "    mtld_res: Dict[Any, Optional[float]] = {}\n",
    "    ld_res: Dict[Any, Optional[float]] = {}\n",
    "    ls_res: Dict[Any, Optional[float]] = {}\n",
    "    mdd_res: Dict[Any, Optional[float]] = {}\n",
    "    cs_res: Dict[Any, Optional[float]] = {}\n",
    "\n",
    "    items = list(df[column].items())  # list[(index, text)]\n",
    "    total_items = len(items)\n",
    "\n",
    "    # ---- Lexical + syntactic (stanza) ----\n",
    "    for idx, text in tqdm(\n",
    "        items,\n",
    "        total=total_items,\n",
    "        desc=\"Computing lexical & syntactic complexity (sequential)\",\n",
    "    ):\n",
    "        metrics = _analyze_text_all(text, lang=lang)\n",
    "        mtld_res[idx] = metrics[\"MTLD\"]\n",
    "        ld_res[idx] = metrics[\"LD\"]\n",
    "        ls_res[idx] = metrics[\"LS\"]\n",
    "        mdd_res[idx] = metrics[\"MDD\"]\n",
    "        cs_res[idx] = metrics[\"CS\"]\n",
    "\n",
    "\n",
    "    # ---- Discourse measures (spaCy: LC & CoH) ----\n",
    "    if spacy_nlp is None:\n",
    "        raise ValueError(\n",
    "            \"spacy_nlp must be provided to compute LC and CoH. \"\n",
    "            \"Load a spaCy model with vectors, e.g. 'en_core_web_md', and \"\n",
    "            \"pass it as spacy_nlp=...\"\n",
    "        )\n",
    "\n",
    "    discourse = compute_discourse_measures(df, spacy_nlp, column=column)\n",
    "    lc_vec = discourse[\"LC\"]\n",
    "    coh_vec = discourse[\"CoH\"]\n",
    "\n",
    "    lc_res: Dict[Any, float] = {}\n",
    "    coh_res: Dict[Any, float] = {}\n",
    "\n",
    "    # Map arrays back to DataFrame indices\n",
    "    for i, idx in enumerate(df.index):\n",
    "        lc_res[idx] = float(lc_vec[i])\n",
    "        coh_res[idx] = float(coh_vec[i])\n",
    "\n",
    "    return {\n",
    "        \"MTLD\": mtld_res,\n",
    "        \"LD\": ld_res,\n",
    "        \"LS\": ls_res,\n",
    "        \"MDD\": mdd_res,\n",
    "        \"CS\": cs_res,\n",
    "        \"LC\": lc_res,\n",
    "        \"CoH\": coh_res,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11af2ab",
   "metadata": {},
   "source": [
    "## Calcul métriques Simple/Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63769d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_both_sides_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    col_simple: str = COL_SIMPLE,\n",
    "    col_complex: str = COL_COMPLEX,\n",
    "    lang: str = LANG,\n",
    "    spacy_nlp=None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule les 7 métriques de complexité pour chaque ligne, pour Simple et Complex,\n",
    "    en réutilisant strictement compute_all_complexity_measures_df (test.ipynb).\n",
    "\n",
    "    Retour : DataFrame original + 14 colonnes :\n",
    "      - simple_MTLD ... simple_CoH\n",
    "      - complex_MTLD ... complex_CoH\n",
    "    \"\"\"\n",
    "    required = {col_simple, col_complex}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}. Colonnes présentes: {list(df.columns)}\")\n",
    "\n",
    "    if spacy_nlp is None:\n",
    "        raise ValueError(\"spacy_nlp doit être fourni (modèle avec vecteurs)\")\n",
    "\n",
    "    # Mesures Simple\n",
    "    simple = compute_all_complexity_measures_df(df, column=col_simple, lang=lang, spacy_nlp=spacy_nlp)\n",
    "    # Mesures Complex\n",
    "    complex_ = compute_all_complexity_measures_df(df, column=col_complex, lang=lang, spacy_nlp=spacy_nlp)\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # On aligne explicitement sur l'index du DataFrame\n",
    "    for m in MEASURES:\n",
    "        out[f\"simple_{m}\"] = pd.Series(simple[m]).reindex(out.index)\n",
    "        out[f\"complex_{m}\"] = pd.Series(complex_[m]).reindex(out.index)\n",
    "\n",
    "    # Conversion numérique (sécurité)\n",
    "    metric_cols = [f\"simple_{m}\" for m in MEASURES] + [f\"complex_{m}\" for m in MEASURES]\n",
    "    for c in metric_cols:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e42537",
   "metadata": {},
   "source": [
    "## Ajout orig_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e41759fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_orig_id(df: pd.DataFrame, id_col: str = ID_COL) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ajoute une colonne identifiant stable 'orig_id' basée sur le numéro de ligne d'origine (0-based).\n",
    "    Si la colonne existe déjà, on ne la modifie pas.\n",
    "    \"\"\"\n",
    "    if id_col in df.columns:\n",
    "        return df\n",
    "    df2 = df.reset_index(drop=True).copy()\n",
    "    df2.insert(0, id_col, df2.index.astype(int))\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e59b8a",
   "metadata": {},
   "source": [
    "## Filtrage NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11abd1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_with_any_nan_metrics(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Supprime toute ligne pour laquelle au moins une des 14 métriques (Simple/Complex) est NaN.\n",
    "    Retourne :\n",
    "      - df_kept_nan : lignes conservées\n",
    "      - df_removed_nan : lignes supprimées à cause de NaN\n",
    "    \"\"\"\n",
    "    metric_cols = [f\"simple_{m}\" for m in MEASURES] + [f\"complex_{m}\" for m in MEASURES]\n",
    "    mask_ok = df[metric_cols].notna().all(axis=1)\n",
    "    return df[mask_ok].copy(), df[~mask_ok].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca6486",
   "metadata": {},
   "source": [
    "## Dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be549f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominance_flags(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Définition :\n",
    "    Une ligne est en dominance si :\n",
    "      1) Simple atteint ou dépasse Complex sur TOUTES les métriques (>=),\n",
    "      2) ET il y a au moins une amélioration stricte (>) dans chacune des 3 dimensions :\n",
    "         - lexicale : MTLD, LD, LS\n",
    "         - syntaxique : MDD, CS\n",
    "         - discursive : LC, CoH\n",
    "\n",
    "    Retour :\n",
    "      - Series booléenne : True si dominance atteinte, sinon False.\n",
    "    \"\"\"\n",
    "    lex = [\"MTLD\", \"LD\", \"LS\"]\n",
    "    synt = [\"MDD\", \"CS\"]\n",
    "    disc = [\"LC\", \"CoH\"]\n",
    "\n",
    "    # 1) Condition \"atteint ou dépasse\" sur toutes les métriques\n",
    "    all_measures = lex + synt + disc\n",
    "    ge_all = []\n",
    "    for m in all_measures:\n",
    "        ge_all.append(df[f\"simple_{m}\"] >= df[f\"complex_{m}\"])\n",
    "    ge_all = pd.concat(ge_all, axis=1).all(axis=1)\n",
    "\n",
    "    # 2) Amélioration stricte dans chaque dimension\n",
    "    gt_lex = pd.concat([(df[f\"simple_{m}\"] > df[f\"complex_{m}\"]) for m in lex], axis=1).any(axis=1)\n",
    "    gt_synt = pd.concat([(df[f\"simple_{m}\"] > df[f\"complex_{m}\"]) for m in synt], axis=1).any(axis=1)\n",
    "    gt_disc = pd.concat([(df[f\"simple_{m}\"] > df[f\"complex_{m}\"]) for m in disc], axis=1).any(axis=1)\n",
    "\n",
    "    # Dominance finale\n",
    "    return ge_all & gt_lex & gt_synt & gt_disc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f058e4",
   "metadata": {},
   "source": [
    "## Export CSV (All / Kept / Removed / Removed IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ce55bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_csv_bundle(dataset_name: str, df_all: pd.DataFrame, df_kept: pd.DataFrame, df_removed: pd.DataFrame) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte les résultats en CSV (UTF-8), selon le format attendu.\n",
    "    \"\"\"\n",
    "    all_path = OUT_DIR / f\"{dataset_name}_with_metrics_all.csv\"\n",
    "    kept_path = OUT_DIR / f\"{dataset_name}_with_metrics_kept.csv\"\n",
    "    removed_path = OUT_DIR / f\"{dataset_name}_with_metrics_removed.csv\"\n",
    "    removed_ids_path = OUT_DIR / f\"{dataset_name}_removed_ids.csv\"\n",
    "\n",
    "    df_all.to_csv(all_path, index=False, encoding=\"utf-8\")\n",
    "    df_kept.to_csv(kept_path, index=False, encoding=\"utf-8\")\n",
    "    df_removed.to_csv(removed_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    pd.DataFrame({ID_COL: df_removed[ID_COL].astype(int).sort_values()}).to_csv(\n",
    "        removed_ids_path, index=False, encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"all\": str(all_path),\n",
    "        \"kept\": str(kept_path),\n",
    "        \"removed\": str(removed_path),\n",
    "        \"removed_ids\": str(removed_ids_path),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604e71e",
   "metadata": {},
   "source": [
    "## Pipeline dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7653d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_offline(dataset_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Prépare un dataset :\n",
    "      1) charge les données via load_dataset (test.ipynb)\n",
    "      2) ajoute orig_id (numéro de ligne d'origine)\n",
    "      3) calcule les métriques offline (Simple & Complex)\n",
    "      4) supprime les lignes avec NaN sur une métrique\n",
    "      5) supprime les lignes en dominance\n",
    "      6) exporte en CSV\n",
    "\n",
    "    Sortie finale (df_kept) : 16 colonnes minimales :\n",
    "      orig_id, Simple, Complex, 7 métriques Simple, 7 métriques Complex\n",
    "    \"\"\"\n",
    "    df_raw = load_dataset(dataset_name)\n",
    "    df_raw = add_orig_id(df_raw, id_col=ID_COL)\n",
    "\n",
    "    # On conserve uniquement les colonnes texte + id\n",
    "    df_base = df_raw[[ID_COL, COL_SIMPLE, COL_COMPLEX]].copy()\n",
    "\n",
    "    # Calcul des métriques\n",
    "    df_all = compute_both_sides_metrics(df_base, spacy_nlp=spacy_nlp)\n",
    "\n",
    "    # Marquage NaN\n",
    "    df_no_nan, df_removed_nan = drop_rows_with_any_nan_metrics(df_all)\n",
    "    df_no_nan[\"removed_reason\"] = \"\"  # juste pour cohérence si besoin\n",
    "\n",
    "    # Dominance\n",
    "    dom = dominance_flags(df_no_nan)\n",
    "    df_no_nan[\"simple_dominates_complex\"] = dom\n",
    "\n",
    "    df_removed_dom = df_no_nan[df_no_nan[\"simple_dominates_complex\"]].copy()\n",
    "    df_kept = df_no_nan[~df_no_nan[\"simple_dominates_complex\"]].copy()\n",
    "\n",
    "    # Bundle removed = NaN + dominance \n",
    "    df_removed_nan = df_removed_nan.copy()\n",
    "    if not df_removed_nan.empty:\n",
    "        df_removed_nan[\"simple_dominates_complex\"] = False\n",
    "        df_removed_nan[\"removed_reason\"] = \"nan_metrics\"\n",
    "\n",
    "    if not df_removed_dom.empty:\n",
    "        df_removed_dom[\"removed_reason\"] = \"dominance\"\n",
    "\n",
    "    df_removed = pd.concat([df_removed_nan, df_removed_dom], axis=0).sort_values(ID_COL)\n",
    "\n",
    "    # Création rendu metric\n",
    "    metric_cols_simple = [f\"simple_{m}\" for m in MEASURES]\n",
    "    metric_cols_complex = [f\"complex_{m}\" for m in MEASURES]\n",
    "    cols_16 = [ID_COL, COL_SIMPLE, COL_COMPLEX] + metric_cols_simple + metric_cols_complex\n",
    "\n",
    "    df_all_export = df_all[cols_16].copy()\n",
    "    df_kept_export = df_kept[cols_16].copy()\n",
    "    df_removed_export = df_removed[cols_16 + [\"removed_reason\"]].copy() if \"removed_reason\" in df_removed.columns else df_removed[cols_16].copy()\n",
    "\n",
    "    paths = export_csv_bundle(dataset_name, df_all_export, df_kept_export, df_removed_export)\n",
    "\n",
    "    return {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"rows_all\": int(df_all_export.shape[0]),\n",
    "        \"rows_kept\": int(df_kept_export.shape[0]),\n",
    "        \"rows_removed\": int(df_removed_export.shape[0]),\n",
    "        \"paths\": paths,\n",
    "        \"df_all\": df_all_export,\n",
    "        \"df_kept\": df_kept_export,\n",
    "        \"df_removed\": df_removed_export,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16cb9e4",
   "metadata": {},
   "source": [
    "## Exécution sur tous les datasets + aperçu DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9244597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preparing: ose_adv_int.csv ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing lexical & syntactic complexity (sequential):   0%|          | 0/189 [00:00<?, ?it/s]2025-12-19 22:12:39 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 7.48MB/s]                    \n",
      "2025-12-19 22:12:40 INFO: Downloaded file to C:\\Users\\rroll\\stanza_resources\\resources.json\n",
      "2025-12-19 22:12:40 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-12-19 22:12:40 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "======================================\n",
      "\n",
      "2025-12-19 22:12:40 INFO: Using device: cpu\n",
      "2025-12-19 22:12:40 INFO: Loading: tokenize\n",
      "2025-12-19 22:12:40 INFO: Loading: mwt\n",
      "2025-12-19 22:12:40 INFO: Loading: pos\n",
      "2025-12-19 22:12:41 INFO: Loading: lemma\n",
      "2025-12-19 22:12:42 INFO: Loading: constituency\n",
      "2025-12-19 22:12:42 INFO: Loading: depparse\n",
      "2025-12-19 22:12:42 INFO: Done loading processors!\n",
      "Computing lexical & syntactic complexity (sequential): 100%|██████████| 189/189 [16:47<00:00,  5.33s/it]\n",
      "Computing lexical & syntactic complexity (sequential): 100%|██████████| 189/189 [20:43<00:00,  6.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: 189 | Kept: 189 | Removed: 0\n",
      "Saved to: {'all': 'data_prepared\\\\ose_adv_int_with_metrics_all.csv', 'kept': 'data_prepared\\\\ose_adv_int_with_metrics_kept.csv', 'removed': 'data_prepared\\\\ose_adv_int_with_metrics_removed.csv', 'removed_ids': 'data_prepared\\\\ose_adv_int_removed_ids.csv'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_id</th>\n",
       "      <th>Simple</th>\n",
       "      <th>Complex</th>\n",
       "      <th>simple_MTLD</th>\n",
       "      <th>simple_LD</th>\n",
       "      <th>simple_LS</th>\n",
       "      <th>simple_MDD</th>\n",
       "      <th>simple_CS</th>\n",
       "      <th>simple_LC</th>\n",
       "      <th>simple_CoH</th>\n",
       "      <th>complex_MTLD</th>\n",
       "      <th>complex_LD</th>\n",
       "      <th>complex_LS</th>\n",
       "      <th>complex_MDD</th>\n",
       "      <th>complex_CS</th>\n",
       "      <th>complex_LC</th>\n",
       "      <th>complex_CoH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>When you see the word Amazon, whats the first ...</td>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.263514</td>\n",
       "      <td>3.523032</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>1.435897</td>\n",
       "      <td>0.866181</td>\n",
       "      <td>96.971182</td>\n",
       "      <td>0.564593</td>\n",
       "      <td>0.313559</td>\n",
       "      <td>3.658385</td>\n",
       "      <td>3.826087</td>\n",
       "      <td>1.618182</td>\n",
       "      <td>0.886974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>To tourists, Amsterdam still seems very libera...</td>\n",
       "      <td>﻿Amsterdam still looks liberal to tourists, wh...</td>\n",
       "      <td>80.286316</td>\n",
       "      <td>0.541850</td>\n",
       "      <td>0.304878</td>\n",
       "      <td>3.209069</td>\n",
       "      <td>4.650000</td>\n",
       "      <td>1.197339</td>\n",
       "      <td>0.867083</td>\n",
       "      <td>91.400000</td>\n",
       "      <td>0.533917</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>3.397510</td>\n",
       "      <td>4.684211</td>\n",
       "      <td>1.164811</td>\n",
       "      <td>0.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Brazils latest funk sensation, Anitta, has won...</td>\n",
       "      <td>﻿Brazil’s latest funk sensation, Anitta, has w...</td>\n",
       "      <td>100.372439</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.283069</td>\n",
       "      <td>3.170267</td>\n",
       "      <td>3.944444</td>\n",
       "      <td>2.546243</td>\n",
       "      <td>0.822437</td>\n",
       "      <td>92.232609</td>\n",
       "      <td>0.527691</td>\n",
       "      <td>0.316832</td>\n",
       "      <td>3.109058</td>\n",
       "      <td>4.104167</td>\n",
       "      <td>3.159700</td>\n",
       "      <td>0.822927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>It has mapped the worlds highest peaks, the oc...</td>\n",
       "      <td>﻿It has charted the world’s highest peaks, the...</td>\n",
       "      <td>83.285714</td>\n",
       "      <td>0.564322</td>\n",
       "      <td>0.246201</td>\n",
       "      <td>3.209074</td>\n",
       "      <td>4.409091</td>\n",
       "      <td>2.027730</td>\n",
       "      <td>0.879231</td>\n",
       "      <td>108.234454</td>\n",
       "      <td>0.556650</td>\n",
       "      <td>0.309735</td>\n",
       "      <td>3.386614</td>\n",
       "      <td>5.037037</td>\n",
       "      <td>2.083650</td>\n",
       "      <td>0.882598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The controversial auction of a Banksy mural th...</td>\n",
       "      <td>﻿The controversial auction of a Banksy mural t...</td>\n",
       "      <td>74.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.277592</td>\n",
       "      <td>3.501930</td>\n",
       "      <td>5.818182</td>\n",
       "      <td>1.913413</td>\n",
       "      <td>0.892664</td>\n",
       "      <td>78.500000</td>\n",
       "      <td>0.494268</td>\n",
       "      <td>0.296392</td>\n",
       "      <td>3.488317</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>1.922876</td>\n",
       "      <td>0.883444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>The huge fortunes made by the worlds richest 1...</td>\n",
       "      <td>﻿The vast fortunes made by the world’s richest...</td>\n",
       "      <td>75.797571</td>\n",
       "      <td>0.529644</td>\n",
       "      <td>0.220149</td>\n",
       "      <td>3.189067</td>\n",
       "      <td>4.173913</td>\n",
       "      <td>1.747881</td>\n",
       "      <td>0.880336</td>\n",
       "      <td>89.698761</td>\n",
       "      <td>0.543372</td>\n",
       "      <td>0.262048</td>\n",
       "      <td>3.404561</td>\n",
       "      <td>4.458333</td>\n",
       "      <td>1.977153</td>\n",
       "      <td>0.869142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Back in 2005, when BlackBerry brought instant ...</td>\n",
       "      <td>﻿Back in 2005, when BlackBerry brought instant...</td>\n",
       "      <td>100.424337</td>\n",
       "      <td>0.549043</td>\n",
       "      <td>0.315904</td>\n",
       "      <td>3.029889</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>1.835985</td>\n",
       "      <td>0.785541</td>\n",
       "      <td>121.664773</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.330827</td>\n",
       "      <td>3.221481</td>\n",
       "      <td>2.981132</td>\n",
       "      <td>2.238967</td>\n",
       "      <td>0.809628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>The right of Bolivias indigenous Indian tribes...</td>\n",
       "      <td>﻿A major international row with wide-ranging i...</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>0.541606</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>3.206160</td>\n",
       "      <td>3.794118</td>\n",
       "      <td>2.085457</td>\n",
       "      <td>0.842960</td>\n",
       "      <td>63.952921</td>\n",
       "      <td>0.543584</td>\n",
       "      <td>0.369710</td>\n",
       "      <td>3.417543</td>\n",
       "      <td>4.212121</td>\n",
       "      <td>1.825871</td>\n",
       "      <td>0.864913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Male bosses are paid bonuses double the size o...</td>\n",
       "      <td>﻿Male bosses are being paid bonuses double the...</td>\n",
       "      <td>96.635533</td>\n",
       "      <td>0.565966</td>\n",
       "      <td>0.175676</td>\n",
       "      <td>3.106347</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>3.007968</td>\n",
       "      <td>0.889848</td>\n",
       "      <td>95.714286</td>\n",
       "      <td>0.538806</td>\n",
       "      <td>0.224377</td>\n",
       "      <td>3.393355</td>\n",
       "      <td>5.318182</td>\n",
       "      <td>3.143317</td>\n",
       "      <td>0.871018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Brazil experienced one of its biggest nights o...</td>\n",
       "      <td>﻿Brazil experienced one of its biggest nights ...</td>\n",
       "      <td>117.666667</td>\n",
       "      <td>0.593484</td>\n",
       "      <td>0.338902</td>\n",
       "      <td>3.042276</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.333809</td>\n",
       "      <td>0.809157</td>\n",
       "      <td>116.480666</td>\n",
       "      <td>0.588964</td>\n",
       "      <td>0.365201</td>\n",
       "      <td>3.211171</td>\n",
       "      <td>3.846154</td>\n",
       "      <td>2.972509</td>\n",
       "      <td>0.837656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   orig_id                                             Simple  \\\n",
       "0        0  When you see the word Amazon, whats the first ...   \n",
       "1        1  To tourists, Amsterdam still seems very libera...   \n",
       "2        2  Brazils latest funk sensation, Anitta, has won...   \n",
       "3        3  It has mapped the worlds highest peaks, the oc...   \n",
       "4        4  The controversial auction of a Banksy mural th...   \n",
       "5        5  The huge fortunes made by the worlds richest 1...   \n",
       "6        6  Back in 2005, when BlackBerry brought instant ...   \n",
       "7        7  The right of Bolivias indigenous Indian tribes...   \n",
       "8        8  Male bosses are paid bonuses double the size o...   \n",
       "9        9  Brazil experienced one of its biggest nights o...   \n",
       "\n",
       "                                             Complex  simple_MTLD  simple_LD  \\\n",
       "0  ﻿When you see the word Amazon, what’s the firs...    86.666667   0.569231   \n",
       "1  ﻿Amsterdam still looks liberal to tourists, wh...    80.286316   0.541850   \n",
       "2  ﻿Brazil’s latest funk sensation, Anitta, has w...   100.372439   0.540000   \n",
       "3  ﻿It has charted the world’s highest peaks, the...    83.285714   0.564322   \n",
       "4  ﻿The controversial auction of a Banksy mural t...    74.750000   0.500000   \n",
       "5  ﻿The vast fortunes made by the world’s richest...    75.797571   0.529644   \n",
       "6  ﻿Back in 2005, when BlackBerry brought instant...   100.424337   0.549043   \n",
       "7  ﻿A major international row with wide-ranging i...    68.500000   0.541606   \n",
       "8  ﻿Male bosses are being paid bonuses double the...    96.635533   0.565966   \n",
       "9  ﻿Brazil experienced one of its biggest nights ...   117.666667   0.593484   \n",
       "\n",
       "   simple_LS  simple_MDD  simple_CS  simple_LC  simple_CoH  complex_MTLD  \\\n",
       "0   0.263514    3.523032   3.714286   1.435897    0.866181     96.971182   \n",
       "1   0.304878    3.209069   4.650000   1.197339    0.867083     91.400000   \n",
       "2   0.283069    3.170267   3.944444   2.546243    0.822437     92.232609   \n",
       "3   0.246201    3.209074   4.409091   2.027730    0.879231    108.234454   \n",
       "4   0.277592    3.501930   5.818182   1.913413    0.892664     78.500000   \n",
       "5   0.220149    3.189067   4.173913   1.747881    0.880336     89.698761   \n",
       "6   0.315904    3.029889   2.857143   1.835985    0.785541    121.664773   \n",
       "7   0.339623    3.206160   3.794118   2.085457    0.842960     63.952921   \n",
       "8   0.175676    3.106347   3.280000   3.007968    0.889848     95.714286   \n",
       "9   0.338902    3.042276   3.500000   2.333809    0.809157    116.480666   \n",
       "\n",
       "   complex_LD  complex_LS  complex_MDD  complex_CS  complex_LC  complex_CoH  \n",
       "0    0.564593    0.313559     3.658385    3.826087    1.618182     0.886974  \n",
       "1    0.533917    0.360656     3.397510    4.684211    1.164811     0.869300  \n",
       "2    0.527691    0.316832     3.109058    4.104167    3.159700     0.822927  \n",
       "3    0.556650    0.309735     3.386614    5.037037    2.083650     0.882598  \n",
       "4    0.494268    0.296392     3.488317    5.166667    1.922876     0.883444  \n",
       "5    0.543372    0.262048     3.404561    4.458333    1.977153     0.869142  \n",
       "6    0.550725    0.330827     3.221481    2.981132    2.238967     0.809628  \n",
       "7    0.543584    0.369710     3.417543    4.212121    1.825871     0.864913  \n",
       "8    0.538806    0.224377     3.393355    5.318182    3.143317     0.871018  \n",
       "9    0.588964    0.365201     3.211171    3.846154    2.972509     0.837656  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results = {}\n",
    "\n",
    "DATA_DIR = Path(\"data_sampled\")\n",
    "\n",
    "# Liste des CSV à traiter\n",
    "CSV_FILES_TO_RUN = [\n",
    "    # \"ose_adv_ele.csv\",\n",
    "    \"ose_adv_int.csv\",\n",
    "    # \"swipe.csv\",\n",
    "    # 'vikidia.csv'\n",
    "]\n",
    "\n",
    "for csv_name in CSV_FILES_TO_RUN:\n",
    "    csv_path = DATA_DIR / csv_name\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Fichier introuvable : {csv_path.resolve()}\")\n",
    "\n",
    "    dataset_name = csv_path.stem\n",
    "    print(f\"\\n=== Preparing: {csv_path.name} ===\")\n",
    "\n",
    "    info = prepare_dataset_offline(dataset_name)\n",
    "    results[dataset_name] = info\n",
    "\n",
    "    print(f\"All: {info['rows_all']} | Kept: {info['rows_kept']} | Removed: {info['rows_removed']}\")\n",
    "    print(\"Saved to:\", info[\"paths\"])\n",
    "\n",
    "# Aperçu\n",
    "example_name = next(iter(results.keys()))\n",
    "df_prepared = results[example_name][\"df_kept\"]\n",
    "df_prepared.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
