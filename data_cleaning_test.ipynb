{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a828717",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Préparation des données d'évaluation (version alignée sur test.ipynb)\n",
    "\n",
    "- Calcule hors-ligne les mesures de complexité (lexicales, syntaxiques, discursives)\n",
    "  * Lexical:  MTLD, LD, LS   (stanza + COW5000 via textcomplexity + WordNet)\n",
    "  * Syntaxe:  MDD, CS        (stanza: depparse + constituency)\n",
    "  * Discours: LC, CoH        (spaCy: lemmas + word vectors)\n",
    "\n",
    "- Écarte toute ligne si une mesure est None/NaN (robustesse exigée)\n",
    "- Marque et supprime les lignes où Simple domine déjà Complex (>= toutes métriques non-NaN\n",
    "  + progrès strict dans chaque famille lex/syn/disc), en conservant leurs IDs originaux\n",
    "- Conserve les IDs d’origine même après filtrage\n",
    "- Exporte: __augmented.csv/.parquet, __filtered.csv/.parquet, __removed_ids.json, __report.json\n",
    "\n",
    "Usage:\n",
    "  python prepare_from_notebook.py --input data_sampled/OSE_adv_ele.csv --sep '\\t' --outdir prepared\n",
    "\n",
    "Tu peux aussi lui donner un autre dataset listé dans ton dict si besoin.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe63c480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rroll\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Imports & dataset mapping\n",
    "from __future__ import annotations\n",
    "\n",
    "# Standard\n",
    "import json, math\n",
    "from dataclasses import dataclass\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import stanza\n",
    "import spacy\n",
    "\n",
    "# textcomplexity for COW top-5000 (en.json)\n",
    "import importlib.resources as pkg_resources\n",
    "import textcomplexity\n",
    "\n",
    "# sklearn (used indirectly in coherence; keep handy)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ---- Datasets (.txt TSV)\n",
    "datasets = {\n",
    "    'ose_adv_ele': 'data_sampled/OSE_adv_ele.txt',\n",
    "    'ose_adv_int': 'data_sampled/OSE_adv_int.txt',\n",
    "    'swipe':       'data_sampled/swipe.txt',\n",
    "    'vikidia':     'data_sampled/vikidia.txt',\n",
    "}\n",
    "\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, sep='\\t')\n",
    "\n",
    "def load_dataset(name: str) -> pd.DataFrame:\n",
    "    if name not in datasets:\n",
    "        raise ValueError(f\"Dataset {name} not found\")\n",
    "    return load_data(datasets[name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc59993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Resource setup\n",
    "def _ensure_resources():\n",
    "    try:\n",
    "        stanza.download(\"en\", processors=\"tokenize,pos,lemma,depparse,constituency\", verbose=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        _ = wn.synsets(\"dog\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"wordnet\")\n",
    "        nltk.download(\"omw-1.4\")\n",
    "\n",
    "    try:\n",
    "        spacy.load(\"en_core_web_md\")\n",
    "    except OSError:\n",
    "        raise RuntimeError(\n",
    "            \"spaCy model 'en_core_web_md' is required. \"\n",
    "            \"Install with: python -m spacy download en_core_web_md\"\n",
    "        )\n",
    "\n",
    "_ensure_resources()\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_md\")\n",
    "spacy_nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "_STANZA_PIPELINES: Dict[str, stanza.Pipeline] = {}\n",
    "def get_stanza_pipeline(lang: str = \"en\", use_gpu: bool = False) -> stanza.Pipeline:\n",
    "    if lang not in _STANZA_PIPELINES:\n",
    "        _STANZA_PIPELINES[lang] = stanza.Pipeline(\n",
    "            lang=lang,\n",
    "            processors=\"tokenize,pos,lemma,depparse,constituency\",\n",
    "            tokenize_no_ssplit=False,\n",
    "            use_gpu=use_gpu,\n",
    "            verbose=False,\n",
    "        )\n",
    "    return _STANZA_PIPELINES[lang]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b766883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Constants & COW loader\n",
    "CONTENT_UPOS = {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"}   # stanza POS\n",
    "CONTENT_POS_SPACY = {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}       # spaCy POS\n",
    "METRICS = [\"MTLD\",\"LD\",\"LS\",\"MDD\",\"CS\",\"LC\",\"CoH\"]\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def load_cow_top5000_en() -> set:\n",
    "    with pkg_resources.files(textcomplexity).joinpath(\"en.json\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "        lang_def = json.load(f)\n",
    "    most_common = lang_def[\"most_common\"]  # list of [word, xpos]\n",
    "    return {w.lower() for w, xpos in most_common}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dbb42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Lexical metrics (stanza)\n",
    "def _compute_mtld(tokens: Iterable[str], ttr_threshold: float = 0.72) -> Optional[float]:\n",
    "    tokens = [t for t in tokens if t]\n",
    "    if not tokens: return None\n",
    "    types, factor_count, in_factor = set(), 0.0, 0\n",
    "    for tok in tokens:\n",
    "        in_factor += 1\n",
    "        types.add(tok)\n",
    "        ttr = len(types)/in_factor\n",
    "        if ttr < ttr_threshold:\n",
    "            factor_count += 1.0\n",
    "            types, in_factor = set(), 0\n",
    "    if in_factor > 0:\n",
    "        final_ttr = len(types)/in_factor\n",
    "        if final_ttr < 1.0:\n",
    "            frac = (1.0 - final_ttr)/(1.0 - ttr_threshold)\n",
    "            factor_count += max(0.0, min(1.0, frac))\n",
    "    if factor_count == 0: return None\n",
    "    return len(tokens)/factor_count\n",
    "\n",
    "def _compute_lexical_density(total_tokens: int, content_tokens: int) -> Optional[float]:\n",
    "    if total_tokens == 0: return None\n",
    "    return content_tokens/total_tokens\n",
    "\n",
    "def _compute_lexical_sophistication_cow(content_forms: Iterable[str], cow_top5000: set) -> Optional[float]:\n",
    "    forms = [f for f in content_forms if f]\n",
    "    if not forms: return None\n",
    "    off_list = sum(1 for f in forms if f not in cow_top5000)\n",
    "    return off_list/len(forms)\n",
    "\n",
    "def lexical_measures_from_doc(doc: stanza.Document) -> Dict[str, Optional[float]]:\n",
    "    cow = load_cow_top5000_en()\n",
    "    mtld_tokens, total_tokens, content_tokens, content_forms = [], 0, 0, []\n",
    "    for sent in doc.sentences:\n",
    "        for w in sent.words:\n",
    "            if w.upos == \"PUNCT\": \n",
    "                continue\n",
    "            lemma = (w.lemma or w.text or \"\").lower()\n",
    "            if not lemma: \n",
    "                continue\n",
    "            mtld_tokens.append(lemma); total_tokens += 1\n",
    "            if w.upos in CONTENT_UPOS:\n",
    "                content_tokens += 1\n",
    "                content_forms.append((w.text or \"\").lower())\n",
    "    return {\n",
    "        \"MTLD\": _compute_mtld(mtld_tokens) if mtld_tokens else None,\n",
    "        \"LD\":   _compute_lexical_density(total_tokens, content_tokens),\n",
    "        \"LS\":   _compute_lexical_sophistication_cow(content_forms, cow),\n",
    "    }\n",
    "\n",
    "def lexical_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return {\"MTLD\": None, \"LD\": None, \"LS\": None}\n",
    "    doc = get_stanza_pipeline(lang)(text)\n",
    "    return lexical_measures_from_doc(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5443ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Syntactic metrics (stanza)\n",
    "def mdd_from_doc(doc: stanza.Document) -> Optional[float]:\n",
    "    sentence_mdds = []\n",
    "    for sent in doc.sentences:\n",
    "        dists = []\n",
    "        for w in sent.words:\n",
    "            if not w.head or w.head == 0: \n",
    "                continue\n",
    "            dists.append(abs(w.id - w.head))\n",
    "        if dists:\n",
    "            sentence_mdds.append(sum(dists)/len(dists))\n",
    "    if not sentence_mdds: return None\n",
    "    return sum(sentence_mdds)/len(sentence_mdds)\n",
    "\n",
    "def _count_clauses_in_tree(tree) -> int:\n",
    "    if tree is None: return 0\n",
    "    cnt = 1 if getattr(tree, \"label\", \"\").startswith(\"S\") else 0\n",
    "    for ch in getattr(tree, \"children\", []):\n",
    "        if hasattr(ch, \"label\"):\n",
    "            cnt += _count_clauses_in_tree(ch)\n",
    "    return cnt\n",
    "\n",
    "def cs_from_doc(doc: stanza.Document) -> Optional[float]:\n",
    "    counts = []\n",
    "    for sent in doc.sentences:\n",
    "        tree = getattr(sent, \"constituency\", None)\n",
    "        if tree is None: \n",
    "            continue\n",
    "        counts.append(_count_clauses_in_tree(tree))\n",
    "    if not counts: return None\n",
    "    return sum(counts)/len(counts)\n",
    "\n",
    "def syntactic_measures_from_doc(doc: stanza.Document) -> Dict[str, Optional[float]]:\n",
    "    return {\"MDD\": mdd_from_doc(doc), \"CS\": cs_from_doc(doc)}\n",
    "\n",
    "def syntactic_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return {\"MDD\": None, \"CS\": None}\n",
    "    doc = get_stanza_pipeline(lang)(text)\n",
    "    return syntactic_measures_from_doc(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3864192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Discourse metrics (spaCy + WordNet)\n",
    "def is_content_token_spacy(tok) -> bool:\n",
    "    return tok.is_alpha and not tok.is_stop and tok.pos_ in CONTENT_POS_SPACY\n",
    "\n",
    "@lru_cache(maxsize=100000)\n",
    "def get_related_lemmas(lemma: str) -> set:\n",
    "    lemma = lemma.lower()\n",
    "    rel = set()\n",
    "    for syn in wn.synsets(lemma):\n",
    "        for l in syn.lemmas():\n",
    "            rel.add(l.name().lower().replace(\"_\", \" \"))\n",
    "            for ant in l.antonyms():\n",
    "                rel.add(ant.name().lower().replace(\"_\", \" \"))\n",
    "        for hyper in syn.hypernyms():\n",
    "            for l in hyper.lemmas(): rel.add(l.name().lower().replace(\"_\", \" \"))\n",
    "        for hypo in syn.hyponyms():\n",
    "            for l in hypo.lemmas(): rel.add(l.name().lower().replace(\"_\", \" \"))\n",
    "        for mer in syn.part_meronyms() + syn.member_meronyms() + syn.substance_meronyms():\n",
    "            for l in mer.lemmas(): rel.add(l.name().lower().replace(\"_\", \" \"))\n",
    "        for hyper in syn.hypernyms():\n",
    "            for sib in hyper.hyponyms():\n",
    "                if sib == syn: continue\n",
    "                for l in sib.lemmas(): rel.add(l.name().lower().replace(\"_\", \" \"))\n",
    "    rel.discard(lemma)\n",
    "    return rel\n",
    "\n",
    "def lexical_cohesion_single(text: str, nlp) -> float:\n",
    "    if not isinstance(text, str) or not text.strip(): return 0.0\n",
    "    doc = nlp(text)\n",
    "    m = sum(1 for t in doc if t.is_alpha)\n",
    "    if m == 0: return 0.0\n",
    "    sents = list(doc.sents)\n",
    "    if len(sents) < 2: return 0.0\n",
    "    sent_lemmas: List[set] = []\n",
    "    for s in sents:\n",
    "        lemmas = set(t.lemma_.lower() for t in s if is_content_token_spacy(t))\n",
    "        if lemmas: sent_lemmas.append(lemmas)\n",
    "    if len(sent_lemmas) < 2: return 0.0\n",
    "    cohesive = 0\n",
    "    for i in range(len(sent_lemmas)-1):\n",
    "        for j in range(i+1, len(sent_lemmas)):\n",
    "            li, lj = sent_lemmas[i], sent_lemmas[j]\n",
    "            cohesive += len(li & lj)\n",
    "            for lemma in li:\n",
    "                cohesive += len(get_related_lemmas(lemma) & lj)\n",
    "    return float(cohesive)/float(m)\n",
    "\n",
    "def sentence_vector(sent, D: int) -> np.ndarray:\n",
    "    vecs = [t.vector for t in sent if t.has_vector and not t.is_punct and not t.is_space]\n",
    "    if not vecs: return np.zeros(D, dtype=\"float32\")\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def coherence_single(text: str, nlp) -> float:\n",
    "    if not isinstance(text, str) or not text.strip(): return 0.0\n",
    "    if nlp.vocab.vectors_length == 0:\n",
    "        raise RuntimeError(\"spaCy model must have word vectors (use 'en_core_web_md').\")\n",
    "    doc = nlp(text)\n",
    "    sents = list(doc.sents)\n",
    "    if len(sents) < 2: return 0.0\n",
    "    D = nlp.vocab.vectors_length\n",
    "    vecs = [sentence_vector(s, D) for s in sents]\n",
    "    sims: List[float] = []\n",
    "    for i in range(len(vecs)-1):\n",
    "        v1, v2 = vecs[i], vecs[i+1]\n",
    "        denom = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "        if denom == 0.0: continue\n",
    "        sims.append(float(np.dot(v1, v2)/denom))\n",
    "    return float(np.mean(sims)) if sims else 0.0\n",
    "\n",
    "def compute_discourse_measures_series(texts: pd.Series, nlp) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    lc = np.array([lexical_cohesion_single(t, nlp) for t in texts], dtype=\"float32\")\n",
    "    coh = np.array([coherence_single(t, nlp) for t in texts], dtype=\"float32\")\n",
    "    return lc, coh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c63333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — All measures DF(column) => 7 columns\n",
    "def compute_all_measures_df(df: pd.DataFrame, column: str, lang: str = \"en\") -> pd.DataFrame:\n",
    "    out = pd.DataFrame(index=df.index, columns=METRICS, dtype=\"float64\")\n",
    "    pipe = get_stanza_pipeline(lang)\n",
    "    texts = df[column].fillna(\"\").astype(str)\n",
    "\n",
    "    # Lexical + syntactic (stanza)\n",
    "    for idx, text in tqdm(list(texts.items()), desc=f\"Stanza {column}\", total=len(texts)):\n",
    "        try:\n",
    "            if not text.strip():\n",
    "                m = {\"MTLD\": None, \"LD\": None, \"LS\": None, \"MDD\": None, \"CS\": None}\n",
    "            else:\n",
    "                doc = pipe(text)\n",
    "                m = {}\n",
    "                m.update(lexical_measures_from_doc(doc))\n",
    "                m.update(syntactic_measures_from_doc(doc))\n",
    "            out.loc[idx, [\"MTLD\",\"LD\",\"LS\",\"MDD\",\"CS\"]] = [m[\"MTLD\"], m[\"LD\"], m[\"LS\"], m[\"MDD\"], m[\"CS\"]]\n",
    "        except Exception:\n",
    "            out.loc[idx, [\"MTLD\",\"LD\",\"LS\",\"MDD\",\"CS\"]] = [None, None, None, None, None]\n",
    "\n",
    "    # Discourse (spaCy)\n",
    "    try:\n",
    "        lc_vec, coh_vec = compute_discourse_measures_series(texts, spacy_nlp)\n",
    "        out.loc[texts.index, \"LC\"]  = lc_vec\n",
    "        out.loc[texts.index, \"CoH\"] = coh_vec\n",
    "    except Exception:\n",
    "        out.loc[texts.index, \"LC\"]  = 0.0\n",
    "        out.loc[texts.index, \"CoH\"] = 0.0\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1bae13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — ComplexityScore & vectorized dominance\n",
    "@dataclass(frozen=True)\n",
    "class ComplexityScore:\n",
    "    MTLD: float; LD: float; LS: float\n",
    "    MDD: float; CS: float\n",
    "    LC: float;  CoH: float\n",
    "\n",
    "    @property\n",
    "    def lex(self): return {\"MTLD\": self.MTLD, \"LD\": self.LD, \"LS\": self.LS}\n",
    "    @property\n",
    "    def syn(self): return {\"MDD\": self.MDD, \"CS\": self.CS}\n",
    "    @property\n",
    "    def dis(self): return {\"LC\": self.LC, \"CoH\": self.CoH}\n",
    "\n",
    "    @staticmethod\n",
    "    def from_row(row: pd.Series, prefix: str) -> \"ComplexityScore\":\n",
    "        def v(name):\n",
    "            try: return float(row[f\"{prefix}_{name}\"])\n",
    "            except Exception: return float(\"nan\")\n",
    "        return ComplexityScore(\n",
    "            MTLD=v(\"MTLD\"), LD=v(\"LD\"), LS=v(\"LS\"),\n",
    "            MDD=v(\"MDD\"), CS=v(\"CS\"),\n",
    "            LC=v(\"LC\"),  CoH=v(\"CoH\"),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _isnan(x: Optional[float]) -> bool:\n",
    "        try: return x is None or math.isnan(float(x))\n",
    "        except Exception: return True\n",
    "\n",
    "    def _ge_or_ignore_nan(self, a: float, b: float) -> bool:\n",
    "        return self._isnan(b) or (not self._isnan(a) and a >= b)\n",
    "\n",
    "    def _strict_progress(self, new: Dict[str, float], old: Dict[str, float]) -> bool:\n",
    "        for k in new:\n",
    "            a, b = new[k], old[k]\n",
    "            if not self._isnan(a) and not self._isnan(b) and a > b:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def dominates(self, target: \"ComplexityScore\",\n",
    "                  previous: \"ComplexityScore|None\" = None,\n",
    "                  length_ok: bool = True) -> bool:\n",
    "        for k in METRICS:\n",
    "            if not self._ge_or_ignore_nan(getattr(self, k), getattr(target, k)):\n",
    "                return False\n",
    "        if not length_ok:\n",
    "            return False\n",
    "        if previous is None:\n",
    "            return True\n",
    "        return (self._strict_progress(self.lex, previous.lex) and\n",
    "                self._strict_progress(self.syn, previous.syn) and\n",
    "                self._strict_progress(self.dis, previous.dis))\n",
    "\n",
    "def simple_dominates_complex_flags(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"True if Simple ≥ Complex (ignore NaN targets) + strict progress per family.\"\"\"\n",
    "    ge_all = []\n",
    "    for m in METRICS:\n",
    "        s = df[f\"simple_{m}\"].astype(float)\n",
    "        c = df[f\"complex_{m}\"].astype(float)\n",
    "        ge_m = (s >= c) | c.isna()\n",
    "        ge_all.append(ge_m)\n",
    "    ge_ok = np.logical_and.reduce(ge_all)\n",
    "\n",
    "    def strict_progress(cols_s, cols_c):\n",
    "        s = df[cols_s].astype(float).to_numpy()\n",
    "        c = df[cols_c].astype(float).to_numpy()\n",
    "        valid = ~np.isnan(s) & ~np.isnan(c)\n",
    "        return ((s > c) & valid).any(axis=1)\n",
    "\n",
    "    prog_lex = strict_progress(\n",
    "        [\"simple_MTLD\",\"simple_LD\",\"simple_LS\"],\n",
    "        [\"complex_MTLD\",\"complex_LD\",\"complex_LS\"],\n",
    "    )\n",
    "    prog_syn = strict_progress(\n",
    "        [\"simple_MDD\",\"simple_CS\"],\n",
    "        [\"complex_MDD\",\"complex_CS\"],\n",
    "    )\n",
    "    prog_dis = strict_progress(\n",
    "        [\"simple_LC\",\"simple_CoH\"],\n",
    "        [\"complex_LC\",\"complex_CoH\"],\n",
    "    )\n",
    "\n",
    "    return ge_ok & prog_lex & prog_syn & prog_dis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "810445e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 — Prepare & save helpers\n",
    "def prepare_dataset(\n",
    "    df_in: pd.DataFrame,\n",
    "    col_simple: str = \"Simple\",\n",
    "    col_complex: str = \"Complex\",\n",
    "    lang: str = \"en\",\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    df = df_in.copy()\n",
    "\n",
    "    if \"id\" not in df.columns:\n",
    "        df = df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "    df[\"id\"] = df[\"id\"].astype(int)\n",
    "\n",
    "    assert col_simple in df.columns and col_complex in df.columns, \\\n",
    "        f\"Missing required columns: {col_simple}, {col_complex}\"\n",
    "\n",
    "    m_simple  = compute_all_measures_df(df, column=col_simple,  lang=lang).add_prefix(\"simple_\")\n",
    "    m_complex = compute_all_measures_df(df, column=col_complex, lang=lang).add_prefix(\"complex_\")\n",
    "\n",
    "    df_aug = pd.concat([df[[\"id\", col_simple, col_complex]], m_simple, m_complex], axis=1)\n",
    "\n",
    "    measure_cols = [*(m_simple.columns.tolist()), *(m_complex.columns.tolist())]\n",
    "    df_aug_clean = df_aug.dropna(subset=measure_cols).copy()\n",
    "\n",
    "    df_aug_clean[\"simple_dominates_complex\"] = simple_dominates_complex_flags(df_aug_clean)\n",
    "\n",
    "    df_removed = df_aug_clean[df_aug_clean[\"simple_dominates_complex\"]].copy()\n",
    "    df_kept    = df_aug_clean[~df_aug_clean[\"simple_dominates_complex\"]].copy()\n",
    "\n",
    "    ordered_cols = [\n",
    "        \"id\", col_simple, col_complex,\n",
    "        *[f\"simple_{k}\" for k in METRICS],\n",
    "        *[f\"complex_{k}\" for k in METRICS],\n",
    "        \"simple_dominates_complex\",\n",
    "    ]\n",
    "    df_aug_clean = df_aug_clean[ordered_cols]\n",
    "    df_kept      = df_kept[[c for c in ordered_cols if c != \"simple_dominates_complex\"]]\n",
    "\n",
    "    return df_aug_clean, df_kept, df_removed\n",
    "\n",
    "def save_artifacts(\n",
    "    df_aug: pd.DataFrame,\n",
    "    df_kept: pd.DataFrame,\n",
    "    df_removed: pd.DataFrame,\n",
    "    out_dir: Path,\n",
    "    base_name: str,\n",
    "):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df_aug.to_csv(out_dir / f\"{base_name}__augmented.csv\", index=False)\n",
    "    df_kept.to_csv(out_dir / f\"{base_name}__filtered.csv\", index=False)\n",
    "    try:\n",
    "        df_aug.to_parquet(out_dir / f\"{base_name}__augmented.parquet\", index=False)\n",
    "        df_kept.to_parquet(out_dir / f\"{base_name}__filtered.parquet\", index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    removed_ids = df_removed[\"id\"].astype(int).tolist()\n",
    "    with (out_dir / f\"{base_name}__removed_ids.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"removed_ids\": removed_ids, \"count\": len(removed_ids)}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    report = {\n",
    "        \"total_rows_after_nan_filter\": int(len(df_aug)),\n",
    "        \"kept_rows\": int(len(df_kept)),\n",
    "        \"removed_rows\": int(len(df_removed)),\n",
    "        \"removed_ratio\": float(len(df_removed) / max(1, len(df_aug))),\n",
    "        \"metrics\": METRICS,\n",
    "        \"columns_final_expected\": 16,\n",
    "        \"note\": \"16 = Simple, Complex + 7 mesures Simple + 7 mesures Complex\",\n",
    "    }\n",
    "    with (out_dir / f\"{base_name}__report.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64bc7387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows loaded: 189\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Simple</th>\n",
       "      <th>Complex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿To tourists, Amsterdam still seems very liber...</td>\n",
       "      <td>﻿Amsterdam still looks liberal to tourists, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>﻿Anitta, a music star from Brazil, has million...</td>\n",
       "      <td>﻿Brazil’s latest funk sensation, Anitta, has w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Simple  \\\n",
       "0  ﻿When you see the word Amazon, what’s the firs...   \n",
       "1  ﻿To tourists, Amsterdam still seems very liber...   \n",
       "2  ﻿Anitta, a music star from Brazil, has million...   \n",
       "\n",
       "                                             Complex  \n",
       "0  ﻿When you see the word Amazon, what’s the firs...  \n",
       "1  ﻿Amsterdam still looks liberal to tourists, wh...  \n",
       "2  ﻿Brazil’s latest funk sensation, Anitta, has w...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stanza Simple: 100%|██████████| 189/189 [42:14<00:00, 13.41s/it]\n",
      "Stanza Complex: 100%|██████████| 189/189 [1:05:30<00:00, 20.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After NaN filter: 189\n",
      "Kept: 189 | Removed (Simple dominates): 0\n",
      "\n",
      "Saved to: C:\\Users\\rroll\\Documents\\GitHub\\ISH_projet_agno_agent_intelligent\\prepared\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 — Example run\n",
    "DATASET_NAME = \"ose_adv_ele\"   # choose one: ose_adv_ele | ose_adv_int | swipe | vikidia\n",
    "OUT_DIR = Path(\"prepared\")\n",
    "\n",
    "df_raw = load_dataset(DATASET_NAME)\n",
    "print(\"Rows loaded:\", len(df_raw))\n",
    "display(df_raw.head(3))\n",
    "\n",
    "df_aug, df_kept, df_removed = prepare_dataset(df_raw, col_simple=\"Simple\", col_complex=\"Complex\", lang=\"en\")\n",
    "\n",
    "print(\"After NaN filter:\", len(df_aug))\n",
    "print(\"Kept:\", len(df_kept), \"| Removed (Simple dominates):\", len(df_removed))\n",
    "\n",
    "save_artifacts(df_aug, df_kept, df_removed, OUT_DIR, base_name=DATASET_NAME)\n",
    "\n",
    "print(\"\\nSaved to:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae0a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
